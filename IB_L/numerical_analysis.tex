\documentclass[a4paper]{article}

\def\npart {IB}
\def\nterm {Lent}
\def\nyear {2016}
\def\nlecturer {G. Moore}
\def\ncourse {Numerical Analysis}
\def\nlectures {MWF.10}
\def\nofficial {http://www.damtp.cam.ac.uk/user/sjc1/teaching/NAIB/notes.pdf}
 % remove?
\def\nnotready {}

\input{header}

\begin{document}
\maketitle
{\small
\noindent\textbf{Polynomial approximation}\\
Interpolation by polynomials. Divided differences of functions and relations to derivatives. Orthogonal polynomials and their recurrence relations. Least squares approximation by polynomials. Gaussian quadrature formulae. Peano kernel theorem and applications.\hspace*{\fill} [6]

\vspace{10pt}
\noindent\textbf{Computation of ordinary differential equations}\\
Euler's method and proof of convergence. Multistep methods, including order, the root condition and the concept of convergence. Runge-Kutta schemes. Stiff equations and A-stability.\hspace*{\fill} [5]

\vspace{10pt}
\noindent\textbf{Systems of equations and least squares calculations}\\
LU triangular factorization of matrices. Relation to Gaussian elimination. Column pivoting. Factorizations of symmetric and band matrices. The Newton-Raphson method for systems of non-linear algebraic equations. QR factorization of rectangular matrices by Gram-Schmidt, Givens and Householder techniques. Application to linear least squares calculations.\hspace*{\fill} [5]}

\tableofcontents
\setcounter{section}{-1}
\section{Introduction}
What is numerical analysis? It is the study of algorithms. It is about algorithms for solving problems in, say, linear algebra. In the course, you will find that many of the algorithms are named after great figures in the past, like Newton, Euler and Gauss. They have done a lot of work to come up with good algorithms that aids computation. In the past, they had to implement these algorithms by hand, and it makes sense for them to try to simplify them! Nowadays, we are much luckier and have computers to help us solve problems. We can often solve problems by the press of a button.

However, it is also useful to understand how algorithms works, since we might in the future encounter problems with no pre-written solutions by others, and we might need to come up with something ourselves.

\section{Polynomial interpolation}
Polynomials are important. Many many ideas in numerical analysis are based on polynomials approximations, and it is important to understand. The simplest place to start is through polynomial interpolations.

\begin{notation}
  We write $P_n[x]$ for the real linear vector space of polynomials (with real coefficients) having degree $n$ or less.
\end{notation}
Of course, we can also work with complex polynomials which form a complex vector space. However, for simplicity, we will focus on real polynomials.

It is easy to show that $\dim (P_n[x]) = n + 1$.

\subsection{The interpolation problem}
Suppose we are given $n + 1$ distinct interpolation points $\{x_i\}_{i = 0}^n \subseteq \R$, and $n + 1$ data values $\{f_i\}_{i = 0}^n \subseteq \R$. The objective is to find a $p \in P_n[x]$ such that $p(x_i) = f_i$ for all $i$. In other words, we want to fit a polynomial through the points $(x_i, f_i)$.

We have $n + 1$ coefficients to choose from, and $n + 1$ conditions to satisfy. Given this, in general, there is no guarantee that there is a solution, or that the solution is unique. Our first goal is to show that in the case of polynomial interpolation, the solution exists and is unique.

There are many situations where this may come up. For example, we may have $n + 1$ actual data points, and want to fit a polynomial through the points. Alternatively, we might have a given function $f$, and want to approximate it with a polynomial $p$ such that $p$ and $f$ agree on at least that $n + 1$ points.

\subsection{The Lagrange formula}
It turns out the problem is not too hard. You can probably figure it out yourself if you lock yourself in a room for a few days (or hours). The first solution to this would be via Lagrange cardinal polynomials.
\begin{defi}[Lagrange cardinal polynomials]
  The \emph{Lagrange cardinal polynomials} with respect to the interpolation points $\{x_i\}_{i = 0}^n$ are, for $k = 0, \cdots, n$,
  \[
    \ell_k (x) = \prod_{i = 0, i \not= k}^n \frac{x - x_i}{x_k - x_i}.
  \]
\end{defi}
Note that these polynomials have degree exactly $n$. The significance of these polynomials is that they satisfy
\[
  \ell_k(x_j) = \delta_{jk}.
\]
This is obvious from definition.

With these cardinal polynomials, we can immediately write down a solution to the interpolation problem.
\begin{thm}
  The interpolation problem has exactly one solution.
\end{thm}

\begin{proof}
  We define $p \in P_n[x]$ by
  \[
    p(x) = \sum_{k = 0}^n f_k \ell_k (x).
  \]
  Evaluating at $x_i$ gives
  \[
    p(x_j) = \sum_{k = 0}^n f_k \ell_k(x_j) = \sum_{k = 0}^n f_k \delta_{jk} = f_j.
  \]
  So we get existence.

  For uniqueness, suppose $p, q \in P_n[x]$ are solutions. Then the difference $r = p - q \in P_n[x]$ satisfies $r(x_j) = 0$ for all $j$, ie. it has $n + 1$ roots. However, a non-zero polynomial of degree $n$ can have at most $n$ roots. So in fact $p - q$ is zero, ie. $p = q$.
\end{proof}
So we have solved the problem of polynomials. However, it turns out there is a better way to approach the problem.

\subsection{The Newton formula}
For $k = 0, \cdots, n$, let $p_k \in P_k[x]$ satisfy $p_k(x_i) = f_i$ for $i = 0, \cdots, k$. These are the unique degree-$k$ polynomials that satisfies the first $k$ conditions. Then we can write
\[
  p(x) = p_n(x) = p_0(x) + (p_1(x) - p_0(x)) + \cdots + (p_n(x) - p_{n - 1}(x)).
\]
Hence it suffices to find the differences $p_k - p_{k - 1}$. However, we know that $p_k$ and $p_{k - 1}$ agree on $x_0, \cdots, x_{k - 1}$. So the difference is $0$ at those points, and we have
\[
  p_k(x) - p_{k - 1}(x) = A_k \prod_{i = 0}^{k - 1}(x - x_i),
\]
for some $A_k$ yet to be found out. Then we can write
\[
  p(x) = p_n(x) = A_0 + \sum_{k = 1}^n A_k \prod_{i = 0}^{k - 1} (x - x_i).
\]
This formula has the advantage that it is built up gradually from the interpolation points one-by-one. If we stop the sum at any point, we have obtained the polynomial that interpolates the data for the first $k$ points (for some $k$).

What are the $A_k$'s? For $k = 0$, we know $A_0$ is the unique constant polynomial that interpolates the point at $x_0$, ie. $A_0 = f_0$.

For the others, we note that in the formula for $p_k - p_{k - 1}$, $A_k$ is the leading coefficient of $x^k$. Also, $p_{k - 1}(x)$ has no degree $k$ term. So $A_k$ must be the leading coefficient of $p_k$.

We thus need an efficient algorithm for these \emph{Newton divided differences}. The standard notation for these is
\[
  A_k = f[x_0, \cdots, x_k].
\]
Note in particular that these coefficients depend only on the first $k$ interpolation points.

We will look for a recurrence relation --- a way to express $A_k$ in terms of $A_j$ for $j < k$.

To do so, we first generalize this notion to $f[x_j, \cdots, x_k]$ for $0 \leq j \leq k$. This denotes the leading coefficient of the unique $q \in P_{k - j}[x]$ satisfying $q(x_i) = f_i$ for $i = j, \cdots, k$. Then we get
\begin{thm}[Recurrence relation for Newton divided differences]
  For $0 \leq j < k \leq n$, we have
  \[
    f[x_j, \cdots, x_k] = \frac{f[x_{j + 1}, \cdots, x_k] - f[x_j, \cdots, x_{k - 1}]}{x_k - x_j}.
  \]
\end{thm}

\begin{proof}
  The key to proving this is to relate the interpolating polynomials. Let $q_0, q_1 \in P_{k - j - 1}[x]$ and $q_2 \in P_{k - j}$ satisfy
  \begin{align*}
    q_0(x_i) &= f_i & i &=j, \cdots, k - 1\\
    q_1(x_i) &= f_i & i &=j + 1, \cdots, k\\
    q_2(x_i) &= f_i & i &=j, \cdots, k
  \end{align*}
  We now claim that
  \[
    q_2(x) = \frac{x - x_j}{x_k - x_j} q_1(x) + \frac{x_k - x}{x_k - x_j} q_0(x).
  \]
  Using the properties of $q_0$ and $q_1$, it is easy to show that the right expression satisfies the interpolations of $q_2$. Hence by uniqueness, we know that this is indeed $q_1$. The leading coefficients thus give the result.
\end{proof}
Thus the famous Newton divided difference table can be constructed
\begin{center}
  \begin{tabular}{cccccc}
    \toprule
    $x_i$ & $f_i$ & $f[*, *]$ & $f[*, *, *]$ & $\cdots$ & $f[*, \cdots,*]$\\
    \midrule
    $x_0$ & $f[x_0]$ & $f[x_0, x_1]$ & $f[x_0, x_1, x_2]$ & $\cdots$ & $f[x_0, x_1, \cdots, x_n]$\\
    $x_1$ & $f[x_1]$ & $f[x_1, x_2]$ & $f[x_1, x_2, x_3]$ & $\cdots$\\
    $x_2$ & $f[x_2]$ & $f[x_2, x_3]$ & $f[x_2, x_3, x_4]$ & $\cdots$\\
    $\vdots$ & $\vdots$ & $\vdots$ &\\
    $x_n$ & $f[x_n]$\\
    \bottomrule
  \end{tabular}
\end{center} % beautify
From the first $n$ columns, we can find the $n + 1$th column using the recurrence relation. The values of $A_k$ can then be found at the first row, and this is all we really need. However, to compute the first row, we will need to compute everything in the table.

Here we make two practical points:
\begin{enumerate}
  \item An efficient algorithm ($O(n)$ operations) for evaluating $p(\hat{x})$ at some new value $\hat{x}$ using the above divided difference table is known as \emph{Horner's scheme}:
    \begin{alltt}
      S <- f[\(\mathtt{x\sb{0}}\),..., \(\mathtt{x\sb{n}}\)]
      for k = n - 1,..., 0
          S <- (\(\mathtt{\hat{x}}\) - \(\mathtt{x\sb{k}}\))S + f[\(\mathtt{x\sb{0}}\),..., \(\mathtt{x\sb{k}}\)]
      end
    \end{alltt}
  \item If an extra data point $\{x_{n + 1}, f_{n + 1}\}$ is added, then we only have to compute an extra diagonal $f[x_k, \cdots, x_{n + 1}]$ for $k =n, \cdots, 0$ in the divided difference table to obtain the new coefficient, and the old results can be reused. This requires $O(n)$ operations. This is less straightforward for Lagrange's method.
\end{enumerate}

\subsection{A useful property of divided differences}
In the next couple of sections, we are interested in the error of polynomial interpolation. Suppose the data points come from $f_i = f(x_i)$ for some complicated $f$ we want to approximate. How does the error $e_n(x) = f(x) - p_n(x)$ (where $p_n$ is a degree $n$-interpolation) depend on $n$ and the choice of interpolation points?

Notice that the error is necessarily $0$ at the interpolation points, but this does not necessarily mean that the errors elsewhere will be small.

We first start with a lemma that is purely a calculus result:
\begin{lemma}
  Let $g \in C^m[a, b]$ have a continuous $m$th derivative. Suppose $g$ is zero at $m + \ell$ distinct points. Then $g^{(m)}$ has at least $\ell$ distinct zeros in $[a, b]$.
\end{lemma}

\begin{proof}
  This is a repeated application of Rolle's theorem. We know that between every two zeros of $g$, there is at least one zero of $g' \in C^{m - 1}[a, b]$. So by differentiating once, we have lost at most $1$ zeros. So after differentiating $m$ times, $g^{(m)}$ has lost at most $m$ zeroes. So it still has at least $\ell$ zeroes.
\end{proof}

\begin{thm}
  If $\{x_i\}_{i = 0}^n \in [a, b]$ and $f \in C^n[a, b]$. Then there exists some $\xi \in (a, b)$ such that
  \[
    f[x_0, \cdots, x_n] = \frac{1}{n!} f^{(n)}(\xi).
  \]
\end{thm}

\begin{proof}
  Consider $e = f - p_n \in C^n[a, b]$. This has at least $n + 1$ distinct zeroes in $[a, b]$. So by the lemma, $e^{(n)} = f^{(n)} - p_n^{(n)}$ must vanish at some $\xi \in (a, b)$. But then $p_n^{(n)} = n! f[x_0, \cdots, x_n]$ constantly. So the result follows.
\end{proof}
With this result, we can get error bounds rather easily.

\subsection{Error bounds for polynomial interpolation}
It turns out the error $e = f - p_n$ is ``like the next term in the Newton's formula''. This vague sentence is made precise in the following theorem:
\begin{thm}
  Assume $\{x_i\}_{i = 0}^n \subseteq [a, b]$ and $f \in C[a, b]$. Let $\bar{x} \in [a, b]$ be a non-interpolation point. Then
  \[
    e_n(\bar{x}) = f[x_0, x_1, \cdots, x_n, \bar{x}] \omega(\bar{x}),
  \]
  where
  \[
    \omega(x) = \prod_{i = 0}^n (x - x_i).
  \]
\end{thm}
Note that we forbid the case where $\bar{x}$ is an interpolation point, since it is not clear what the expression $f[x_0, x_1, \cdots, x_n, \bar{x}]$ means. However, if $\bar{x}$ is an interpolation point, then both $e_n(\bar x)$ and $\omega(\bar{x})$ are zero, so the statement is trivial.

\begin{proof}
  We think of $\bar{x} = x_{n + 1}$ as a new interpolation point so that
  \[
    p_{n + 1}(x) - p_n(x) = f[x_0, \cdots, x_n, \bar{x}] \omega(x)
  \]
  for all $x \in R$. In particular, putting $x = \bar{x}$, we have $p_{n + 1}(\bar{x}) = f(\bar{x})$, and we get the result.
\end{proof}

Combining the two results, we find
\begin{thm}
  If in addition $f \in C^{n + 1}[a, b]$, then for each $x \in [a, b]$, we can find $\xi_x \in (a, b)$ such that
  \[
    e_n(x) = \frac{1}{(n + 1)!} f^{(n + 1)}(\xi_x) \omega(x)
  \]
\end{thm}

\begin{proof}
  The statement is trivial if $x$ is an interpolation point --- pick arbitrary $\xi_x$, and both sides are zero. Otherwise, this follows directly from the last two theorems.
\end{proof}

This is an exact result, which is not too useful, since there is no easy constructive way of finding what $\xi_x$ should be. Instead, we usually go for a bound. We introduce the max norm
\[
  \|g\|_{\infty} = \max_{t \in [a, b]} |g(t)|.
\]
This gives the more useful bound
\begin{prop}
  For all $x \in [a, b]$, we have
  \[
    |f(x) - p_n(x)| \leq \frac{1}{(n + 1)!} \|f^{(n + 1)}\|_{\infty} |\omega(x)|
  \]
\end{prop}

We are now going to see what happens when we vary the interpolation points. The error bound only depends on the interpolation points through $\omega(x)$. So can we minimize $\omega(x)$ in some sense by picking some clever interpolation points $\Delta = \{x_i\}_{i = 0}^n$? Here we will have $n$ fixed. So instead, we put $\Delta$ as the subscript. We can write
\[
  \|f - p_{\Delta}\|_{\infty} \leq \frac{1}{(n + 1)!} \|f^{(n + 1)}\|_{\infty} \|\omega_{\Delta}\|_{\infty}.
\]
So the objective is to find a $\Delta$ that minimizes $\|\omega_{\Delta}\|_{\infty}$.

For the moment, we focus on the special case where the interval is $[-1, 1]$. Then later we can get the general solution by an easy change of variable. The solution involves the use of Chebyshev polynomials:
\begin{defi}[Chebyshev polynomial]
  The \emph{Chebyshev polynomial} of degree $n$ on $[-1, 1]$ is defined by
  \[
    T_n(x) = \cos(n \theta),
  \]
  where $x = \cos \theta$ with $\theta\in [0, \pi]$.
\end{defi}
So given an $x$, we find the unique $\theta$ that satisfies $x = \cos \theta$, and then find $\cos (n \theta)$. This is in fact a polynomial in disguise, since from trigonometric identities, we know $\cos (n\theta)$ can be expanded as a polynomial in $\cos \theta$ up to degree $n$.

Two key properties of $T_n$ on $[-1, 1]$ are
\begin{enumerate}
  \item The maximum absolute value is obtained at
    \[
      X_k = \cos\left(\frac{\pi k}{n}\right)
    \]
    for $k = 0, \cdots, n$ with
    \[
      T_n(X_k) = (-1)^k.
    \]
  \item This has $n$ distinct zeroes at
    \[
      x_k = \cos\left(\frac{2k - 1}{2n}\pi\right).
    \]
    for $k = 1, \cdots, n$.
\end{enumerate}
Notice there is an intentional clash between the use of $x_k$ as the zeroes and $x_k$ as the interpolation points --- we will show these are indeed the optimal interpolation points.
\begin{center}
  \begin{tikzpicture}[scale=2]
    \draw [->] (-1.5, 0) -- (1.5, 0) node [right] {$x$};
    \draw [->] (0, -1.5) -- (0, 1.5) node [above] {$T_4(x)$};

    \draw [mblue,domain=0:180, samples=40] plot [smooth] ({cos(\x)}, {cos(4*\x)});
    \draw [dashed] (-1, -1) rectangle (1, 1);
    \node at (-1, 0) [below] {$-1$};
    \node at (1, 0) [below] {$1$};
    \node at (0, 1) [anchor = south west] {$1$};
    \node at (0, -1) [left] {$-1$};
  \end{tikzpicture}
\end{center}
We first prove a recurrence relation for the Chebyshev polynomials:
\begin{lemma}[3-term recurrence relation]
  The Chebyshev polynomials satisfy the recurrence relations
  \[
    T_{n + 1}(x) = 2x T_n(x) - T_{n - 1}(x)
  \]
  with initial conditions
  \[
    T_0(x) = 1,\quad T_1(x) = x.
  \]
\end{lemma}
\begin{proof}
  \[
    \cos((n + 1) \theta) + \cos((n - 1)\theta) = 2\cos \theta \cos(n\theta).
  \]
\end{proof}
In particular, we know the leading coefficient of $T_n$ is $2^{n - 1}$ (for $n \geq 1$).

\begin{thm}[Minimal property for $n \geq 1$]
  On $[-1, 1]$, among all polynomials $p \in P_n[x]$ with leading coefficient $1$, $\frac{1}{2^{n - 1}} \|T_n\|$ minimizes $\|p\|_{\infty}$. Thus, the minimum value is $\frac{1}{2^{n - 1}}$.
\end{thm}

\begin{proof}
  We proceed by contradiction. Suppose there is a polynomial $q_n \in P_n$ with leading coefficient $1$ such that $\|q_n\|_{\infty} < \frac{1}{2^{n - 1}}$. Define a new polynomial
  \[
    r = \frac{1}{2^{n - 1}}T_n - q_n.
  \]
  This is, by assumption, non-zero.

  Since both the polynomials have leading coefficient $1$, the difference must have degree at most $n - 1$, ie. $r \in P_{n - 1}[x]$. Since $\frac{1}{2^{n - 1}}T_n(X_k) = \pm \frac{1}{2^{n - 1}}$, and $|q_n(X_n)| < \frac{1}{2^{n - 1}}$ by assumption, $r$ alternates in sign between these $n + 1$ points. But then by the intermediate value theorem, $r$ has to have at least $n$ zeroes. This is a contradiction, since $r$ has degree $n - 1$, and cannot be zero.
\end{proof}

\begin{cor}
  Consider
  \[
    w_\Delta = \prod_{i = 0}^n (x - x_i) \in P_{n + 1}[x]
  \]
  for any distinct points $\Delta = \{x_i\}_{i = 0}^n \subseteq [-1, 1]$. Then
  \[
    \min_{\Delta} \|\omega_{\Delta}\|_{\infty} = \frac{1}{2^n}.
  \]
  This minimum is achieved by picking the interpolation points to be the zeroes of $T_{n + 1}$, namely
  \[
    x_k = \cos\left(\frac{2k + 1}{2n + 2} \pi\right), \quad k = 0, \cdots, n.
  \]
\end{cor}

\begin{thm}
  For $f \in C^{n + 1}[-1, 1]$, the Chebyshev choice of interpolation points gives
  \[
    \|f - p_n\|_{\infty} \leq \frac{1}{2^n} \frac{1}{(n + 1)!} \|f^{(n + 1)}\|_{\infty}.
  \]
\end{thm}
Suppose $f$ has as many continuous derivatives as we want. Then as we increase $n$, what happens to the error bounds? The coefficients involve dividing by an exponential \emph{and} a factorial. Hence as long as the higher derivatives of $f$ don't blow up to badly, in general, the error will tend to zero as $n \to \infty$.

The last two results can be easily generalized to arbitrary intervals $[a, b]$, and this is left as an exercise for the reader.

\section{Orthogonal polynomials}
We are now going to look at general orthogonal polynomials, where the Chebyshev polynomials is a famous set of example.

Of course, the property of being orthogonal depends on our choice on the scalar product.

\subsection{Scalar product}
The scalar products we are interested in would be generalization of the usual scalar product on Euclidean space,
\[
  \bra \mathbf{x}, \mathbf{y}\ket = \sum_{i = 1}^n x_i y_i.
\]
We want to generalize this to vector spaces of functions and polynomials. We will not provide a formal definition of vector spaces and scalar products on an abstract vector space. Instead, we will just provide some examples of commonly used ones.
\begin{eg}\leavevmode
  \begin{enumerate}
    \item Let $V = C^s[a, b]$, where $[a, b]$ is a finite interval and $s \geq 0$. Pick a weight function $w(x) \in C(a, b)$ such that $w(x) > 0$ for all $x \in (a, b)$, and $w$ is integrable over $[a, b]$. In particular, we allow $w$ to vanish at the end points, or blow up mildly such that it is still integrable.

      We then define the inner product to be
      \[
        \bra f, g\ket = \int_a^b w(x) f(x) d(x)\;\d x.
      \]
    \item We can allow $[a, b]$ to be infinite, eg. $[0, \infty)$ or even $(-\infty, \infty)$, but we have to be more careful. We first define
      \[
        \bra f, g\ket = \int_a^b w(x) f(x) g(x) \;\d x
      \]
      as before, but we now need more conditions. We require that $\int_a^b w(x) x^n\;\d x$ to exist for all $n \geq 0$, since we want to allow polynomials in our vector space. For example, $w(x) = e^{-x}$ on $[0, \infty)$, works, or $w(x) = e^{-x^2}$ on $(-\infty, \infty)$. These are scalar products for $P_n[x]$ for $n \geq 0$, but we cannot extend this definition to all smooth functions since they might blow up too fast at infinity. We will not go into the technical details, since we are only interested in polynomials, and knowing it works for polynomials suffices.
    \item We can also have a discrete inner product, defined by
      \[
        \bra f, g\ket = \sum_{j = 1}^m w_j f(\xi_j) g(\xi_j)
      \]
      with $\{\xi_j\}_{j = 1}^m$ distinct points and $\{w_j\}_{j = 1}^m > 0$. Now we have to restrict ourselves a lot. This is a scalar product for $V = P_{m - 1}[x]$, but not for higher degrees, since a scalar product should satisfy $\bra f, f \ket > 0$ for $f \not= 0$. In particular, we cannot extend this to all smooth functions.
  \end{enumerate}
\end{eg}
With an inner product, we can define orthogonality.
\begin{defi}[Orthogonalilty]
  Given a vector space $V$ and an inner product $\bra \ph, \ph\ket$, two vectors $f, g \in V$ are \emph{orthogonal} if $\bra f, g\ket = 0$.
\end{defi}

\subsection{Orthogonal polynomials}
\begin{defi}[Orthogonal polynomial]
  Given a vector space $V$ of polynomials and inner product $\bra \ph, \ph\ket$, we say $p_n \in P_n[x]$ is the \emph{$n$th orthogonal polynomial} if
  \[
    \bra p_n, q\ket = 0\text{ for all }q \in P_{n - 1}[x].
  \]
  In particular, $\bra p_n, p_m\ket = 0$ for $n \not= m$.
\end{defi}

We said \emph{the} orthogonal polynomial, but we need to make sure such a polynomial has to be unique. It is clearly not unique, since if $p_n$ satisfies these relations, then so does $\lambda p_n$ for all $\lambda \not= 0$. For uniqueness, we need to impose some scaling. We usually do so by requiring the leading polynomial to be $1$, ie. it is monic.

\begin{defi}[Monic polynomial]
  A polynomial $p \in P_n[x]$ is \emph{monic} if the coefficient of $x^n$ is $1$.
\end{defi}
However, most famous traditional polynomials are not monic. They have a different scaling imposed. Still, picking a different scaling will not affect the uniqueness result.

We will stick to requiring them to be monic since this is useful for proving things.

\begin{thm}
  Given a vector space $V$ of functions and an inner product $\bra \ph, \ph \ket$, there exists a unique monic orthogonal polynomial for each degree $n \geq 0$. In addition, $\{p_k\}_{k = 0}^n$ form a basis for $P_n[x]$.
\end{thm}

\begin{proof}
  This is a big induction proof over both parts of the theorem. We induct over $n$. For the base case, we pick $p_0(x) = 1$, which is the only degree-zero monic polynomial.

  Now suppose we already have $\{p_n\}_{k = 0}^n$ satisfying the induction hypothesis.

  Now pick any monic $q_{n + 1} \in P_{n + 1}[x]$, eg. $x^{n + 1}$. We now construct $p_{n + 1}$ from $q_{n + 1}$ by the Gram-Schmidt process. We define
  \[
    p_{n + 1} = q_{n + 1} - \sum_{k = 0}^n \frac{\bra q_{n + 1}, p_k\ket}{\bra p_k, p_k\ket} p_k.
  \]
  This is again monic since $q_{n + 1}$ is, and we have
  \[
    \bra p_{n + 1}, p_m \ket = 0
  \]
  for all $m \leq n$, and hence $\bra p_{n + 1}, p\ket = 0$ for all $p \in P_n[x] = \bra p_0, \cdots,p_n\ket$.

  To obtain uniqueness, assume both $p_{n + 1}, \hat{p}_{n + 1} \in P_{n + 1}[x]$ are both monic orthogonal polynomials. Then $r = p_{n + 1} - \hat{p}_{n + 1} \in P_n[x]$. So
  \[
    \bra r, r\ket = \bra r, p_{n + 1} - \hat{p}_{n + 1}\ket = \bra r, p_{n + 1}\ket - \bra r, \hat{p}_{n + 1}\ket = 0 - 0 = 0.
  \]
  So $r = 0$. So $p_{n + 1} = \hat{p}_{n - 1}$.

  Finally, we have to show that $p_0, \cdots, p_{n + 1}$ form a basis for $P_{n + 1}[x]$. Now note that every $p \in P_{n + 1}[x]$ can be written uniquely as
  \[
    p = cp_{n + 1} + q,
  \]
  where $q \in P_n[x]$. But $\{p_k\}_{k = 0}^n$ is a basis for $P_n[x]$. So $q$ can be uniquely decomposed as a linear combination of $p_0, \cdots, p_n$.

  Alternatively, this follows from the fact that any set of orthogonal vectors must be linearly independent, and since there are $n + 2$ of these vectors and $P_{n + 1}[x]$ has dimension $n + 2$, they must be a basis.
\end{proof}

In practice, following the proof naively is not the best way of producing the new $p_{n + 1}$. Instead, we can reduce a lot of our work by making a clever choice of $q_{n + 1}$.

\subsection{Three-term recurrence relation}
Recall that for the Chebyshev polynomials, we obtained a three-term recurrence relation for them. It turns out these recurrence relations exist in general.

We start by picking $q_{n + 1} = xp_n$ in the previous proof. We now use the fact that
\[
  \bra xf, g\ket = f, xg\ket.
\]
This is not necessarily true for arbitrary inner products, but for most sensible inner products we will meet in this course, this is true. In particular, it is clearly true for inner products of the form
\[
  \bra f, g\ket = \int w(x) f(x) g(x)\;\d x.
\]
Assuming this, we obtain the following theorem.
\begin{thm}
  Monic orthogonal polynomials are generated by
  \[
    p_{k + 1}(x) = (x - \alpha_k)p_k(x) - \beta_k p_{k - 1}(x)
  \]
  with initial conditions
  \[
    p_0 = 1,\quad p_1(x) = (x - \alpha_0) p_0,
  \]
  where
  \[
    \alpha_k = \frac{\bra x p_k, p_k\ket}{\bra p_k, p_k\ket},\quad \beta_k = \frac{\bra p_k, p_k\ket}{\bra p_{k - 1}, p_{k - 1}\ket}.
  \]
\end{thm}

\begin{proof}
  By inspection, the $p_1$ given is monic and satisfies
  \[
    \bra p_1, p_0\ket = 0.
  \]
  Using $q_{n + 1} = x p_n$ in the Gram-Schmidt process gives
  \begin{align*}
    p_{n + 1} &= xp_n - \sum_{k = 0}^n \frac{\bra x p_n, p_k\ket}{\bra p_k, p_k\ket} p_k\\
    \intertext{We have $\bra xp_n, p_k\ket = \bra p_n, xp_k\ket$, and vanishes whenever $x p_k$ has degree less than $n$. So we are left with}
    &= (x - \alpha_n) p_n - \frac{\bra p_n, xp_{n - 1}\ket}{\bra p_{n - 1}, p_{n - 1}\ket} p_{n - 1}.
  \end{align*}
  Now we notice that $xp_{n - 1}$ is a monic polynomial of degree $n$ so we can write this as $x p_{n - 1} = p_n + q$. Thus
  \[
    \bra p_n, xp_{n - 1}\ket = \bra p_n, p_n + q\ket = \bra p_n, p_n\ket.
  \]
  Hence the coefficient of $p_{n - 1}$ is indeed the $\beta$ we defined.
\end{proof}

\subsection{Examples}
The four famous examples are the Legendre polynomials, Chebyshev polynomials, Laguerre polynomials and Hermite polynomials. We will not waste time looking at them. Instead, we will see how the Chebyshev polynomials fit into this framework.

Chebyshev is based on the scalar product defined by
\[
  \bra f, g\ket = \int_{-1}^1 \frac{1}{\sqrt{1 - x^2}} f(x) g(x)\;\d x.
\]
Note that the weight function blows up mildly at the end.

This links up with
\[
  T_n (x) = \cos(n\theta)
\]
for $x = \cos \theta$ via the usual trigonometric substitution. We have
\begin{align*}
  \bra T_n, T_m \ket &= \int_0^\pi \frac{1}{\sqrt{1 - \cos^2 \theta}} \cos(n\theta) \cos (m\theta) \sin \theta \;\d \theta\\
  &= \int_0^\pi \cos(n\theta) \cos (m\theta)\;\d \theta\\
  &= 0\text{ if }m\not= n.
\end{align*}
\subsection{Least-squares polynomial approximation}
This is an alternative to polynomial interpolation. Instead of trying to make a polynomial agree with $f$ at certain points and hoping it is a good approximation elsewhere, the idea is to choose a polynomial $p \in P_n[x]$ to minimize the error
\[
  \bra f - p, f - p\ket = \int_a^b w(x) [f(x) - p(x)]^2 \;\d x.
\]
We can also use alternative inner products such as
\[
  \bra f - p, f - p\ket = \sum_{j = 1}^m w_j [f(\xi_i) - p(\xi_i)]^2.
\]
Unlike polynomial interpolation, there is no guarantee that the approximation agrees with the function anywhere. Unlike polynomial interpolation, there is some guarantee that the total error is small (or as small as we can get, by definition).

The solution is to use orthogonal polynomials with respect to the corresponding inner products
\[
  \bra f, g\ket = \int_a^b w(x) f(x) g(x)\;\d x
\]
or
\[
  \bra f, g\ket = \sum_{j = 1}^m w_j f(\xi_j) g(\xi_j),
\]
in order to minimize
\[
  \|f - p\|^2 = \bra f - p, f - p\ket.
\]
\end{document}
