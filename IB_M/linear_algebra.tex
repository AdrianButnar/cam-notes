\documentclass[a4paper]{article}

\def\npart {IB}
\def\nterm {Michaelmas}
\def\nyear {2015}
\def\nlecturer {S. J. Wadsley}
\def\ncourse {Linear Algebra}
\def\nofficial{https://www.dpmms.cam.ac.uk/~sjw47/LinearAlgebraM15.pdf}
\def\nlectures{MWF.10}
\def\nnotready {}

\input{header}

\begin{document}
\maketitle
{\small
\noindent Definition of a vector space (over $\R$ or $\C$), subspaces, the space spanned by a subset. Linear independence, bases, dimension. Direct sums and complementary subspaces. \hspace*{\fill} [3]

\vspace{5pt}
\noindent Linear maps, isomorphisms. Relation between rank and nullity. The space of linear maps from $U$ to $V$, representation by matrices. Change of basis. Row rank and column rank.\hspace*{\fill} [4]

\vspace{5pt}
\noindent Determinant and trace of a square matrix. Determinant of a product of two matrices and of the inverse matrix. Determinant of an endomorphism. The adjugate matrix.\hspace*{\fill} [3]

\vspace{5pt}
\noindent Eigenvalues and eigenvectors. Diagonal and triangular forms. Characteristic and minimal polynomials. Cayley-Hamilton Theorem over $\C$. Algebraic and geometric multiplicity of eigenvalues. Statement and illustration of Jordan normal form.\hspace*{\fill} [4]

\vspace{5pt}
\noindent Dual of a finite-dimensional vector space, dual bases and maps. Matrix representation, rank and determinant of dual map.\hspace*{\fill} [2]

\vspace{5pt}
\noindent Bilinear forms. Matrix representation, change of basis. Symmetric forms and their link with quadratic forms. Diagonalisation of quadratic forms. Law of inertia, classification by rank and signature. Complex Hermitian forms.\hspace*{\fill} [4]

\vspace{5pt}
\noindent Inner product spaces, orthonormal sets, orthogonal projection, $V = W \oplus W^\bot$. Gram-Schmidt orthogonalisation. Adjoints. Diagonalisation of Hermitian matrices. Orthogonality of eigenvectors and properties of eigenvalues.\hspace*{\fill} [4]}

\tableofcontents
\setcounter{section}{-1}
\section{Introduction}
Linear algebra is the study of vector spaces and linear maps between vector spaces. Unlike IA vector and matrices, most of the time, we will not assume that we already have a fixed coordinate system. In particular, we no longer treat vectors as a ``list of numbers''. Instead, we will provide an axiomatic treatment of vector spaces and the maps.

\section{Definitions}
\subsection{Definitions and examples}
Intuitively, a vector space $V$ over a field $\F$ (or an $\F$-vector space) is a space with two operations:
\begin{itemize}
  \item We can add two vectors $\mathbf{v}_1, \mathbf{v}_2 \in V$ to obtain $\mathbf{v}_1 + \mathbf{v}_2 \in V$.
  \item We can multiply a scalar $\lambda \in \F$ with a vector $\mathbf{v}\in V$ to obtain $\lambda \mathbf{v} \in V$.
\end{itemize}

Of course, these two operations must satisfy certain axioms before we can call it a vector space. However, before going into these details, we first look at a few examples of vector spaces.

\begin{eg}\leavevmode
  \begin{enumerate}
    \item $\R^n = \{\text{column vectors of length }n\text{ with coefficients in }\R\}$ with the usual addition and scalar multiplication is a vector space.

      An $m\times n$ matrix $A$ with coefficients in $\R$ can be viewed as a linear map from $\R^m$ to $\R^n$ via $\mathbf{v} \mapsto A\mathbf{v}$.

      This is a motivational example for vector spaces. When confused about definitions, we can often think what the definition means in terms of $\R^n$ and matrices to get some intuition.

    \item Let $X$ be a set and define $\R^X = \{f: X\to \R\}$ with addition $(f + g)(x) = f(x) + g(x)$ and scalar multiplication $(\lambda f)(x) = \lambda f(x)$. This is a vector space.

      More generally, if $V$ is a vector space, $X$ is a set, we can define $V^X = \{f: X \to V\}$ with addition and scalar multiplication as above.
    \item Let $[a, b]\subseteq \R$ be a closed interval, then
      \[
        C([a, b], \R) = \{f\in \R^{[a,b]}: f\text{ is continuous}\}
      \]
      is a vector space, with operations as above. We also have
      \[
        C^{\infty}([a, b], \R) = \{f\in \R^{[a,b]}: f\text{ is infinitely differentiable}\}
      \]
    \item The set of $m\times n$ matrices with coefficients in $\R$ is a vector space, using the obvious operations, is a vector space.
  \end{enumerate}
\end{eg}

\begin{notation}
  We will use $\F$ to denote an arbitrary field, usually $\R$ or $\C$.
\end{notation}

We will now properly define a vector space, listing the axioms they are required to satisfy.
\begin{defi}[Vector space]
  An \emph{$\F$-vector space} is an (additive) abelian group $V$ together with a function $\F \times V \to V$, written $(\lambda, \mathbf{v}) \mapsto \lambda \mathbf{v}$, such that
  \begin{enumerate}
    \item $\lambda(\mu \mathbf{v}) = \lambda \mu \mathbf{v}$ for all $\lambda, \mu \in \F$, $\mathbf{v}\in V$ \hfill (associativity)
    \item $\lambda(\mathbf{u} + \mathbf{v}) = \lambda \mathbf{u} + \lambda \mathbf{v}$ for all $\lambda\in \F$, $\mathbf{u}, \mathbf{v}\in V$\hfill (distributivity in $V$)
    \item $(\lambda + \mu) \mathbf{v} = \lambda \mathbf{v} + \mu \mathbf{v}$ for all $\lambda, \mu \in \F$, $\mathbf{v}\in V$ \hfill (distributivity in $\F$)
    \item $1\mathbf{v} = \mathbf{v}$ for all $\mathbf{v}\in V$ \hfill (identity)
  \end{enumerate}

  We always write $\mathbf{0}$ for the identity in $V$, and call this the identity. By abuse of notation, we also write $0$ for the trivial vector space $\{0\}$.
\end{defi}
In a general vector space, there is no notion of ``coordinates'', length, angle or distance.

From the axioms, there are a few results we can immediately prove.
\begin{prop}
  In any vector space $V$, $0\mathbf{v} = \mathbf{0}$ for all $v\in V$, and $(-1)\mathbf{v} = -\mathbf{v}$, where $-\mathbf{v}$ is the additive inverse of $\mathbf{v}$.
\end{prop}
Proof is left as an exercise.

Similar to groups and subgroups, we can have a subspace of a vector space.
\begin{defi}[Subspace]
  If $V$ is an $\F$-vector space, then $U\subseteq V$ is an ($\F$-linear) \emph{subspace} if
  \begin{enumerate}
    \item $\mathbf{u}, \mathbf{v}\in U$ implies $\mathbf{u} + \mathbf{v} \in U$.
    \item $\mathbf{u}\in U, \lambda \in \F$ implies $\lambda u\in U$.
    \item $\mathbf{0}\in U$.
  \end{enumerate}
  These conditions can be expressed more concisely as ``$U$ is non-empty and if $\lambda, \mu\in \F, \mathbf{u}, \mathbf{v}\in U$, then $\lambda \mathbf{u} + \mu \mathbf{v}\in U$''. Alternatively, we can write the requirements as $U$ is also a vector space (inheriting the operations from $V$).

  We sometimes write $U\leq V$ is $U$ is a subspace of $V$.
\end{defi}

\begin{eg}\leavevmode
  \begin{enumerate}
    \item $\{(x_1, x_2, x_3) \in \R^3: x_1 + x_2 + x_3 = t\}$ is a subspace of $\R^3$ iff $t = 0$.
    \item Let $X$ be a set. We define the \emph{support} of $f$ in $\F^X$ to by $\supp(f) = \{x\in X: f(x) \not= 0\}$. Then the set of functions with finite support forms a vector subspace. This is since $\supp (f + g) \subseteq \supp(f) \cup \supp(g)$, $\supp (\lambda f) = \supp (f)$ (for $\lambda \not= 0$) and $\supp (0) = \emptyset$.
  \end{enumerate}
\end{eg}

If we have two subspaces $U$ and $V$, there are several things we can do with them. For example, we can take the intersection $U\cap V$. We will shortly show that this will be a subspace. However, taking the union will in general not produce a vector space. Instead, we need the sum:

\begin{defi}[Sum of subspaces]
  Suppose $U, W$ are subspaces of an $\F$ vector space $V$. The \emph{sum} of $U$ and $V$ is
  \[
    U + W = \{u + w: u\in U, w\in W\}.
  \]
\end{defi}

\begin{prop}
  Let $U, W$ be subspaces of $V$. Then $U + W$ and $U\cap W$ are subspaces.
\end{prop}

\begin{proof}
  Let $\mathbf{u}_i + \mathbf{w}_i \in U + W$, $\lambda, \mu\in \F$. Then
  \[
    \lambda(\mathbf{u}_1 + \mathbf{w}_1) + \mu(\mathbf{u}_2 + \mathbf{w}_2) = (\lambda\mathbf{u}_1 + \mu\mathbf{u}_2) + (\lambda\mathbf{w}_1 + \mu\mathbf{w}_2) \in U + W.
  \]
  Similarly, if $\mathbf{v}_i \in U\cap W$, then $\lambda \mathbf{v}_1 + \mu \mathbf{v}_2\in U$ and $\lambda \mathbf{v}_1 + \mu \mathbf{v}_2\in W$. So $\lambda \mathbf{v}_1 + \mu \mathbf{v}_2\in U\cap W$.

  Both $U\cap W$ and $U + W$ contain $\mathbf{0}$, and are non-empty. So done.
\end{proof}

\begin{defi}[Quotient spaces*]
  Suppose that $V$ is a vector space, and $U\subseteq V$ is a subspace. Then the quotient group $V/U$ can be made into a vector space called the \emph{quotient space}, where scalar multiplication is given by $(\lambda, \mathbf{v} + U) = (\lambda \mathbf{v}) + U$.

  This is well defined since if $\mathbf{v} + U = \mathbf{w} + U\in V/U$, then $\mathbf{v} - \mathbf{w} \in U$. Hence for $\lambda \in \F$, we have $\lambda \mathbf{v} - \lambda \mathbf{w} \in U$. So $\lambda \mathbf{v} + U = \lambda \mathbf{w} + U$.
\end{defi}
\subsection{Linear independence, bases and the Steinitz exchange lemma}

\begin{defi}[Span]
  If $V$ is an $\F$-vector space and $S\subseteq V$, then the \emph{span} of $S$ is
  \[
    \bra S\ket = \left\{\sum_{i = 1}^n \lambda_i S_i : \lambda_i \in \F, S_i \in S, n \geq 0\right\}
  \]
  This is the smallest subspace of $V$ containing $S$.

  Note that the sums must be finite. We will not play with infinite sums, since the notion of convergence is not even well defined in a general vector space.
\end{defi}

\begin{eg}\leavevmode
  \begin{enumerate}
    \item Let $V = \R^3$ and $S = \left\{\begin{pmatrix}1\\0\\0\end{pmatrix}, \begin{pmatrix}0\\1\\1\end{pmatrix}, \begin{pmatrix}1\\2\\2\end{pmatrix}\right\}$. Then
      \[
        \bra S\ket = \left\{
          \begin{pmatrix}
            a\\b\\b\\
          \end{pmatrix}: a, b\in \R
        \right\}.
      \]
      Note that any subset of $S$ of order 2 has the same span as $S$.
    \item Let $X$ be a set, $\delta x: X\to \F$ be the function
      \[
        \delta x(y) =
        \begin{cases}
          1 & y = x\\
          0 & y\not= x
        \end{cases}.
      \]
      Then $\bra \delta x: x \in X\ket$ is the set of all functions with finite support.
  \end{enumerate}
\end{eg}

\begin{defi}[Spanning set]
  Let $V$ be a vector space over $\F$ and $S\subseteq V$. $S$ \emph{spans} $V$ if $\bra S\ket = V$.
\end{defi}

\begin{defi}[Linear independence]
  Let $V$ be a vector space over $\F$ and $S\subseteq V$. Then $S$ is \emph{linearly independent} (LI) if whenever
  \[
    \sum_{i = 1}^n \lambda_i \mathbf{s}_i = 0\text{ with } \lambda_i \in \F, \mathbf{s}_1, \mathbf{s}_2, \cdots, \mathbf{s}_n \in S\text{ distinct},
  \]
  we must have $\lambda_i = 0$ for all $i$.

  If $S$ is not linearly independent, we say it is \emph{linearly dependent} (LD).
\end{defi}

\begin{defi}[Basis]
  Let $V$ be a vector space over $\F$ and $S\subseteq V$. Then $S$ is a \emph{basis} for $V$ if $S$ is linearly independent and spans $V$.
\end{defi}

\begin{defi}[Finite dimensional]
  A vector space is \emph{finite dimensional} if there is a finite basis.
\end{defi}
Ideally, we would want to define the \emph{dimension} as the number of vectors in the basis. However, we must first show that this is well-defined. It is certainly plausible that a vector space has a basis of size $7$ as well as a basis of size $3$. We must show that this can never happen, which is something we'll do soon.

We will first have an example:
\begin{eg}
  Again, let $V = \R^3$ and $S = \left\{\begin{pmatrix}1\\0\\0\end{pmatrix}, \begin{pmatrix}0\\1\\1\end{pmatrix}, \begin{pmatrix}1\\2\\2\end{pmatrix}\right\}$. Then $S$ is linearly dependent since
  \[
    1\begin{pmatrix}1\\0\\0\end{pmatrix} + 2\begin{pmatrix}0\\1\\1\end{pmatrix} + (-1) \begin{pmatrix}1\\2\\2\end{pmatrix} = \mathbf{0}.
  \]
  $S$ also does not span $V$ since $\begin{pmatrix}0\\0\\1\end{pmatrix}\not \in \bra S\ket$.
\end{eg}

Note that no linearly independent set can contain $\mathbf{0}$, as $1\cdot \mathbf{0} = \mathbf{0}$. We also have $\bra \emptyset\ket = \{\mathbf{0}\}$ and $\emptyset$ is a basis for this space.

There is an alternative way in which we can define linear independence.
\begin{lemma}
  $S\subseteq V$ is linearly dependent if and only if there are distinct $s_0, \cdots, s_n \in S$ and $\lambda_1, \cdots, \lambda_n\in \F$ such that
  \[
    \sum_{i = 1}^n \lambda_i s_i = s_0.
  \]
\end{lemma}

\begin{proof}
  If $S$ is linearly dependent, then there is some $\lambda_1, \cdots, \lambda_n \in \F$ all non-zero and $s_1,\cdots, s_n \in S$ such that $\sum \lambda_i s_i = 0$. Then
  \[
    s_1 = \sum_{i = 2}^n -\frac{\lambda_i}{\lambda_1} s_i.
  \]
  Conversely, if $s_0 = \sum_{i = 1}^n \lambda_i s_i$, then
  \[
    (-1)s_0 + \sum_{i = 1}^n \lambda_i s_i = 0.
  \]
\end{proof}

We also have an alternative characterization of what it means to be a basis:
\begin{prop}
  If $S = \{\mathbf{e}_1, \cdots, \mathbf{e}_n\}$ is a subset of $V$ over $\F$, then it is a basis if and only if every $\mathbf{v}\in V$ can be written uniquely as a finite linear combination of elements in $S$, ie. as
  \[
    \mathbf{v} = \sum_{i = 1}^n \lambda_i \mathbf{e}_i.
  \]
\end{prop}

\begin{proof}
  We can view this as a combination of two statements: it can be spanned in at least one way, and it can be spanned in at most one way. We will see that the first part corresponds to $S$ spanning $V$, and the second part corresponds to $S$ being linearly independent.

  In fact, $S$ spanning $V$ is defined exactly to mean that every item $\mathbf{v}\in V$ can be written as a finite linear combination in at least one way.

  Now suppose that $S$ is linearly independent, and we have
  \[
    \mathbf{v} = \sum_{i = 1}^n \lambda_i \mathbf{e}_i = \sum_{i = 1}\mu_i \mathbf{e}_i.
  \]
  Then we have
  \[
    \mathbf{0} = \mathbf{v} - \mathbf{v} = \sum_{i = 1}^n (\lambda_i - \mu_i) \mathbf{e}_i.
  \]
  Linear independence implies that $\lambda_i - \mu_i = 0$ for all $i$. Hence $\lambda_i = \mu_i$. So $\mathbf{v}$ can be expressed in a unique way.

  On the other hand, if $S$ is not linearly independent, then we have
  \[
    \mathbf{0} = \sum_{i = 1}^n \lambda_i \mathbf{e}_i
  \]
  where $\lambda_i \not= 0$ for some $i$. But we also know that
  \[
    \mathbf{0} = \sum_{i = 1}^n 0\cdot \mathbf{e}_i.
  \]
  So there are two ways to write $\mathbf{0}$ as a linear combination. So done.
\end{proof}

Now we come to the key theorem:
\begin{thm}[Steinitz exchange lemma]
  Let $V$ be an $\F$-vector space, and $S = \{\mathbf{e}_1, \cdots, \mathbf{e}_n\}$ a finite linearly independent subset of $V$, and $T$ a spanning subset of $V$. Then there is some $T'\subseteq T$ of order $n$ such that $(T\setminus T') \cup S$ still spans $V$. In particular, $|T| \geq n$.
\end{thm}
In some sense, the final remark is the most important part. It tells us that we cannot have a independent set larger than a spanning set.

This is sometimes stated in the following alternative way for $|T| < \infty$.
\begin{cor}
  If $\{\mathbf{e}_1, \cdots, \mathbf{e}_n\}$ is a linearly independent subset of $V$ and $\{\mathbf{f}_1, \cdots, \mathbf{f}_m\}$ spans $V$, then there is a re-ordering of the $\{\mathbf{f}_i\}$ such that $\{\mathbf{e}_1,\cdots, \mathbf{e}_n, \mathbf{f}_{n + 1}, \cdots, \mathbf{f}_m\}$ spans $V$.
\end{cor}

\begin{proof}
  We are going to prove this by finding an element in $T$ and replacing it with $\mathbf{e}_i$, for each $i$, one by one.

  Suppose that we have found $T_r'\subseteq T$ of order $0 \leq r < n$ such that
  \[
    T_r = (T\setminus T_r') \cup \{\mathbf{e}_1, \cdots, \mathbf{e}_r\}
  \]
  spans $V$.

  (Note that the case $r = 0$ is trivial, since we can take $T_r' = \emptyset$, and the case $r = n$ is the theorem which we want to achieve.)

  Suppose we have these. Since $T_r$ spans $V$, we can write
  \[
    \mathbf{e}_{r + 1} = \sum_{i = 1}^k \lambda_i \mathbf{t}_i,\quad \lambda_i \in \F, \mathbf{t}_i \in T_r.
  \]
  We know that the $\mathbf{e}_i$ are linearly independent, so not all $\mathbf{t}_i$'s are $\mathbf{e}_i$'s. So there is some $j$ such that $t_j \in (T\setminus T_r')$.  We can write this as
  \[
    \mathbf{t}_j = \frac{1}{\lambda_j} \mathbf{e}_{r + 1} + \sum_{i \not= j} -\frac{\lambda_i}{\lambda_j} \mathbf{t}_i.
  \]
  We let $T_{r + 1}' = T_r' \cup \{\mathbf{t}_j\}$ of order $r + 1$, and
  \[
    T_{r + 1} = (T\setminus T_{r + 1}') \cup \{\mathbf{e}_1, \cdots, \mathbf{e}_{r + 1}\} = (T_r \setminus \{\mathbf{t}_j\}\} \cup \{\mathbf{e}_{r + 1}\}
  \]
  Since $\mathbf{t}_j$ is in the span of $T_r\cup \{\mathbf{e}_{r + 1}\}$, we have $\mathbf{t}_j \in \bra T_{r + 1}\ket$. So
  \[
    V \supseteq \bra T_{r + 1}\ket \supseteq \bra T_r \ket = V.
  \]
  So $\bra T_{r + 1}\ket = V$.

  Hence we can inductively find $T_n$.
\end{proof}
Note that while we proved this for finite sets, if we assume the Axiom of Choice, we can prove the same for infinite sets of any cardinality by big scary things like Zorn's lemma and transfinite induction, but these are out of the scope of this course, and we needn't care.

\begin{cor}
  Suppose $V$ is a vector space over $\F$ with a basis of order $n$. Then
  \begin{enumerate}
    \item Every basis of $V$ has order $n$.
    \item Any linearly independent set of order $n$ is a basis.
    \item Every spanning set of order $n$ is a basis.
    \item Every finite spanning set contains a basis.
    \item Every linearly independent subset of $V$ can be extended to basis.
  \end{enumerate}
\end{cor}

\begin{proof}
  let $S = \{\mathbf{e}_1, \cdots, \mathbf{e}_n\}$ be the basis for $V$.
  \begin{enumerate}
    \item Suppose $T$ is another basis. Since $S$ is independent and $T$ is spanning, $|T| \geq |S|$. Since $T$ is linearly independent, every finite subset of $T$ is independent. Also, $S$ is spanning. So every finite subset of $T$ has order at most $|S|$. So $|T| \leq |S|$. So $|T| = |S|$.

    \item Suppose now that $T$ is a linearly independent subset of order $n$, but $\bra T\ket \not= V$. Then there is some $\mathbf{v} \in V\setminus \bra T\ket$. We now show that $T\cup \{\mathbf{v}\}$ is independent. Indeed, if
      \[
        \lambda_0 \mathbf{v} + \sum_{i = 1}^m \lambda_i \mathbf{t}_i = 0
      \]
      with $\lambda_i \in \F$, $\mathbf{t}_1, \cdots, \mathbf{t}_m\in T$ distinct, then
      \[
        \lambda_0 \mathbf{v} = \sum_{i = 1}^m (-\lambda_i) t_i.
      \]
      Then $\lambda_0 \mathbf{v} \in \bra T\ket$. So $\lambda_0= 0$. As $T$ is linearly independent, we have $\lambda_0 = \cdots = \lambda_m = 0$. So $T\cup \{\mathbf{v}\}$ is a linearly independent subset of size $> n$. This is a contradiction since $S$ is a spanning set of size $n$.

    \item Let $T$ be a spanning set of order $n$. if $T$ were linearly dependent, then there is some $\mathbf{t}_0, \cdots, \mathbf{t}_m \in T$ distinct and $\lambda_1, \cdots, \lambda_m \in \F$ such that
      \[
        \mathbf{t}_0 = \sum \lambda_i \mathbf{t}_i.
      \]
      So $\mathbf{t}_0 \in \bra T\setminus \{\mathbf{t}_0\}\ket$, ie. $\bra T\setminus \{\mathbf{t}_0\} \ket = V$. So $T\setminus \{\mathbf{t}_0\}$ is a spanning set of order $n - 1$, which is a contradiction.

    \item Suppose $T$ is any finite spanning set. Let $T' \subseteq T$ be a spanning set of least possible size. This exists because $T$ is finite. If $|T'|$ has size $n$, then done by (iii). Otherwise by the Steinitz exchange lemma, it has size $|T'| > n$. So $T'$ must be linearly dependent because $S$ is spanning.  So there is some $\mathbf{t}_0, \cdots, \mathbf{t}_m \in T$ distinct and $\lambda_1, \cdots, \lambda_m \in \F$ such that $\mathbf{t}_0 = \sum \lambda_i \mathbf{t}_i$. Then $T'\setminus \{\mathbf{t}_0\}$ is a smaller spanning set. Contradiction.

    \item Suppose $T$ is a linearly independent set. Since $S$ spans, there is some $S' \subseteq S$ of order $|T|$ such that $(S\setminus S')\cup T$ spans $V$ by the Steinitz exchange lemma. So by (ii), $(S\setminus S')\cup T$ is a basis of $V$ containing $T$.
  \end{enumerate}
\end{proof}
Finally, we can define dimension.
\begin{defi}[Dimension]
  If $V$ is a vector space $\F$ iwht finite basis $S$, then the \emph{dimension} of $V$, written
  \[
    \dim V = \dim_{\F}V = |S|.
  \]
\end{defi}
By the corollary, $\dim V$ does not depend on the choice of $S$. However, it does depend on $\F$. For example, $\dim_\C \C = 1$ (since $\{1\}$ is a basis), but $\dim_\R \C = 2$ (since $\{1, i\}$ is a basis).

After defining the dimension, we can prove a few things about dimensions.
\begin{lemma}
  If $V$ is a finite dimensional vector space over $\F$, $U\subseteq V$ is a proper subspace, then $U$ is finite dimensional and $\dim U < \dim $.
\end{lemma}

\begin{proof}
  Every linearly independent subset of $V$ has size at most $\dim V$. So let $S \subseteq U$ be a linearly independent subset of largest size. We want to show that $S$ spans $U$ and $|S| < \dim V$.

  If $\mathbf{v}\in V\setminus \bra S\ket$, then $S\cup \{\mathbf{v}\}$ is linearly independent. So $\mathbf{v}\not\in U$ by maximality of $S$. This means that $\bra S\ket = U$.

  Since $U\not= V$, there is some $\mathbf{v}\in V\setminus U = V\setminus \bra S\ket$. So $S\cup \{\mathbf{v}\}$ is a linearly independent subset of order $|S| + 1$. So $|S| + 1 \leq \dim V$. In particular, $\dim U = |S| < \dim V$.
\end{proof}

\begin{prop}
  If $U, W$ are subspaces of a finite dimensional vector space $V$, then
  \[
    \dim (U + W) = \dim U + \dim W - \dim (U\cap W).
  \]
\end{prop}
The proof is not hard, as long as we manage to pick the right basis to do the proof. This is our slogan:
\begin{center}
  When you choose a basis, always choose the right basis.
\end{center}
We need a basis for all four of them, and we want to compare the basis. So we want to pick bases that are compatible.

\begin{proof}
  let $R = \{\mathbf{v}_1, \cdots, \mathbf{v}_r\}$ be a basis for $U\cap W$. This is a linearly independent subset of $U$. So we can extend it to be a basis of $U$ by
  \[
    S = \{\mathbf{v}_1, \cdots, \mathbf{v}_r, \mathbf{u}_{r + 1}, \cdots, \mathbf{u}_s\}.
  \]
  Similarly, for $W$, we can obtain a basis
  \[
    T = \{\mathbf{v}_1, \cdots, \mathbf{v}_r, \mathbf{w}_{r + 1}, \cdots, \mathbf{w}_t\}.
  \]
  We want to show that $\dim (U + W) = s + t - r$. It is sufficient to prove that $S\cup T$ is a basis for $U + W$.

  We first show spanning. Suppose $\mathbf{u} + \mathbf{w} \in U + W$, $\mathbf{u}\in U, \mathbf{w}\in W$.  Then $\mathbf{u}\in \bra S\ket$ and $\mathbf{w}\in \bra T\ket$. So $\mathbf{u} + \mathbf{w} \in \bra S\cup T\ket$. So $U + W = \bra S \cup T\ket$.

  To show linear independence, suppose we have a linear relation
  \[
    \sum_{i = 1}^r \lambda_i \mathbf{v}_i + \sum_{j = r + 1}^s \mu_j \mathbf{u}_j + \sum_{k = r + 1}^t \nu_k \mathbf{w}_k = 0.
  \]
  So
  \[
    \sum \lambda_i \mathbf{v}_i + \sum \mu_j \mathbf{u}_j = - \sum \nu_k \mathbf{w}_k.
  \]
  Since the left hand side is something in $U$, and the right hand side is something in $W$, they both lie in $U\cap W$.

  Since $S$ is a basis of $U$, there is only one way of writing the left hand vector as a sum of $\mathbf{v}_i$ and $\mathbf{u}_j$. However, since $R$ is a basis of $U\cap W$, we can write the left hand vector just as a sum of $\mathbf{v}_i$'s. So we must have $\mu_j = 0$ for all $j$. Then we have
  \[
    \sum \lambda_i \mathbf{v}_i + \sum \nu_k \mathbf{w}_k = \mathbf{0}.
  \]
  Finally, since $T$ is linearly independent, $\lambda_i = \nu_k = 0$ for all $i, k$. So $S\cup T$ is linearly independent.
\end{proof}

\begin{prop}(non-examinable)
  If $V$ is a finite dimensional vector space $\F$ and $U\cup V$ is a subspace, then
  \[
    \dim V = \dim U + \dim V/U.
  \]
\end{prop}
We can view this as a linear algebra version of Lagrange's theorem. Combined with the first isomorphism theorem for vector spaces, this gives the rank-nullity theorem.

\begin{proof}
  Let $\{\mathbf{u}_1, \cdots, \mathbf{u}_m\}$ be a basis for $U$ and extend this to a basis $\{\mathbf{u}_1, \cdots, \mathbf{u}_m,\allowbreak \mathbf{v}_{m + 1}, \cdots, \mathbf{v}_n\}$ for $V$. We want to show that $\{\mathbf{v}_{m + 1} + U, \cdots, \mathbf{v}_n + U\}$ is a basis for $V/U$.

  It is easy to see that this spans $V/U$. If $v + U \in V/U$, then we can write
  \[
    \mathbf{v} = \sum \lambda_i \mathbf{u}_i + \sum \mu_i \mathbf{v}_i.
  \]
  Then
  \[
    \mathbf{v} + U = \sum \mu_i (\mathbf{v}_i + U) + \sum \lambda_i (\mathbf{u}_i + U) =  \sum \mu_i (\mathbf{v}_i + U).
  \]
  So done.

  To show that they are linearly independent, suppose that
  \[
    \sum \lambda_i (\mathbf{v}_i + U) = \mathbf{0} + U = U.
  \]
  Then this requires
  \[
    \sum \lambda_i \mathbf{v}_i \in U.
  \]
  Then we can write this as a linear combination of the $\mathbf{u}_i$'s. So
  \[
    \sum \lambda_i \mathbf{v}_i = \sum \mu_j \mathbf{u}_j
  \]
  for some $\mu_j$. Since $\{\mathbf{u}_1, \cdots, \mathbf{u}_m, \mathbf{v}_{n + 1}, \cdots, \mathbf{v}_n\}$ is a basis for $V$, we must have $\lambda_i = \mu_j = 0$ for all $i, j$. So $\{\mathbf{v}_i + U\}$ is linearly independent.
\end{proof}
\subsection{Direct sums}
We are going to define direct sums in many ways in order to confuse students.
\begin{defi}[(Internal) direct sum]
  Suppose $V$ is a vector space over $\F$ and $U, W\subseteq V$ are subspaces. We say that $V$ is the \emph{(internal) direct sum} of $U$ and $W$ if
  \begin{enumerate}
    \item $U + W = V$
    \item $U \cap W = 0$.
  \end{enumerate}
  We write $V = U\oplus W$.

  Equivalently, this requires that every $\mathbf{v}\in V$ can be written uniquely as $\mathbf{u} + \mathbf{w}$ with $\mathbf{u}\in U, \mathbf{w}\in W$. We say that $U$ and $W$ are \emph{complementary subspaces} of $V$.
\end{defi}

\begin{eg}
  Let $V = \R^2$, and $U = \bra \begin{pmatrix}0\\1\end{pmatrix}\ket$. Then $\bra \begin{pmatrix}1\\1\end{pmatrix}\ket$ and $\bra \begin{pmatrix}1\\0\end{pmatrix}\ket$ are both complementary subspaces to $U$ in $V$.
\end{eg}

\begin{defi}[(External) direct sum]
  If $U, W$ are vector spaces over $\F$, the \emph{(external) direct sum} is $U\oplus W = \{(\mathbf{u}, \mathbf{v}): \mathbf{u}\in U, \mathbf{w}\in W\}$, with addition and scalar multiplication componentwise:
  \[
    (\mathbf{u}_1, \mathbf{w}_1) + (\mathbf{u}_2, \mathbf{w}_2) = (\mathbf{u}_1 + \mathbf{u}_2, \mathbf{w}_1 + \mathbf{w}_2),\quad \lambda (\mathbf{u}, \mathbf{w}) = (\lambda \mathbf{u}, \lambda \mathbf{w}).
  \]
\end{defi}
The difference between these two definitions is that the first is decomposing $V$ into smaller spaces, while the second is building a bigger space based on two spaces.

Note, however, that the external direct sum $U\oplus W$ is the internal direct sum of $U$ and $W$ viewed as subspaces of $U\oplus W$, ie. as the internal direct sum of $\{(\mathbf{u}, \mathbf{0}): \mathbf{u}\in U\}$ and $\{(\mathbf{0}, \mathbf{v}): \mathbf{v}\in V\}$.

\begin{defi}[(Multiple) (internal) direct sum]
  If $U_1, \cdots, U_n\subseteq V$ are subspaces of $V$, then $V$ is the \emph{(internal) direct sum}
  \[
    V = U_1 \oplus \cdots \oplus U_n = \bigoplus_{i = 1}^n U_i,
  \]
  if every $\mathbf{v}\in V$ can be written uniquely as $\mathbf{v} = \sum \mathbf{u}_i$ with $\mathbf{u}_i \in U_i$.

  This can be extended to an infinite sum with the same definition, just noting that the sum $\mathbf{v} = \sum \mathbf{u}_i$ has to be finite.
\end{defi}
For more details, see Example Sheet 1 Q. 10, where we prove in particular that $\dim V = \sum \dim U_i$.

\begin{defi}[(Multiple) (external) direct sum]
  If $U_1, \cdots, U_n$ are vector spaces over $\F$, the external direct sum is
  \[
    U_1 \oplus \cdots \oplus U_n = \bigoplus_{i = 1}^n U_i = \{(\mathbf{u}_1, \cdots, \mathbf{u}_n): \mathbf{u}_i \in U_i\},
  \]
  with pointwise operations.

  This can be made into an infinite sum if we require that all but finitely many of the $\mathbf{u}_i$ have to be zero.
\end{defi}

\section{Linear maps}
\subsection{Definitions and examples}
\begin{defi}[Linear map]
  Let $U, V$ be vector spaces over $\F$. Then $\alpha: U\to V$ is a \emph{linear map} if
  \begin{enumerate}
    \item $\alpha(\mathbf{u}_1 + \mathbf{u}_2) = \alpha(\mathbf{u}_1) + \alpha(\mathbf{u}_2)$ for all $\mathbf{u}_i \in U$.
    \item $\alpha(\lambda \mathbf{u}) = \lambda \alpha (\mathbf{u})$ for all $\lambda \in \F, \mathbf{u}\in U$.
  \end{enumerate}
  We write $\mathcal{L}(U, V)$ for the set of linear maps $U\to V$.
\end{defi}
There are a few things we should take note of:
\begin{itemize}
  \item If we are lazy, we can combine the two requirements to the single requirement that
    \[
      \alpha (\lambda \mathbf{u}_1 + \mu \mathbf{u}_2) = \lambda \alpha(\mathbf{u}_1) + \mu \alpha(\mathbf{u}_2).
    \]
  \item It is easy to see that if $\alpha$ is linear, then it is a group homomorphism (if we view vector spaces as groups). In particular, $\alpha (\mathbf{0}) = \mathbf{0}$.
  \item If we want to stress the field $\F$, we say that $\alpha$ is $\F$-linear. For example, complex conjugation is a map $\C \to \C$ that is $\R$-linear but not $\C$-linear.
\end{itemize}

\begin{eg}\leavevmode
  \begin{enumerate}
    \item Let $A$ be an $n\times m$ matrix with coefficients in $\F$. We will write $A\in M_{n, m}(\F)$. Then $\alpha: \F^m \to \F^n$ defined by $\mathbf{v}\to A\mathbf{v}$ is linear.

      Recall matrix multiplication is is defined by: if $A_{ij}$ is the $ij$th coefficient of $A$, then the $i$th coefficient of $A\mathbf{v}$ is $A_{ij}\mathbf{v}_j$. So we have
      \begin{align*}
        \alpha(\lambda \mathbf{u} + \mu \mathbf{v})_i &= \sum_{j = 1}^m A_{ij}(\lambda \mathbf{u} + \mu \mathbf{v})_j \\
        &= \lambda \sum_{j = 1}^m A_{ij}u_j + \mu \sum_{j = 1}^m A_{ij} v_j \\
        &= \lambda \alpha(\mathbf{u})_i + \mu \alpha(\mathbf{v})_i.
      \end{align*}
      So $\alpha$ is linear.
    \item Let $X$ be a set and $g\in \F^X$. Then we define $m_g: \F^x \to \F^x$ by $m_g(f)(x) = g(x) f(x)$. Then $m_g$ is linear. For example, $f(x) \mapsto 2x^2 f(x)$ is linear.
    \item Integration $I: (C([a, b]), \R) \to (C([a, b]), \R)$ defined by $f\mapsto \int_a^x f(t) \;\d t$ is linear.
    \item Differentiation $D: (C^\infty ([a, b]), \R) \to (C^\infty ([a, b]), \R)$ by $ f\mapsto f'$ is linear.
    \item If $\alpha, \beta\in \mathcal{L}(U, V)$, then $\alpha + \beta$ defined by $(\alpha +  \beta)(\mathbf{u}) = \alpha(\mathbf{u}) + \beta(\mathbf{u})$ is linear.

      Also, if $\lambda \in \F$, then $\lambda \alpha$ defined by $(\lambda \alpha)(\mathbf{u}) = \lambda (\alpha (\mathbf{u}))$ is also linear.

      In this way, $\mathcal{L}(U, V)$ is also a vector space over $\F$.
    \item Composition of linear maps is linear. Using this, we can show that many things are linear, like differentiating twice, or adding and then multiplying linear maps.
  \end{enumerate}
\end{eg}

Just like everything else, we want to define isomorphisms.
\begin{defi}[Isomorphism]
  We say a linear map $\alpha: U\to V$ is an \emph{isomorphism} if there is some $\beta: V\to U$ (also linear) such that $\alpha \circ \beta = \id_V$ and $\beta\circ \alpha = \id_U$.

  If there exists an isomorphism $U\to V$, we say $U$ and $V$ are \emph{isomorphic}, and write $U\cong V$.
\end{defi}

\begin{lemma}
  If $U$ and $V$ are vector spaces over $\F$ and $\alpha: U\to V$, then $\alpha$ is an isomorphism iff $\alpha$ is a bijective linear map.
\end{lemma}

\begin{proof}
  If $\alpha$ is an isomorphism, then it is clearly bijective since it has an inverse function.

  Suppose $\alpha$ is a linear bijection. Then as a function, it has an inverse $\beta: V\to U$. We want to show that this is linear. Let $\mathbf{v}_1, \mathbf{v}_2 \in V$, $\lambda, \mu \in \F$. We have
  \[
    \alpha \beta(\lambda \mathbf{v}_1 + \mu \mathbf{v}_2) = \lambda \mathbf{v}_1 + \mu \mathbf{v}_2 = \lambda \alpha \beta (\mathbf{v}_1) + \mu \alpha \beta (\mathbf{v}_2) = \alpha (\lambda \beta(\mathbf{v}_1) + \mu \beta (\mathbf{v}_2)).
  \]
  Since $\alpha$ is injective, we have
  \[
    \beta(\lambda \mathbf{v}_1 + \mu \mathbf{v}_2) = \lambda \beta (\mathbf{v}_1) + \mu \beta (\mathbf{v}_2).
  \]
  So $\beta$ is linear.
\end{proof}

\begin{defi}[Image and kernel]
  Let $\alpha: U\to V$ be a linear map. Then the \emph{image} of $\alpha$ is
  \[
    \im \alpha = \{\alpha (\mathbf{u}): \mathbf{u}\in U\}.
  \]
  The \emph{kernel} of $\alpha$ is
  \[
    \ker \alpha = \{\mathbf{u}: \alpha (\mathbf{u}) = \mathbf{0}\}.
  \]
\end{defi}
It is easy to show that these are subspaces of $V$ and $U$ respectively.

\begin{eg}\leavevmode
  \begin{enumerate}
    \item Let $A\in M_{m, n}(\F)$ and $\alpha: \F^n \to \F^m$ be the linear map $\mathbf{v}\mapsto A\mathbf{v}$. Then the system of linear equations
      \[
        \sum_{j = 1}^m A_{ij}x_j = b_i,\quad 1 \leq i \leq n
      \]
      has a solution iff $(b_1, \cdots, b_n) \in \im \alpha$.

      The kernel of $\alpha$ contains all solution to $\sum_j A_{ij}x_j = 0$.
    \item Let $\beta: C^{\infty}(\R, \R) \to C^{\infty}(\R, \R)$ that sends
      \[
        \beta(f)(t) = f''(t) + p(t) f'(t) + q(t) f(t).
      \]
      for some $p, q\in C^{\infty}(\R, \R)$.

      Then if $y(t) \in \im \beta$, then there is a solution (in $C^\infty (\R, \R)$) to
      \[
        f''(t) + p(t) f'(t) + q(t) f(t) = y(t).
      \]
      Similarly, $\ker \beta$ contains the solutions to the homogeneous equation
      \[
        f''(t) + p(t) f'(t) + q(t) f(t) = 0.
      \]
  \end{enumerate}
\end{eg}

If two vector spaces are isomorphic, then do they have to have the same dimension? If they have the same dimension, do they have to be isomorphic? We will answer these questions now.
\begin{prop}
  Let $\alpha: U\to V$ be an $\F$-linear map. Then
  \begin{enumerate}
    \item If $\alpha$ is injective and $S\subseteq U$ is linearly independent, then $\alpha (S)$ is linearly independent in $V$.
    \item If $\alpha$ is surjective and $S\subseteq U$ spans $U$, then $\alpha (S)$ spans $V$.
    \item If $\alpha$ is isomorphic and $S\subseteq U$ is a basis, then $\alpha(S)$ is a basis for $V$.
  \end{enumerate}
\end{prop}
Here (iii) immediately shows that two isomorphic spaces have the same dimension.
\begin{proof}\leavevmode
  \begin{enumerate}
    \item We prove the contrapositive. Suppose that $\alpha$ is injective and $\alpha(S)$ is linearly dependent. So there are $\mathbf{s}_0, \cdots, \mathbf{s}_n \in S$ distinct and $\lambda_1, \cdots, \lambda_n\in \F$ not all zero such that
      \[
        \alpha(\mathbf{s}_0) = \sum_{i = 1}^n \lambda_i \alpha(\mathbf{s}_i) = \alpha\left(\sum_{i = 1}^n \lambda_i \mathbf{s}_i\right).
      \]
      Since $\alpha$ is injective, we must have
      \[
        \mathbf{s}_0 = \sum_{i = 1}^n \lambda_i \mathbf{s}_i.
      \]
      This is a non-trivial relation of the $\mathbf{s}_i$ in $U$. So $S$ is linearly dependent.
    \item Suppose $\alpha$ is surjective and $S$ spans $U$. Pick $\mathbf{v} \in V$. Then there is some $\mathbf{u}\in U$ such that $\alpha(\mathbf{u}) = \mathbf{v}$. Since $S$ spans $U$, there is some $\mathbf{s}_1, \cdots, \mathbf{s}_n\in S$ and $\lambda_1, \cdots, \lambda_n\in \F$ such that
      \[
        \mathbf{u} = \sum_{I = 1}^n \lambda_i \mathbf{s}_i.
      \]
      Then
      \[
        \mathbf{v} = \alpha (\mathbf{u}) = \sum_{i = 1}^n \lambda_i \alpha (\mathbf{s}_i).
      \]
      So $\alpha (S)$ spans $V$.
    \item Follows immediately from (i) and (ii).
  \end{enumerate}
\end{proof}

\begin{cor}
  If $U$ and $V$ are finite-dimensional vector spaces over $\F$ and $\alpha: U\to V$ is an isomorphism, then $\dim U = \dim V$.
\end{cor}
Note that we restrict it to finite-dimensional spaces since we've only shown that dimensions are well-defined for finite dimensional spaces. Otherwise, the proof works just finite for infinite dimensional spaces.

\begin{proof}
  Let $S$ be a basis for $U$. Then $\alpha(S)$ is a basis for $V$. Since $\alpha$ is injective, $|S| = |\alpha(S)|$. So done.
\end{proof}

How about the other way round? If two vector spaces have the same dimension, are they necessarily isomorphic? The answer is yes, at least for finite-dimensional ones.

However, we will not just prove that they are isomorphic. We will show that they are isomorphic in \emph{many ways}.
\begin{prop}
  Suppose $V$ is a $\F$-vector space of dimension $n < \infty$. Then writing $\mathbf{e}_1,\cdots, \mathbf{e}_n$ for the standard basis of $\F^n$, there is a bijection
  \[
    \Phi: \{\text{isomorphisms }\F^n \to V\} \to \{\text{(ordered) basis} (\mathbf{v}_1, \cdots, \mathbf{v}_n)\text{ for }V\},
  \]
  defined by
  \[
    \alpha \mapsto (\alpha (\mathbf{e}_1), \cdots, \alpha(\mathbf{e}_n)).
  \]
\end{prop}

\begin{proof}
  We first make sure this is indeed a function --- if $\alpha$ is an isomorphism, then from our previous proposition, we know that it sends a basis to a basis. So $(\alpha(\mathbf{e}_1), \cdots, \alpha(\mathbf{e}_n))$ is indeed a basis for $V$.

  We now have to prove surjectivity and injectivity.

  Suppose $\alpha, \beta: \F^n \to V$ are isomorphism such that $\Phi(\alpha) = \Phi(\beta)$. In other words, $\alpha (\mathbf{e}_i) = \beta(\mathbf{e}_i)$ for all $i$. We want to show that $\alpha = \beta$. We have
  \[
    \alpha\left(
    \begin{pmatrix}
      x_1\\\vdots\\x_n
    \end{pmatrix}
    \right) = \alpha \left(\sum_{i = 1}^n x_i \mathbf{e}_i\right) = \sum x_i \alpha (\mathbf{e}_i) = \sum x_i \beta (\mathbf{e}_i) = \beta\left(
    \begin{pmatrix}
      x_1\\\vdots\\x_n
    \end{pmatrix}\right).
  \]
  Hence $\alpha = \beta$.

  Next, suppose that $(\mathbf{v}_1, \cdots, \mathbf{v}_n)$ is an ordered basis for $V$. Then define
  \[
    \alpha\left(
    \begin{pmatrix}
      x_1\\\vdots\\x_n
    \end{pmatrix}
    \right) = \sum x_i \mathbf{v}_i.
  \]
  It is easy to check that this is well-defined and linear. We also know that $\alpha$ is injective since $(\mathbf{v}_1, \cdots, \mathbf{v}_n)$ is linearly independent. So if $\sum x_i \mathbf{v}_i = \sum y_i \mathbf{v}_i$, then $x_i = y_i$. Also, $\alpha$ is surjective since $(\mathbf{v}_1, \cdots, \mathbf{v}_n)$ spans $V$. So $\alpha$ is an isomorphism, and by construction $\Phi(\alpha) = (\mathbf{v}_1, \cdots, \mathbf{v}_n)$.
\end{proof}

\subsection{Linear maps and matrices}
We are going to look at the relationship between (abstract) linear maps and (concrete) matrices.
\begin{prop}
  Suppose $U, V$ are vector spaces over $\F$ and $S = \{\mathbf{e}_1, \cdots, \mathbf{e}_n\}$ is a basis for $U$. Then every function $f: S \to V$ extends uniquely to a linear map $U \to V$.
\end{prop}
The slogan is ``to define a linear map, it suffices to define its values on a basis''.

\begin{proof}
  For uniqueness, first suppose $\alpha, \beta: U \to V$ are linear and extend $f: S \to V$. We have sort-of proved this already just now.

  If $\mathbf{u}\in U$, we can write $\mathbf{u} = \sum_{i = 1}^n u_i \mathbf{e}_i$ with $u_i \in \F$ since $S$ spans. Then
  \[
    \alpha (\mathbf{u}) = \alpha\left(\sum u_i \mathbf{e}_i\right) = \sum u_i \alpha (\mathbf{e}_i) = \sum u_i f( \mathbf{e}_i).
  \]
  Similarly,
  \[
    \beta( \mathbf{u}) = \sum u_i f(\mathbf{e}_i).
  \]
  So $\alpha (\mathbf{u}) = \beta(\mathbf{u})$ for every $\mathbf{u}$. So $\alpha = \beta$.

  For existence, if $\mathbf{u} \in U$, we can write $\mathbf{u} = \sum u_i \mathbf{e}_i$ in a unique way. So defining
  \[
    \alpha(\mathbf{u}) = \sum u_i f(\mathbf{e}_i)
  \]
  is unambiguous. To show linearity, let $\lambda, \mu\in \F$, $\mathbf{u}, \mathbf{v}\in U$. Then
  \begin{align*}
    \alpha (\lambda \mathbf{u} + \mu \mathbf{v}) &= \alpha \left(\sum (\lambda u_i + \mu v_i) \mathbf{e}_i\right) \\
    &= \sum (\lambda u_i + \mu v_i) f(\mathbf{e}_i)\\
    &= \lambda \left(\sum u_i f(\mathbf{e}_i)\right) + \mu \left(\sum v_i f(\mathbf{e}_i)\right)\\
    &= \lambda \alpha(\mathbf{u}) + \mu \alpha(\mathbf{v}).
  \end{align*}
  Moreover, $\alpha$ does extend $f$.
\end{proof}

\begin{cor}
  If $U$ and $V$ are finite-dimensional vector spaces over $\F$ with bases $(\mathbf{e}_1, \cdots, \mathbf{e}_m)$ and $(\mathbf{f}_1, \cdots, \mathbf{f}_n)$ respectively, then there is a bijection
  \[
    \Mat_{n, m}(\F) \to \mathcal{L}(U, V),
  \]
  sending $A$ to the unique linear map $\alpha$ such that $\alpha(\mathbf{e}_i) =  \sum a_{ji} \mathbf{f}_j$.
\end{cor}
We can interpret this has follows: the $i$th column of $A$ tells us how to write $\alpha (\mathbf{e}_i)$ in terms of the $\mathbf{f}_j$.

We can also draw a fancy diagram. Given bases $\mathbf{e}_1, \cdots, \mathbf{e}_m$, by our bijection, we get an isomorphism $s(\mathbf{e}_i): U\to \F^m$. Similarly, we get an isomorphism $s(\mathbf{f}_i): V\to \F^n$.

Since a matrix is a linear map $A: \F^m \to \F^n$, given a matrix $A$, we can produce a linear map $\alpha: U\to V$ via the following composition
\[
  \begin{tikzcd}
    U \ar[r, "s(\mathbf{e}_i)"] & \F^m \ar[r, "A"] & \F^n \ar[r, "s(\mathbf{f}_i)^{-1}"] & V.
  \end{tikzcd}
\]
We can put this into a square:
\[
  \begin{tikzcd}[row sep=large]
    \F^m \ar[r, "A"] & \F^n\\
    U \ar[u, "s(\mathbf{e}_i)"] \ar[r, "\alpha"] & V \ar[u, "s(\mathbf{f}_i)"']
  \end{tikzcd}
\]
\begin{proof}
  If $\alpha$ is a linear map $U \to V$, then for each $1 \leq u \leq m$, we can write $\alpha(\mathbf{e}_i)$ uniquely as
  \[
    \alpha(\mathbf{e}_i) = \sum_{j = 1}^n a_{ji} \mathbf{f}_j
  \]
  for some $a_{ji} \in \F$. The previous proposition tells us that every matrix $A$ arises in this way, and $\alpha$ is determined by $A$.
\end{proof}

\begin{defi}[Matrix representing map]
  We call the matrix corresponding to a linear map $\alpha\in \mathcal{L}(U, V)$ under the corollary the \emph{matrix representing} $\alpha$ with respect to $(\mathbf{e}_1, \cdots, \mathbf{e}_m)$ and $(\mathbf{f}_1, \cdots, \mathbf{f}_n)$.
\end{defi}

It is an exercise to show that the bijection $\Mat_{n, m}(\F) \to \mathcal{L}(U, V)$ is an isomorphism of the vector spaces and deduce that $\dim \mathcal{L}(U, V) = (\dim U)(\dim V)$.

\begin{prop}
  Suppose $U, V, W$ are finite-dimensional vector spaces over $\F$ with bases $R = (\mathbf{u}_1, \cdots, \mathbf{u}_r)$ , $S = (\mathbf{v}_1, .., \mathbf{v}_2)$ and $T = (\mathbf{w}_1, \cdots, \mathbf{w}_t)$ respectively.

  If $\alpha: U\to V$ and $\beta: V\to W$ are linear represented by $A$ and $B$ respectively (with respect to $R$, $S$ and $T$), then $\beta\alpha$ is linear and represented by $BA$ with respect to $R$ and $T$.
\end{prop}
\[
  \begin{tikzcd}[row sep=large]
    \F^r \ar[r, "A"] & \F^s \ar [r, "B"] & \F^t\\
    U \ar[u, "s(R)"] \ar[r, "\alpha"] & V \ar[u, "s(S)"] \ar [r, "\beta"] & W \ar[u, "s(T)"']
  \end{tikzcd}
\]
\begin{proof}
  Verifying $\beta\alpha$ is linear is straightforward. Next we write $\beta\alpha(\mathbf{u}_i)$ as a linear combination of $\mathbf{w}_1, \cdots, \mathbf{w}_t$:
  \begin{align*}
    \beta\alpha(\mathbf{u}_i) &= \beta\left(\sum_k A_{ki}\mathbf{v}_k\right) \\
    &= \sum_k A_{ki}\beta(\mathbf{v}_k) \\
    &= \sum_k A_{ki}\sum_j B_{jk} \mathbf{w}_j \\
    &= \sum_j \left(\sum_k B_{jk}A_{ki}\right)\mathbf{w}_j\\
    &= \sum_j (BA)_{ji} \mathbf{w}_j
  \end{align*}
\end{proof}

\subsection{The first isomorphism theorem and the rank-nullity theorem}
We will start by stating the first isomorphism theorem, and deduce the rank-nullity theorem as an immediate consequence of it. The first isomorphism theorem is an exact analogy of that for groups.

\begin{thm}[First isomoprhism theorem]
  Let $\alpha: U\to V$ be a linear map. Then $\ker \alpha$ and $\im \alpha$ are subspaces of $U$ and $V$ respectively. Moreover, $\alpha$ induces an isomorphism
  \begin{align*}
    \bar{\alpha}: U/\ker \alpha &\to \im \alpha\\
    (\mathbf{u} + \ker \alpha) &\mapsto \alpha(\mathbf{u})
  \end{align*}
\end{thm}
Note that if we view a vector space as an abelian group, then this is exactly the first isomorphism theorem of groups.

\begin{proof}
  We know that $\mathbf{0} \in \ker \alpha$ and $\mathbf{0}\in \im \alpha$.

  Suppose $\mathbf{u}_1, \mathbf{u}_2 \in \ker \alpha$ and $\lambda_1, \lambda_2\in \F$. Then
  \[
    \alpha (\lambda_1 \mathbf{u}_1 + \lambda_2 \mathbf{u}_2) = \lambda_1 \alpha(\mathbf{u}_1) + \lambda_2 \alpha(\mathbf{u}_2)  = \mathbf{0}.
  \]
  So $\lambda_1 \mathbf{u}_1 + \lambda_2 \mathbf{u}_2 \in \ker \alpha$. So $\ker \alpha$ is a subspace.

  Similarly, if $\alpha(\mathbf{u}_1), \alpha(\mathbf{u}_2) \in \im \alpha$, then $\lambda\alpha(\mathbf{u}_1) + \lambda_2 \alpha(\mathbf{u}_2) = \alpha(\lambda_1 \mathbf{u}_1 + \lambda_2 \mathbf{u}_2) \in \im \alpha$. So $\im \alpha$ is a subspace.

  Now by the first isomorphism theorem of groups, $\bar{\alpha}$ is a well-defined isomorphism of groups. So it remains to observe
  \[
    \bar{\alpha}(\lambda(\mathbf{u} + \ker \alpha)) = \alpha (\lambda \mathbf{u}) = \lambda \alpha(\mathbf{u}) = \lambda (\bar{\alpha}(\mathbf{u} + \ker \alpha)).
  \]
  So $\bar {\alpha}$ is indeed a linear map.
\end{proof}

\begin{defi}[Rank and nullity]
  If $\alpha: U\to V$ is a linear map between finite-dimensional vector spaces over $\F$ (in fact we just need $U$ to be finite-dimensional), the \emph{rank} of $\alpha$ is the number $r(\alpha) = \dim \im \alpha$. The \emph{nullity} of $\alpha$ is the number $n(\alpha) = \dim \ker \alpha$.
\end{defi}

\begin{cor}[Rank-nullity theorem]
  If $\alpha: U \to V$ is a linear map and $U$ is finite-dimensional, then
  \[
    r(\alpha) + n(\alpha) = \dim U.
  \]
\end{cor}

\begin{proof}
  By the first isomorphism theorem, we know that $U/\ker \alpha \cong \im \alpha$, we know that $\dim \im \alpha = \dim (U/\ker \alpha) = \dim U - \dim \ker \alpha$. So the result follows.
\end{proof}

We can also prove this result without the first isomorphism theorem, and say a bit more in the mean time.
\begin{prop}
  If $\alpha: U\to V$ is a linear map between finite-dimensional vector spaces over $\F$, then there are bases $(\mathbf{e}_1, \cdots, \mathbf{e}_m)$ for $U$ and $(\mathbf{f}_1, \cdots, \mathbf{f}_n)$ for $V$ such that $\alpha$ is represented by the matrix
  \[
    \begin{pmatrix}
      I_r & 0\\
      0 & 0
    \end{pmatrix},
  \]
  where $r = r(\alpha)$ and $I_r$ is the $r\times r$ identity matrix.

  In particular, $r(\alpha) + n(\alpha) = \dim U$.
\end{prop}

\begin{proof}
  Let $\mathbf{e}_{k + 1}, \cdots, \mathbf{e}_m$ be a basis for the kernel of $\alpha$. Then we can extend this to a basis of the $(\mathbf{e}_1,\cdots, \mathbf{e}_m)$.

  Let $\mathbf{f}_i = \alpha(\mathbf{e}_i)$ for $1 \leq i \leq k$. We now show that $(\mathbf{f}_1, \cdots, \mathbf{f}_k)$ is a basis for $\im \alpha$ (and thus $k = r$). We first show that it spans. Suppose $\mathbf{v}\in \im \alpha$. Then we have
  \[
    \mathbf{v} = \alpha\left(\sum_{i = 1}^m \lambda_i \mathbf{e}_i\right)
  \]
  for some $\lambda_i \in \F$. By linearity, we can write this as
  \[
    \mathbf{v} = \sum_{i = 1}^m \lambda_i \alpha(\mathbf{e}_i) = \sum_{i = 1}^k \lambda_i \mathbf{f}_i + \mathbf{0}.
  \]
  So $\mathbf{v}\in \bra \mathbf{f}_1, \cdots, \mathbf{f}_k\ket$.

  To show linear dependence, suppose that
  \[
    \sum_{i = 1}^k \mu_i \mathbf{f}_i = 0.
  \]
  So we have
  \[
    \alpha \left(\sum_{i = 1}^k \mu_i \mathbf{e}_i\right) = \mathbf{0}.
  \]
  So $\sum_{i = 1}^k \mu_i \mathbf{e}_i \in \ker \alpha$. Since $(\mathbf{e}_{k + 1}, \cdots, \mathbf{e}_m)$ is a basis for $\ker \alpha$, we can write
  \[
    \sum_{i = 1}^k \mu_i \mathbf{e}_i = \sum_{i = k + 1}^m \mu_i \mathbf{e}_i
  \]
  for some $\mu_i$ ($i = k + 1, \cdots, m$). Since $(\mathbf{e}_1, \cdots, \mathbf{e}_m)$ is a basis, we must have $\mu_i = 0$ for all $i$. So they are linearly independent.

  Now we extend $(\mathbf{f}_1, \cdots, \mathbf{f}_r)$ to a basis for $V$, and
  \[
    \alpha(\mathbf{e}_i) =
    \begin{cases}
      \mathbf{f}_i & 1 \leq i \leq k\\
      0 & k + 1 \leq i \leq m.
    \end{cases}
  \]
\end{proof}

\begin{eg}
  Let
  \[
    W = \{x\in \R^5: x_1 + x_2 + x_3 = 0 = x_3 - x_4 - x_5\}.
  \]
  What is $\dim W$? Well, it clearly is $3$, but how can we prove it?

  We can consider the map $\alpha: \R^5 \to \R^2$ given by
  \[
    \begin{pmatrix}
      x_1\\\vdots \\ x_5
    \end{pmatrix}
    \mapsto
    \begin{pmatrix}
      x_1 + x_2 + x_5\\
      x_3 - x_4 - x_5.
    \end{pmatrix}
  \]
  Then $\ker \alpha = W$. So $\dim W = 5 - r(\alpha)$. We know that $\alpha(1, 0, 0, 0, 0) = (1, 0)$ and $\alpha(0, 0, 1, 0, 0) = (0, 1)$. So $r(\alpha) = \dim \im \alpha = 2$. So $\dim W = 3$.
\end{eg}
More generally, the rank-nullity theorem gives that $m$ linear equations in $n$ have a space of solutions of dimension at least $n - m$.

\begin{eg}
  Suppose that $U$ and $W$ are subspaces of $V$, all of which are finite-dimensional vector spaces of $\F$. We let
  \begin{align*}
    \alpha: U\oplus W &\to V\\
    (\mathbf{u}, \mathbf{w}) &\mapsto \mathbf{u} + \mathbf{w},
  \end{align*}
  where the $\oplus$ is the \emph{external} direct sum. Then $\im \alpha =U + W$ and
  \[
    \ker \alpha = \{(\mathbf{u}, -\mathbf{u}): \mathbf{u}\in U\cap W\} \simeq \dim W.
  \]
  Then we have
  \[
    \dim U + \dim W = \dim (U\oplus W) = r(\alpha) + n(\alpha) = \dim(U + W) + \dim (U\cap W).
  \]
  This is a result we've previously obtained through fiddling with basis and horrible stuff.
\end{eg}

\begin{cor}
  Suppose $\alpha: U\to V$ is a linear map betwee vector spaces over $\F$ both of dimension $n < \infty$. Then the following are equivalent
  \begin{enumerate}
    \item $\alpha$ is injective
    \item $\alpha$ is surjective
    \item $\alpha$ is an isomoprhism
  \end{enumerate}
\end{cor}

\begin{proof}
  It is clear that, (iii) implies (i) and (ii), and (i) and (ii) together implies (iii). So it suffices to show that (i) and (ii) are equivalent.

  Note that $\alpha$ is injective iff $n(\alpha) = 0$, and $\alpha$ is surjective iff $r(\alpha) = \dim V = n$. By the rank-nullity theorem, $n(\alpha) + r(\alpha) = n$. So the result follows immediately.
\end{proof}

\begin{lemma}
  Let $A \in M_{n, n}(\F) = M_n(\F)$ be a square matrix. The following are equivalent
  \begin{enumerate}
    \item There exists $B\in M_n (\F)$ such that $BA = I_n$.
    \item There exists $C\in M_n (\F)$ such that $AC = I_n$.
  \end{enumerate}
  If these hold, then $B = C$. We call $A$ \emph{invertible} or \emph{non-singular}, and write $A^{-1} = B = C$.
\end{lemma}

\begin{proof}
  Let $\alpha, \beta, \gamma, \iota: \F^n \to \F^n$ be the linear maps represented by matrices $A, B, C, I_n$ rrespectively with respect to the standard basis.

  We note that (i) is equivalent to saying that there exists $\beta$ such that $\beta\alpha = \iota$. This is true iff $\alpha$ is injective, which is true iff $\alpha$ is an isomoprhism, which is true iff $\alpha$ has an inverse $\alpha^{-1}$.

  Similarly, (ii) is equivalent ot saying that there exists $\gamma$ such that $\alpha\gamma = \iota$. This is truee iff $\alpha$ is injective, which is true iff $\alpha$ is isomoprhism, which is true iff $\alpha$ has an inverse $\alpha^{-1}$.

  So these are the same things, and we have $\beta = \alpha^{-1} = \gamma$.
\end{proof}

\subsection{Change of basis}
Suppose we have a linear map $\alpha: U \to V$. Given a basis $\{\mathbf{e}_i\}$ for $U$, and a basis $\{\mathbf{f}_i\}$ for $V$, we can obtain a matrix $A$.
\[
  \begin{tikzcd}[row sep=large]
    U  \ar[r, "\alpha"] & V\\
    \F^m \ar[r, "A"] \ar[u, "(\mathbf{e}_i)"] & \F^n \ar[u, "(\mathbf{f}_i)"']
  \end{tikzcd}
\]
We now want to consider what happens when we have two different basis $\{\mathbf{u}_i\}$ and $\{\mathbf{e}_i\}$ of $U$. These will then give rise to two different maps from $\F^m$ to our space $U$, and the two basis can be related by a change-of-basis map $P$. We can put them in the following diagram:
\[
  \begin{tikzcd}[row sep=large]
    U \ar[r, "\iota_U"] & U\\
    \F^m \ar[r, "P"] \ar[u, "(\mathbf{u}_i)"] & \F^m \ar[u, "(\mathbf{e}_i)"]
  \end{tikzcd}
\]
where $\iota_U$ is the identity map. If we perform a change of basis for both $U$ and $V$, we can stitch the diagrams together as
\[
  \begin{tikzcd}[row sep=large]
    U \ar[r, "\iota_U"] & U  \ar[r, "\alpha"] & V & V\ar[l, "\iota_V"']\\
    \F^m \ar[r, "P"] \ar[u, "(\mathbf{u}_i)"] \ar [rrr, bend right, "B"'] & \F^m \ar[r, "A"] \ar[u, "(\mathbf{e}_i)"] & \F^n \ar[u, "(\mathbf{f}_i)"] & \F^n \ar[l, "Q"'] \ar[u, "(\mathbf{v}_i)"]
  \end{tikzcd}
\]
Then if we want a matrix representing the map $U\to V$ with respect to bases $(\mathbf{u}_i)$ and $(\mathbf{v}_i)$, we can write it as the composition
\[
  B = Q^{-1}AP.
\]
We can write this as a theorem:
\begin{thm}[]
  Suppose that $(\mathbf{e}_1, \cdots, \mathbf{e}_m)$ and $(\mathbf{u}_1,\cdots, \mathbf{u}_m)$ are basis for a finite-dimensional vector space $U$ over $\F$, and $(\mathbf{f}_1, \cdots, \mathbf{f}_n)$ and $(\mathbf{v}_1, \cdots, \mathbf{v}_n)$ are basis of a finite-dimensional vector space $V$ over $\F$.

  Let $\alpha: U\to V$ be a linear map represented by a matrix $A$ with respect to $(\mathbf{e}_i)$ and $(\mathbf{f}_i)$ and by $B$ with respect to $(\mathbf{u}_i)$ and $(\mathbf{v}_i)$. Then
  \[
    B = Q^{-1}AP,
  \]
  where $P$ and $Q$ are given by
  \[
    \mathbf{u}_i = \sum_{k = 1}^m P_{ki}\mathbf{e}_k,\quad \mathbf{v}_i = \sum_{k = 1}^n Q_{ki}\mathbf{f}_k.
  \]
\end{thm}
Note that one can view $P$ as the matrix representing the identity map $i_U$ from $U$ with basis $(\mathbf{u}_i$) to $U$ with basis $(\mathbf{e}_i)$, and similarly for $Q$. So both are invertible.

\begin{proof}
  On the one hand, we have
  \[
    \alpha(\mathbf{u}_i) = \sum_{j = 1}^n B_{ji}\mathbf{v}_j = \sum_j\sum_\ell B_{ji} Q_{\ell j}\mathbf{f}_\ell = \sum_\ell [QB]_{\ell i}\mathbf{f}_\ell.
  \]
  On the other hand, we can write
  \[
    \alpha (\mathbf{u}_i) = \alpha \left(\sum_{k = 1}^m P_{ki}\mathbf{e}_k\right) = \sum_{k = 1}^m P_{ki} \sum_\ell A_{\ell k}\mathbf{f}_\ell = \sum_{\ell}[AP]_{\ell i} f_\ell.
  \]
  Since the $\mathbf{f}_\ell$ are linearly independent, we conclude that
  \[
    QB = AP.
  \]
  Since $Q$ is invertible, we get $B = Q^{-1}AP$.
\end{proof}

\begin{defi}[Equivalent matrices]
  We say $A, B\in \Mat_{n, m}(\F)$ are \emph{equivalent} if there are invertible matrices $P\in\Mat_{m}(\F)$, $Q\in \Mat_n (\F)$ such that $B = Q^{-1}AP$.
\end{defi}
Since $\GL_K(\F) = \{A \in \Mat_k(\F): A\text{ is invertible}\}$ is a group, for each $k \geq 1$, this is indeed an equivalence relation. The equivalence classes are orbits under the action of $\GL_m(\F) \times \GL_n(\F)$, given by
\begin{align*}
  \GL_m(\F) \times \GL_n(\F)\times \Mat_{n, m}(\F) &\to \Mat (\F)\\
  (P, Q, A) &\mapsto QAP^{-1}.
\end{align*}
Two matrices are equivalent if and only if they represent the same linear map with respect to different basis.

\begin{cor}
  If $A\in \Mat_{n, m}(\F)$, then there exists invertible matrices $P \in \GL_m(\F), Q\in \GL_n(\F)$ so that
  \[
    Q^{-1}AP =
    \begin{pmatrix}
      I_r & 0\\
      0 & 0
    \end{pmatrix}
  \]
  for some $0 \leq r \leq \min(m, n)$.
\end{cor}
This is just a rephrasing of the proposition we had last time. But this tells us there are $\min(m, n) + 1$ orbits of the action above parametrized by $r$.

\begin{defi}[Column and row rank]
  If $A\in \Mat_{n, m}(\F)$,  then
  \begin{itemize}
    \item The \emph{column rank} of $A$, written $r(A)$, is the dimension of the subspace of $\F^n$ spanned by the columns of $A$.
    \item The \emph{row rank} of $A$, written $r(A)$, is the dimension of the subspace of $\F^m$ spanned by the rows of $A$. Alternatively, it is the column rank of $A^T$.
  \end{itemize}
\end{defi}
There is no a priori reason why these should be equal to each other. However, it turns out they are always equal.

Note that if $\alpha: \F^m \to \F^n$ is the linear represented by $A$ (with respect to the standard basis), then $r(A) = r(\alpha)$, ie. the column rank is the rank. Moreover, since the rank of a map is independent of the basis, equivalent matrices have the same column rank.

\begin{thm}[]
  If $A \in \Mat_{n, m}(\F)$, then $r(A) = r(A^T)$, ie. the row rank is equivalent to the column rank.
\end{thm}

\begin{proof}
  We know that there are some invertible $P, Q$ such that
  \[
    Q^{-1}AP =
    \begin{pmatrix}
      I_r & 0\\
      0 & 0
    \end{pmatrix},
  \]
  where $r = r(A)$. We can transpose this whole equation to obtain
  \[
    (Q^{-1}AP)^T = P^T A^T (Q^T)^{-1} =
    \begin{pmatrix}
      I_r & 0\\
      0 & 0
    \end{pmatrix}
  \]
  So $r(A^T) = r$.
\end{proof}

\subsection{Elementary matrix operations}
We are now going to re-prove our corollary that we can find $P, Q$ such that $Q^{-1}AP = \begin{pmatrix} I_r & 0\\ 0 & 0 \end{pmatrix}$ in a way that involves matrices only. This will give a concrete way to find $P$ and $Q$, but is less elegant.

To do so, we need to introduce \emph{elementary matrices}.
\begin{defi}[Elementary matrices]
  We call the following matrices of $\GL_n(\F)$ \emph{elementary matrices}:
    \setcounter{MaxMatrixCols}{11}
  \[
    S_{ij}^n =
    \begin{pmatrix}
      1\\
      & \ddots\\
      & & 1\\
      & & & 0 & & & & 1\\
      & & & & 1\\
      & & & & & \ddots\\
      & & & & & & 1\\
      & & & 1 & & & & 0\\
      & & & & & & & & 1\\
      & & & & & & & & & \ddots\\
      & & & & & & & & & & 1
    \end{pmatrix}
  \]
This is called a reflection, where the rows we changed are the $i$th and $j$th row.
  \[
    E_{ij}^n (\lambda) =
    \begin{pmatrix}
      1 \\
      & \ddots \\
      & & 1 & & \lambda\\
      & & & \ddots\\
      & & & & 1\\
      & & & & & \ddots\\
      & & & & & & 1
    \end{pmatrix}
  \]
  This is called a shear, where $\lambda$ appears at the $i,j$th entry.
  \[
    T_{i}^n (\lambda) =
    \begin{pmatrix}
      1 \\
      & \ddots\\
      & & 1 \\
      & & & \lambda\\
      & & & & 1\\
      & & & & & \ddots\\
      & & & & & & 1
    \end{pmatrix}
  \]
  This is called a shear, where $\lambda\not= 0$ appears at the $i$th column.
\end{defi}
Observe that if $A$ is a $m\times n$ matrix, then
\begin{enumerate}
  \item $AS_{ij}^n$ is obtained from $A$ by swapping the $i$ and $j$ columns.
  \item $AE_{Ij}^n(\lambda)$ is obtained by adding $\lambda\times$ column $i$ to column $j$.
  \item $AT_i^n (\lambda)$ is obtained from $A$ by rescaling the $i$th column by $\lambda$.
\end{enumerate}
Multiplying on the left instead of the right would result in the same operations performed on the rows instead of the columns.

\begin{prop}
  If $A\in \Mat_{n, m}(\F)$, then there exists invertible matrices $P \in \GL_m(\F), Q\in \GL_n(\F)$ so that
  \[
    Q^{-1}AP =
    \begin{pmatrix}
      I_r & 0\\
      0 & 0
    \end{pmatrix}
  \]
  for some $0 \leq r \leq \min(m, n)$.
\end{prop}

We are going to start with $A$, and then apply these operations to get it into this form.

\begin{proof}
  We claim that there are elementary matrices $E_1^m, \cdots, E_a^m$ and $F_1^n, \cdots, F_b^n$ (these $E$ are not necessarily the shears, but but any elementary matrix) such that
  \[
    E_1^m \cdots E_a^m AF_1^n \cdots F_b^n =
    \begin{pmatrix}
      I_r & 0\\
      0 & 0
    \end{pmatrix}
  \]
  This suffices since the $E_i^m \in \GL_M(\F)$ and $F_j^n \in \GL_n(\F)$. Moreover, to prove the claim, it suffices to find a sequence of elementary row and column operations reducing $A$ to this form.

  If $A = 0$, then done. If not, there is some $i, j$ such that $A_{ij} \not= 0$. By swapping row $1$ and row $i$; and then column $1$ and column $j$, we can assume $A_{11} \not= 0$. By rescaling row $1$ by $\frac{1}{A_{11}}$, we can further assume $A_{11} = 1$.

  Now we can add $-A_{1j}$ times column $1$ to column $j$ for each $j \not= 1$, and then add $-A_{i1}$ times row $1$ to row $i \not= 1$.  Then we now have
  \[
    A =
    \begin{pmatrix}
      1 & 0 & \cdots & 0\\
      0 \\
      \vdots & & B\\
      0 &
    \end{pmatrix}
  \]
  Now $B$ is smaller than $A$. So by induction on the size of $A$, we can reduce $B$ to a matrix of the required form, so done.
\end{proof}
It is an exercise to show that the row and column operations do not change the row rank or column rank. So deduce that they are equal.

\section{Duality}
Duality is a principle we will find throughout mathematics. For example, in IB Optimisation, we considered the dual problems of linear programs. Here we will look for the dual of vector spaces. In general, we try to look at our question in a ``mirror'' and hope that the mirror problem is easier to solve than the original mirror.

\subsection{Dual space}
To specify a subspace of $\F^n$, we can write down linear equations that its elements satisfy. For example, if we have the subspace $U = \bra \begin{pmatrix} 1\\2\\1 \end{pmatrix}\ket\subseteq \F^3$, we can specify this by saying $\begin{pmatrix}x_1\\ x_2\\ x_3\end{pmatrix} \in U$ if and only if
\begin{align*}
  x_1 - x_3 &= 0\\
  2x_1 - x_2 &= 0.
\end{align*}
However, characterizing a space in terms of equations involves picking some particular equations out of the many possibilities. In general, we do not like making arbitrary choices. Hence the solution is to consider \emph{all} possible such equations. We will show that these form a subspace in some space.

We can interpret these equations in terms of linear maps $\F^n \to \F$. For example $x_1 = x_3 = 0$ if and only if $\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix} \in \ker \theta$, where $\theta: \F^3 \to \F$ is defined by $\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix} \mapsto x_1 - x_3$.

This works well with the vector space operations. If $\theta_1, \theta_2: \F^n \to \F$ vanish on some subspace of $\F^n$, and $\lambda, \mu\in \F$, then $\lambda \theta_1 + \mu \theta_2$ also vanishes on the subspace. So the set of all maps $\F^n \to \F$ that vanishes on $U$ forms a vector space.

To formalize this notion, we introduce dual spaces.

\begin{defi}[Dual space]
  Let $V$ be a vector space over $\F$. The \emph{dual} of $V$ is defined as
  \[
    V^* = \mathcal{L}(V, \F) = \{\theta: V \to \F: \theta\text{ linear}\}.
  \]
  Elements of $V^*$ are called \emph{linear functionals} or \emph{linear forms}.
\end{defi}
By convention, we use Roman letters for elements in $V$, and Greek letters for elements in $V^*$.

\begin{eg}\leavevmode
  \begin{itemize}
    \item If $V = \R^3$ and $\theta: V\to \R$ that sends $\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix} \mapsto x_1 - x_3$, then $\theta \in V^*$.
    \item Let $V = \F^X$. Then for any fixed $x$, $\theta: V\to \F$ defined by $f \mapsto f(x)$ is in $V^*$.
    \item Let $C([0, 1], \R)$. Then $f \mapsto \int_0^1 f(t)\;\d t \in V^*$.
    \item The trace $\tr: M_n(\F)\to \F$ defined by $A\mapsto \sum_{i = 1}^n A_{ii}$ is in $M_n(\F)^*$.
  \end{itemize}
\end{eg}

It turns out it is rather easy to specify how the dual space looks like, at least in the case where $V$ is finite dimensional.
\begin{lemma}
  If $V$ is a finite-dimensional vector space over $f$ with basis $(\mathbf{e}_1, \cdots, \mathbf{e}_n)$, then there is a basis $(\varepsilon_1, \cdots, \varepsilon_n)$  for $V^*$ (called the \emph{dual basis} to $(\mathbf{e}_1, \cdots, \mathbf{e}_n)$) such that
  \[
    \varepsilon_i(\mathbf{e}_j) = \delta_{ij}.
  \]
\end{lemma}

\begin{proof}
  Since linear maps are characterized by their values on a basis, there exists unique choices for $\varepsilon_1, \cdots, \varepsilon_n \in V^*$. Now we show that $(\varepsilon_1, \cdots, \varepsilon_n)$ is a basis.

  Suppose $\theta \in V^*$. We show that we can write it uniquely as a combination of $\varepsilon_1, \cdots, \varepsilon_n$. We have $\theta = \sum_{i = 1}^n \lambda_i \varepsilon_i$ if and only if $\theta(\mathbf{e}_j) = \sum_{i = 1}^n \varepsilon_i(\mathbf{e}_j)$ if and only if $\lambda_j = \theta(\mathbf{e}_j)$. So we have uniqueness and existence.
\end{proof}

\begin{cor}
  If $V$ is finite dimensional, then $\dim V = \dim V^*$.
\end{cor}
When $V$ is not finite dimensional, this need not be true. However, we know that the dimension of $V^*$ is at least as big as that of $V$, since the above gives a set of $\dim V$ many independent vectors in $V^*$.

It helps to come up with a more concrete example of how dual spaces look like. Consider the vector space $\F^n$, where we treat each element as a column vector (with respect to the standard basis). Then we can regard elements of $V^*$ as just row vectors $(a_1, \cdots, a_n) = \sum_{j = 1}^n a_j\varepsilon_j$ with respect to the dual basis. We have
\[
  \left(\sum a_j \varepsilon_j\right)\left(\sum_{x_i}\mathbf{e}_i\right) = \sum_{i, j} a_j x_i \delta_{ij} = \sum_{i = 1}^n a_i x_i =
  \begin{pmatrix}
    a_1 & \cdots & a_n
  \end{pmatrix}
  \begin{pmatrix}
    x_1\\\vdots\\x_n
  \end{pmatrix}.
\]
This is exactly what we want.

Now what happens when we change basis? How will the dual basis change?
\begin{prop}
  Let $V$ be a finite-dimensional vector space over $\F$ with bases $(\mathbf{e}_1, \cdots, \mathbf{e}_n)$ and $(\mathbf{f}_1, \cdots, \mathbf{f}_n)$, and that $P$ is the change of basis matrix so that
  \[
    \mathbf{f}_i = \sum_{k = 1}^n P_{ki}\mathbf{e}_k.
  \]
  Let $(\varepsilon_1, \cdots, \varepsilon_n)$ and $(\eta_1, \cdots, \eta_n)$ be the corresponding dual bases so that
  \[
    \varepsilon_i (\mathbf{e}_j) = \delta_{ij} = \eta_i (\mathbf{f}_j).
  \]
  Then the change of basis matrix from $(\varepsilon_1, \cdots, \varepsilon_n)$ to $(\eta_1, \cdots, \eta_n)$ is $(P^{-1})^T$, ie.
  \[
    \varepsilon_i = \sum_{\ell = 1}^n P_{\ell i}^T \eta_\ell.
  \]
\end{prop}

\begin{proof}
  For convenience, write $Q = P^{-1}$ so that
  \[
    \mathbf{e}_j = \sum_{k = 1}^n Q_{kj}\mathbf{f}_k.
  \]
  So we can compute
  \begin{align*}
    \left(\sum_{\ell = 1}^\infty P_{i\ell}\eta_\ell\right)(\mathbf{e}_j) &= \left(\sum_{\ell = 1}^\infty P_{i\ell}\eta_\ell\right)\left(\sum_{k = 1}^n Q_{kj}\mathbf{f}_k\right)\\
    &= \sum_{k, \ell} P_{i\ell}\delta_{\ell k} Q_{kj}\\
    &= \sum_{k, \ell} P_{i\ell} Q_{\ell j}\\
    &= [PQ]_{ij}\\
    &= \delta_{ij}.
  \end{align*}
  So $\varepsilon_i = \sum_{\ell = 1}^n P_{\ell i}^T \eta_\ell$.
\end{proof}

Now we'll return to our original motivation, and think how we can define subspaces of $V^*$ in terms of subspaces of $V$, and vice versa.

\begin{defi}[Annihilator]
  Let $U\subseteq V$. Then the \emph{annihilator} of $U$ is
  \[
    U^0 = \{\theta\in V^* : \theta(\mathbf{u}) = 0, \forall \mathbf{u}\in U\}.
  \]
  If $W \subseteq V^*$, then the \emph{annihilator} of $V$ is
  \[
    W^0 = \{\mathbf{v}\in V: \theta(\mathbf{v}) = 0,\forall \theta \in W\}.
  \]
\end{defi}
One might object that $W^0$ should be a subset of $V^{**}$ and not $V$. We will later show that there is a canonical isomorphism between $V^{**}$ and $V$, and this will all make sense.

\begin{eg}
  Consider $\R^3$ with standard basis $\mathbf{e}_1, \mathbf{e}_2, \mathbf{e}_3$; $(R^3)^*$ with dual basis $(\varepsilon_1, \varepsilon_2, \varepsilon_3)$. If $U = \bra \mathbf{e}_1 + 2\mathbf{e}_2 + \mathbf{e}_3\ket$ and $W = \bra \varepsilon_1 - \varepsilon_3, 2\varepsilon_1 - \varepsilon_2\ket $, then $U^0 = W$ and $W^0 = U$.
\end{eg}
We see that the dimension of $U$ and $U^0$ add up to three, which is the dimension of $\R^3$. This is typical.

\begin{prop}
  Let $V$ be a vector space over $\F$ and $U$ a subspace. Then
  \[
    \dim U + \dim U^0 = \dim V.
  \]
\end{prop}
We are going to prove this in many ways.
\begin{proof}
  Let $(\mathbf{e}_1, \cdots, \mathbf{e}_k)$ be a basis for $U$ and extend to $(\mathbf{e}_1, \cdots, \mathbf{e}_n)$ a basis for $V$. Consider the dual basis for $V^*$, say $(\varepsilon_1, \cdots, \varepsilon_n)$. Then we will show that
  \[
    U^0 = \bra \varepsilon_{k + 1}, \cdots,  \varepsilon_{n}\ket.
  \]
  So $\dim U^0 = n - k$ as required. This is easy to prove --- if $j > k$, then $\varepsilon_j(\mathbf{e}_i) = 0$ for all $ i \leq k$. So $\varepsilon_{k + 1}, \cdots, \varepsilon_n \in U^0$. On the other hand, suppose $\theta \in U^0$. Then we can write
  \[
    \theta = \sum_{j = 1}^n \lambda_j \varepsilon_j.
  \]
  But then $0 = \theta (\mathbf{e}_i) = \lambda_i$ for $i \leq k$. So done.
\end{proof}

\begin{proof}
  Consider the restriction map $V^* \to U^*$, given by $\theta \mapsto \theta|_U$. This is obviously linear. Since every linear map $U \to \F$ can be extended to $V\to \F$, this is a surjection. Moreover, the kernel is $U^0$. So by rank-nullity theorem,
  \[
    \dim V^* = \dim U^0 + \dim U^*.
  \]
  Since $\dim V^* = \dim V$ and $\dim U^* = \dim U$, we're done.
\end{proof}

\begin{proof}
  We can show that $U^0 \simeq (V/U)^*$, and then deduce the result. Details are left as an exercise.
\end{proof}

\subsection{Dual maps}
Since linear algebra is the study of vector spaces and linear maps between them, after dualizing vector spaces, we should be able to dualize linear maps as well. If we have a map $\alpha: V \to W$, then after dualizing, the map will go \emph{the other direction}, ie. $\alpha^*: W^* \to V^*$. This is a characteristic common to most dualization processes in mathematics.

\begin{defi}[Dual map]
  Let $V, W$ be vector spaces over $\F$ and $\alpha: V\to W \in \mathcal{L}(V, W)$. The \emph{dual map} to $\alpha$, written $\alpha^*: W^* \to V^*$ is given by $\theta \mapsto \theta \circ \alpha$. Since the composite of linear maps is linear, $\alpha^*(\theta) \in V^*$. So this is a genuine map.
\end{defi}

\begin{prop}
  Let $\alpha \in \mathcal{L}(V, w)$ be a linear map. Then $\alpha^* \in \mathcal{L}(W^*, V^*)$ is a linear map.
\end{prop}

\begin{proof}
  Let $\lambda, \mu \in \F$ and $\theta_1, \theta_2 \in W^*$. We want to show
  \[
    \alpha^*(\lambda \theta_1 + \mu \theta_2) = \lambda \alpha^*(\theta_1) + \mu \alpha^*(\theta_2).
  \]
  To show this, we show that for every $\mathbf{v} \in V$, the left and right give the same result. We have
  \begin{align*}
    \alpha^*(\lambda \theta_1 + \mu \theta_2)(\mathbf{v}) &= (\lambda \theta_1 + \mu \theta_2)(\alpha \mathbf{v}) \\
    &= \lambda \theta_1 (\alpha (\mathbf{v})) + \mu \theta_2 (\alpha(\mathbf{v})) \\
    &= (\lambda \alpha^*(\theta_1)+ \mu \alpha^*(\theta_2))(\mathbf{v}).
  \end{align*}
  So $\alpha^* \in \mathcal{L}(W^*, V^*)$.
\end{proof}

What happens to the matrices when we take the dual map? The answer is that we get the transpose.
\begin{prop}
  Let $V, W$ be finite-dimensional vector spaces over $\F$ and $\alpha: V\to W$ be a linear map. Let $(\mathbf{e}_1, \cdots, \mathbf{e}_n)$ be a basis for $V$ and $(\mathbf{f}_1, \cdots, \mathbf{f}_m)$ be a basis for $W$; $(\varepsilon_1, \cdots, \varepsilon_n)$ and $(\eta_1, \cdots, \eta_m)$ the corresponding dual bases.

  Suppose $\alpha$ is represented by $A$ with respect to $(\mathbf{e}_i)$ and $(\mathbf{f}_i)$ for $V$ and $W$. Then $\alpha^*$ is represented by $A^T$ with respect to the corresponding dual bases.
\end{prop}

\begin{proof}
  We are given that
  \[
    \alpha (\mathbf{e}_i) = \sum_{k = 1}^m A_{ki}\mathbf{f}_k.
  \]
  We must compute $\alpha^*(\eta_i)$. To do so, we evaluate it at $\mathbf{e}_j$. We have
  \[
    \alpha^*(\eta_i)(\mathbf{e}_j) = \eta_i(\alpha(\mathbf{e}_j)) = \eta_i\left(\sum_{k = 1}^m A_{kj}\mathbf{f}_k\right) = \sum_{k = 1}^m A_{kj} \delta_{ik} = A_{ij}.
  \]
  We can also write this as
  \[
    \alpha^*(\eta_i)(\mathbf{e}_j) = \sum_{k = 1}^n A_{ik} \varepsilon_k (\mathbf{e}_j).
  \]
  Since this is true for all $j$, we have
  \[
    \alpha^*(\eta_i) \sum_{k = 1}^n A_{ik}\varepsilon_k = \sum_{k = 1}^n A_{ki}^T \varepsilon_k.
  \]
  So done.
\end{proof}

Note that if $\alpha: U\to V$ and $\beta: V\to W$, $\theta \in W^*$, then
\[
  (\beta\alpha)^*(\theta) = \theta\beta\alpha = \alpha^*(\theta\beta) = \alpha^*(\beta^*(\theta)).
\]
So we have $(\beta\alpha)^* = \alpha^*\beta^*$. This is obviously true for the finite-dimensional case, since that's how transposes of matrices work.

Similarly, if $\alpha, \beta: U \to V$, then $(\lambda\alpha + \mu\beta)^* = \lambda\alpha^* + \mu\beta^*$.

What happens when we change basis? If $B = Q^{-1}AP$ for some invertible $P$ and $Q$, then
\[
  B^T = (Q^{-1}AP)^T = P^TA^T(Q^{-1})^{-1} = ((P^{-1})^T)^{-1} A^T (Q^{-1})^T,
\]
as expected.

As we said, we can use dualization to translate problems about a vector space to its dual. The following lemma gives us some good tools to do so:
\begin{lemma}
  Let $\alpha \in \mathcal{L}(V, W)$ with $V, W$ finite dimensional vector spaces over $\F$. Then
  \begin{enumerate}
    \item $\ker \alpha^* = (\im \alpha)^0$.
    \item $r(\alpha) = r(\alpha^*)$ (which is another proof that row rank is equal to column rank).
    \item $\im \alpha^* = (\ker \alpha)^0$.
  \end{enumerate}
\end{lemma}
At first sight, (i) and (iii) look quite similar. However, (i) is almost trivial to prove, but (iii) is rather hard.

\begin{proof}\leavevmode
  \begin{enumerate}
    \item If $\theta \in W^*$, then
      \begin{align*}
        \theta \in \ker \alpha^* &\Leftrightarrow \alpha^*(\theta) = 0 \\
        &\Leftrightarrow (\forall \mathbf{v}\in V)\; \theta \alpha(\mathbf{v}) = 0 \\
        &\Leftrightarrow (\forall \mathbf{w}\in \im \alpha)\; \theta(\mathbf{w}) = 0\\
        &\Leftrightarrow \theta \in (\im \alpha)^0
      \end{align*}
    \item As $\im \alpha \leq W$, we've seen that
      \[
        \dim \im \alpha + \dim (\im \alpha)^0 = \dim W.
      \]
      Using (i), we see
      \[
        n(\alpha^*) = \dim (\im \alpha)^0.
      \]
      So
      \[
        r(\alpha) + n(\alpha^*) = \dim W = \dim W^*.
      \]
      By the rank-nullity theorem, we have $r(\alpha) = r(\alpha^*)$.
    \item The proof in (i) doesn't quite work here. We can only show that one includes the other. To draw the conclusion, we will show that the two spaces have the dimensions, and hence must be equal.

      Let $\theta \in \im \alpha^*$. Then $\theta = \phi \alpha$ for some $\phi \in W^*$. If $\mathbf{v}\in \ker\alpha$, then
      \[
        \theta(\mathbf{v}) = \phi(\alpha(\mathbf{v})) = \phi(\mathbf{0}) = \mathbf{0}.
      \]
      So $\im \alpha^* \subseteq (\ker\alpha)^0$.

      But we know
      \[
        \dim (\ker \alpha)^0 + \dim \ker \alpha = \dim V,
      \]
      So we have
      \[
        \dim (\ker \alpha)^0 = \dim V - n(\alpha) = r(\alpha) = r(\alpha^*) = \dim \im \alpha^*.
      \]
      Hence we must have $\im \alpha^* = (\ker \alpha)^0$.
  \end{enumerate}
\end{proof}

Not only do we want to get from $V$ to $V^*$, we want to get back from $V^*$ to $V$. We can take the dual of $V^*$ to get a $V^{**}$. We know that $V^{**}$ is isomorphic to $V$, since they have the same dimension. However, there are many possible such isomorphisms. It turns out there is a ``canonical'' or ``natural'' isomorphism between the two.

\begin{lemma}
  Let $V$ be a vector space over $\F$. Then there is a canonical linear map $\ev: V\to (V^*)^*$ given by
  \[
    \ev (\mathbf{v})(\theta) = \theta(\mathbf{v}).
  \]
  We call this the \emph{evaluation} map.
\end{lemma}
We call this ``canonical'' since this does not require picking a particular basis of the vector spaces. It is in some sense a ``natural'' map.

\begin{proof}
  We first show that $\ev(\mathbf{v}) \in V^{**}$ for all $\mathbf{v}\in V$, ie. $\ev (\mathbf{v})$ is linear for any $\mathbf{v}$. For any $\lambda, \mu\in \F$, $\theta_1, \theta_2 \in V^*$, then for $\mathbf{v} \in V$, we have
  \begin{align*}
    \ev(\mathbf{v})(\lambda\theta_1 + \mu\theta_2) &= (\lambda\theta_1 + \mu\theta_2)(\mathbf{v}) \\
    &= \lambda\theta_1(\mathbf{v}) + \mu\theta_2(\mathbf{v}) \\
    &= \lambda\ev(\mathbf{v})(\theta_1) + \mu \ev(\mathbf{v})(\theta_2).
  \end{align*}
  So done. Now we show that $\ev$ itself is linear. Let $\lambda, \mu\in \F$, $\mathbf{v}_1, \mathbf{v}_2 \in V$. We want to show
  \[
    \ev(\lambda\mathbf{v}_1 + \mu \mathbf{v}_2) = \lambda \ev (\mathbf{v}_1) + \mu \ev(\mathbf{v}_2).
  \]
  To show these are equal, pick $\theta \in V^*$. Then
  \begin{align*}
    \ev(\lambda \mathbf{v}_1 + \mu \mathbf{v}_2)(\theta) &= \theta(\lambda\mathbf{v}_1 + \mu \mathbf{v}_2) \\
    &= \lambda\theta(\mathbf{v}_1) + \mu \theta(\mathbf{v}_2) \\
    &= \lambda \ev(\mathbf{v}_1)(\theta) + \mu \ev(\mathbf{v}_2)(\theta) \\
    &= (\lambda \ev(\mathbf{v}_1) + \mu \ev(\mathbf{v}_2))(\theta).
  \end{align*}
  So done.
\end{proof}
In the special case where $V$ is finite-dimensional, this is an isomorphism.
\begin{lemma}
  If $V$ is finite-dimensional, then $\ev: V \to V^{**}$ is an isomorphism.
\end{lemma}
This is very false for infinite dimensional spaces. In fact, this is true \emph{only} for finite-dimensional vector spaces (assuming the axiom of choice), and some (weird) people use this as the definition of finite-dimensional vector spaces.

\begin{proof}
  We first show it is injective. Suppose $\ev(\mathbf{v}) = \mathbf{0}$ for some $\mathbf{v}\in V$. Then $\theta (\mathbf{v}) = \ev (\mathbf{v})(\theta) = 0$ for all $\theta \in V^*$. So $\dim \bra \mathbf{v}\ket^0 = \dim V^* = \dim V$. So $\dim \bra \mathbf{v} \ket = 0$. So $\mathbf{v} = 0$. So $\ev$ is injective. Since $V$ and $V^{**}$ have the same dimension, this is also surjective. So done.
\end{proof}
From now on, we will just pretend that $V$ and $V^{**}$ are the same thing, at least when $V$ is finite dimensional.

Note that this lemma does not just say that $V$ is isomorphic to $V^{**}$ (we already know that since they have the same dimension). This says there is a completely canonical way to choose the isomorphism.

In general, if $V$ has a basis, then $\ev$ is injective, but not surjective. So we can think of $V$ as a subspace of $V^{**}$ in a canonical way.

\begin{lemma}
  Let $V, W$ be finite-dimensional vector spaces over $\F$ after identifying ($V$ and $V^{**}$) and ($W$ and $W^{**}$) by the evaluation map. Then we have
  \begin{enumerate}
    \item If $U\leq V$, then $U^{00} = U$.
    \item If $\alpha\in \mathcal{L}(V, W)$, then $\alpha^{**} = \alpha$.
  \end{enumerate}
\end{lemma}

\begin{proof}\leavevmode
  \begin{enumerate}
    \item Let $\mathbf{u} \in U$. Then $\mathbf{u}(\theta) = \theta(\mathbf{u}) = 0$ for all $\theta \in U^0$. So $\mathbf{u}$ annihilates everything in $U^0$. So $\mathbf{u} \in U^{00}$. So $U \subseteq U^{00}$. We also know that
      \[
        \dim U = \dim V  - \dim U^0 = \dim V - (\dim V - \dim U^{00}) = \dim U^{00}.
      \]
      So we must have $U = U^{00}$.
    \item Let $(\mathbf{e}_1, \cdots, \mathbf{e}_n)$ be a basis for $V$ and $(\mathbf{f}_1, \cdots, \mathbf{f}_m)$ be a basis for $W$, and let $(\varepsilon_1, \cdots, \varepsilon_n)$ and $(\eta_1, \cdots, \eta_n)$ be the corresponding dual basis. We know that
      \[
        \mathbf{e}_i(\varepsilon_j) = \delta_{ij} = \varepsilon_j(\mathbf{e}_i),\quad \mathbf{f}_i(\eta_j) = \delta_{ij} = \eta_j(\mathbf{f}_i).
      \]
      So $(\mathbf{e}_1, \cdots, \mathbf{e}_n)$ is dual to $(\varepsilon_1, \cdots, \varepsilon_n)$, and similarly for $\mathbf{f}$ and $\eta$.

      If $\alpha$ is represented by $A$, then $\alpha^*$ is represented by $A^T$. So $\alpha^{**}$ is represented by $(A^T)^T = A$. So done.
  \end{enumerate}
\end{proof}

\begin{prop}
  Let $V$ be a finite-dimensional vector space $\F$ and $U_1$, $U_2$ are subspaces of $V$. Then we have
  \begin{enumerate}
    \item $(U_1 + U_2)^0 = U_1^0 \cap U_2^0$
    \item $(U_1 \cap U_2)^0 = U_1^0 + U_2^0$
  \end{enumerate}
\end{prop}

\begin{proof}\leavevmode
  \begin{enumerate}
    \item Suppose $\theta \in V^*$. Then
      \begin{align*}
        \theta \in (U_1 + U_2)^0 &\Leftrightarrow \theta (\mathbf{u}_1 + \mathbf{u}_2) = 0\text{ for all }\mathbf{u}_i \in U_i\\
        &\Leftrightarrow \theta (\mathbf{u}) = 0\text{ for all }\mathbf{u} \in U\cup U_2\\
        &\Leftrightarrow \theta \in U_1^0 \cap U_2^0.
      \end{align*}
    \item We have
      \[
        (U_1 \cap U_2)^0 = ((U_1^0)^0 \cap (U_2^0)^0)^0 = (U_1^0 + U_2^0)^{00} = U_1^0 + U_2^0.
      \]
      So done.
  \end{enumerate}
\end{proof}

\section{Bilinear forms (I)}
\begin{defi}[Bilinear form]
  Let $V, W$ be vector spaces over $\F$. Then a function $\phi: V\times W \to \F$ is a \emph{bilinear form} if it is linear in each variable, ie. for each $\mathbf{v} \in V$, $\phi(\mathbf{v}, \ph): W \to \F$ is linear; for each $\mathbf{w} \in W$, $\phi(\ph, \mathbf{w}): V\to \F$ is linear.
\end{defi}

\begin{eg}
  The map defined by
  \begin{align*}
    V\times V^* &\to \F\\
    (\mathbf{v}, \theta) &\mapsto  \theta(\mathbf{v}) = \ev(\mathbf{v})(\theta)
  \end{align*}
  is a bilinear form.
\end{eg}

\begin{eg}
  Let $V = W = \F^n$. Then the function $(\mathbf{v}, \mathbf{w}) = \sum_{i = 1}^n v_i w_i$ is bilinear.
\end{eg}

\begin{eg}
  If $V = W = C([0, 1], \R)$, then
  \[
    (f, g) \mapsto \int_0^a fg \;\d t
  \]
  is a bilinear form.
\end{eg}

\begin{eg}
  Let $A \in \Mat_{m, n}(\F)$. Then
  \begin{align*}
    \phi: \F^m \times \F^n &\to \F\\
       (\mathbf{v}, \mathbf{w}) &\mapsto \mathbf{v}^T A\mathbf{w}
  \end{align*}
  is bilinear. Note that the (real) dot product is the special case of this, where $n = m$ and $A = I$.
\end{eg}
In fact, this is the most general form of bilinear forms on finite-dimensional vector spaces.

\begin{defi}[Matrix representing bilinear form]
  Let $(\mathbf{e}_1, \cdots, \mathbf{e}_n)$ be a basis for $V$ and $(\mathbf{f}_1, \cdots, \mathbf{f}_m)$ be a basis for $W$, and $\psi: V\times W \to \F$. Then the \emph{matrix $A$ representing $\psi$} with respect to the basis is defined to be
  \[
    A_{ij} = \psi(\mathbf{e}_i, \mathbf{f}_j).
  \]
\end{defi}
Note that if $\mathbf{v} = \sum \lambda_i \mathbf{e}_i$ and $\mathbf{w} = \sum \mu_j \mathbf{f}_j$, then by linearity, we get
\begin{align*}
  \psi(\mathbf{v}, \mathbf{w}) &= \psi\left(\sum \lambda_i \mathbf{e}_i, \mathbf{w}\right) \\
  &= \sum_i \lambda_i \psi(\mathbf{e}_i, \mathbf{w})\\
  &= \sum_i \lambda_i \psi\left(\mathbf{e}_i, \sum \mu_j \mathbf{f}_j\right)\\
  &= \sum_{i, j} \lambda_i \mu_j \psi(\mathbf{e}_i, \mathbf{f}_j)\\
  &= \lambda^T A \mu.
\end{align*}
So $\psi$ is determined by $A$.

We have identified linear maps with matrices, and we have identified bilinear maps with matrices. However, you shouldn't think linear maps are bilinear maps. They are, obviously, two different things. In fact, the matrices representing matrices and bilinear forms transform differently when we change basis.

\begin{prop}
  Suppose $(\mathbf{e}_1, \cdots, \mathbf{e}_n)$ and $(\mathbf{v}_1, \cdots, \mathbf{v}_n)$ are basis for $V$ such that
  \[
    \mathbf{v}_i = \sum P_{ki}\mathbf{e}_k\text{ for all }i = 1,\cdots, n;
  \]
  and $(\mathbf{f}_1, \cdots, \mathbf{f}_m)$ and $(\mathbf{w}_1, \cdots, \mathbf{w}_m)$ are bases for $W$ such that
  \[
    \mathbf{w}_i = \sum Q_{\ell j} \mathbf{f}_\ell\text{ for all }j = 1, \cdots, m.
  \]
  Let $\psi: V\times W \to \F$ be a bilinear form represented by $A$ with respect to $(\mathbf{e}_1, \cdots, \mathbf{e}_n)$, and $(\mathbf{f}_1, \cdots, \mathbf{f}_m)$ by $B$ with respect to $(\mathbf{v}_1, \cdots, \mathbf{v}_n)$ and $\mathbf{w}_1, \cdots, \mathbf{w}_n)$. Then
  \[
    B = P^T AQ.
  \]
\end{prop}
The difference with the transformation laws of matrices is this time we are taking \emph{transposes}, not \emph{inverses}.

\begin{proof}
  We have
  \begin{align*}
    B_{ij} &= \phi(\mathbf{v}_i, \mathbf{w}_j)\\
    &= \phi\left(\sum P_{ki}\mathbf{e}_k, \sum Q_{\ell j}\mathbf{f}_\ell\right)\\
    &= \sum P_{ki}Q_{\ell j}\phi(\mathbf{e}_k, \mathbf{f}_\ell)\\
    &= \sum_{k, \ell} P^T_{ik} A_{k\ell} Q_{\ell j}\\
    &= (P^T AQ)_{ij}.
  \end{align*}
\end{proof}
Note that while the transformation laws for bilinear forms and linear maps are different, we still get that two matrices are representing the same bilinear form with respect to different bases if and only if they are equivalent, since if $B = P^T AQ$, then $B = ((P^{-1})^T)^T AQ$.

If we are given a bilinear form $\psi: V\times W \to \F$, we immediately get two linear maps:
\[
  \psi_L: V\to W^*,\quad \psi_R: W \to V^*,
\]
defined by $\psi_L(\mathbf{v})(\mathbf{w}) = \psi(\mathbf{v}, \mathbf{w}) = \psi_R(\mathbf{w})(\mathbf{v})$.

For example, if $\psi: V\times V^* \to \F$, is defined by $(\mathbf{v}, \theta) \mapsto \theta(\mathbf{v})$, then $\psi_L: V\to V^{**}$ is the evaluation map. On the other hand, $\psi_R: V^* \to V^*$ is the identity map.

\begin{lemma}
  Let $(\varepsilon_1,\cdots, \varepsilon_n)$ be a basis for $V^*$ dual to $(\mathbf{e}_1, \cdots, \mathbf{e}_n)$ of $V$; $(\eta_1,\cdots, \eta_n)$ be a basis for $W^*$ dual to $(\mathbf{f}_1, \cdots, \mathbf{f}_n)$ of $W$.

  If $A$ represents $\psi$ with respect to $(\mathbf{e}_1, \cdots, \mathbf{e}_n)$ and $(\mathbf{f}_1, \cdots, \mathbf{f}_m)$, then $A$ also represents $\psi_R$ with respect to $(\mathbf{f}_1,\cdots, \mathbf{f}_m)$ and $(\varepsilon_1, \cdots, \varepsilon_n)$; and $A^T$ represents $\psi_L$ with respect to $(\mathbf{e}_1, \cdots, \mathbf{e}_n)$ and $(\eta_1, \cdots, \eta_m)$.
\end{lemma}

\begin{proof}
  We just have to compute
  \[
    \psi_L(\mathbf{e}_i)(\mathbf{e}_j) = A_{ij} = \left(\sum A_{i\ell} \eta_\ell\right) (\mathbf{f}_j).
  \]
  So we get
  \[
    \psi_L(\mathbf{e}_i) = \sum A_{\ell i}^T\eta_\ell.
  \]
  So $A^T$ represents $\psi_L$.

  We also have
  \[
    \psi_R(\mathbf{f}_j)(\mathbf{e}_i) = A_{ij}.
  \]
  So
  \[
    \psi_R(\mathbf{f}_j) = \sum A_{kj}\varepsilon_k.
  \]
\end{proof}

\begin{defi}[Left and right kernel]
  The kernel of $\psi_L$ is \emph{left kernel} of $\psi$, while the kernel of $\psi_R$ is the \emph{right kernel} of $\psi$.
\end{defi}
Then by definition, $\mathbf{v}$ is in the left kernel if $\psi(\mathbf{v}, \mathbf{w}) = 0$ for all $\mathbf{w} \in W$.

More generally, if $T\subseteq V$, then we write
\[
  T^\bot = \{\mathbf{w} \in W: \psi(\mathbf{t}, \mathbf{w}) = 0\text{ for all }\mathbf{t} \in T\}.
\]
Similarly, if $U\subseteq W$, then we write
\[
  ^\bot U = \{\mathbf{v} \in V: \psi(\mathbf{v}, \mathbf{u}) = 0\text{ for all }\mathbf{u}\in U\}.
\]
In particular, $V^\bot = \ker \psi_R$ and $^\bot W = \ker \psi_L$.

If we have a non-trivial left (or right) kernel, then in some sense, some elements in $V$ (or $W$) are ``useless'', and we don't like these.
\begin{defi}[Non-degenerate bilinear form]
  $\psi$ is \emph{non-degenerate} if the left and right kernels are both trivial. We say $\psi$ is \emph{degenerate} otherwise.
\end{defi}

\begin{defi}[Rank of bilinear form]
  If $\psi: V\to W$ is a bilinear form $\F$ on a finite-dimensional vector space $V$, then the \emph{rank} of $V$ is the rank of any matrix reperesenting $\phi$. This is well-defined since $r(P^T AQ) = r(A)$ if $P$ and $Q$ are invertible.

  Alternatively, it is the rank of $\psi_L$ (or $\psi_R$).
\end{defi}

\begin{lemma}
  Let $V$ and $W$ be finite-dimensional vector spaces over $\F$ with bases $(\mathbf{e}_1, \cdots, \mathbf{e}_n)$ and $(\mathbf{f}_1, \cdots, \mathbf{f}_m)$ be their basis respectively.

  Let $\psi: V\times W \to \F$ be a bilinear form represented by $A$ with respect to these bases. Then $\phi$ is non-degenerate if and only if $A$ is (square and) invertible. In particular, $V$ and $W$ have the same dimension.
\end{lemma}
We can understand this as saying if there are too many things in $V$ (or $W$), then some of them are bound to be useless.

\begin{proof}
  Since $\psi_R$ and $\psi_L$ are represented by $A$ and $A^T$ (in some order), they both have trivial kernel if and only if $n(A) = n(A^T) = 0$. So we need $r(A) = \dim V$ and $r(A^T) = \dim W$. So we need $\dim V = \dim W$ and $A$ have full rank, ie. the corresponding linear map is bijective. So done.
\end{proof}

\begin{eg}
  The map
  \begin{align*}
    \F^2 \times \F^2 &\to \F\\
    \begin{pmatrix}
      a\\c
    \end{pmatrix},
    \begin{pmatrix}
      b\\d
    \end{pmatrix} &\mapsto ad - bc
  \end{align*}
  is a bilinear form. This, obviously, corresponds to the determinant of a 2-by-2 matrix. We have $\psi(\mathbf{v}, \mathbf{w}) = -\psi(\mathbf{w}, \mathbf{v})$ for all $\mathbf{v}, \mathbf{w}\in \F^2$.
\end{eg}

\section{Determinants of matrices}
We probably all know what the determinant is. Here we are going to give a slightly more abstract definition, and spend quite a lot of time trying motivate this definition.

Recall that $S_n$ is the group of permutations of $\{1, \cdots, n\}$, and there is a unique group homomorphism $\varepsilon: S_n \to \{\pm 1\}$ such that $\varepsilon(\sigma) = 1$ if $\sigma$ can be written as a product of an even number of transpositions; $\varepsilon(\sigma) = -1$ if $\sigma$ can be written as an odd number of transpositions. It is proved in IA Groups that this is well-defined.

\begin{defi}[Determinant]
  Let $A \in \Mat_{n, n}(\F)$. Its \emph{determinant} is
  \[
    \det A = \sum_{\sigma \in S_n} \varepsilon(\sigma) \prod_{i = 1}^n A_{i \sigma(i)}.
  \]
\end{defi}
This is a big scary definition, and we will see this is the same as the determinant we are used to.

\begin{eg}
  If $n = 2$, then $S_2 = \{\id, (1\; 2)\}$. So
  \[
    \det A = A_{11}A_{22} - A_{12} A_{21}.
  \]
  When $n = 3$, then $S_3$ has 6 elements, and
  \begin{align*}
    \det A &= A_{11}A_{22}A_{33} + A_{12}A_{23}A_{31} + A_{13}A_{21}A_{32}\\
    &\quad - A_{11}A_{23}A_{32} - A_{22}A_{31}A_{13} - A_{33}A_{12}A_{21}.
  \end{align*}
\end{eg}

We will first prove a few easy and useful lemmas about the determinant, and then later see why it is defined this way.
\begin{lemma}
  $\det A = \det A^T$.
\end{lemma}

\begin{proof}
  \begin{align*}
    \det A^T &= \sum_{\sigma \in S_n} \varepsilon(\sigma) \prod_{i = 1}^n A_{\sigma(i)i}\\
    &= \sum_{\sigma \in S_n} \varepsilon (\sigma) \prod_{j = 1}^n A_{j \sigma^{-1}(j)}\\
    &= \sum_{\tau \in S_n} \varepsilon (\tau^{-1}) \prod_{j = 1}^n A_{j \tau (j)}\\
    \intertext{Since $\varepsilon(\tau) = \varepsilon(\tau^{-1})$, we get}
    &= \sum_{\tau \in S_n} \varepsilon (\tau) \prod_{j = 1}^n A_{j \tau (j)}\\
    &= \det A.
  \end{align*}
\end{proof}

\begin{lemma}
  If $A$ is an upper triangular matrix, ie.
  \[
    A =
    \begin{pmatrix}
      a_{11} & a_{12} & \cdots & a_{1n}\\
      0 & a_{22} & \cdots & a_{2n}\\
      \vdots & \vdots & \ddots & \vdots\\
      0 & 0 & \cdots & a_{nn}
    \end{pmatrix}
  \]
  Then
  \[
    \det A = \prod_{i = 1}^n a_{ii}.
  \]
\end{lemma}

\begin{proof}
  We have
  \[
    \det A = \sum_{\sigma \in S_n} \varepsilon(\sigma) \prod_{i = 1}^n A_{i\sigma (i)}\\
  \]
  But $A_{i \sigma(i)} = 0$ whenever $i > \sigma(i)$. So
  \[
    \prod_{i = 1}^n A_{i\sigma(i)} = 0
  \]
  if there is some $i \in \{1, \cdots, n\}$ such that $i > \sigma(i)$.

  However, the only permutation in which $i \leq \sigma(i)$ for all $i$ is the identity. So the only thing that contributes in the sum is $\sigma = \id$. So
  \[
    \det A = \prod_{i = 1}^n A_{ii}.
  \]
\end{proof}
To motivate this definition, we need a notion of volume. How can we define \emph{volume} on a vector space? It should be clear that the ``volume'' cannot be uniquely determined, since it depends on what units we are using. For example, saying the volume is ``$1$'' is meaningless unless we provide the units, eg. $\SI{}{\centi\meter\cubed}$. So we have an axiomatic definition for what it means for something to denote a ``volume''.

\begin{defi}[Volume form]
  A \emph{volume form} on $\F^n$ is a function $d: \F^n \times \cdots \times \F^n \to \F$ that is
  \begin{enumerate}
    \item Multilinear, ie. for all $i$ and all $\mathbf{v}_1, \cdots, \mathbf{v}_{i - 1}, \mathbf{v}_{i + 1}, \cdots, \mathbf{v}_n \in \F^n$, we have
      \[
        \d (\mathbf{v}_1, \cdots, \mathbf{v}_{i - 1}, \ph, \mathbf{v}_{i + 1}, \cdots, \mathbf{v}_n) \in (\F^n)^*.
      \]
    \item Alternating, ie. if $\mathbf{v}_i = \mathbf{v}_j$ for some $i \not= j$, then
      \[
        d(\mathbf{v}_1, \cdots, \mathbf{v}_n) = 0.
      \]
  \end{enumerate}
\end{defi}
We should think of $d(\mathbf{v}_1, \cdots, \mathbf{v}_n)$ as the $n$-dimensional volume of the parallelopiped spanned by $\mathbf{v}_1, \cdots, \mathbf{v}_n$.

We can view $A \in \Mat_n(\F)$ as an $n$-tuple in $\F^n$ by considering its columns $A = (A^{(1)}\; A^{(2)}\; \cdots \; A^{(n)})$, with $A^{(i)} \in \F^n$. Then we have
\begin{lemma}
  $\det A$ is a volume form.
\end{lemma}

\begin{proof}
  To see that $\det$ is multilinear, it is sufficient to show that each
  \[
    \prod_{i = 1}^n A_{i \sigma(i)}
  \]
  is multilinear for all $\sigma \in S_n$, since linear combinations of multilinear forms are multilinear. But each such product is contains precisely one entry from each column, and so is multilinear.

  To show it is alternating, suppose now there are some $k, \ell$ distinct such that $A^{(k)} = A^{(\ell)}$. We let $\tau$ be the transposition $(k\; \ell)$. By Lagrange's theorem, we can write
  \[
    S_n = A_n \amalg \tau A_n,
  \]
  where $A_n = \ker \varepsilon$ and $\amalg$ is the disjoint union. We also know that
  \[
    \sum_{\sigma \in A_n} \prod_{i = 1}^n A_{i \sigma (i)} = \sum_{\sigma \in A_n} \prod_{i = 1}^n A_{i, \tau\sigma(i)},
  \]
  since if $\sigma(i)$ is not $k$ or $j$, then $\tau$ does nothing; if $\sigma(i)$ is $k$ or $j$, then $\tau$ just swaps them around, but $A^{(k)} = A^{(j)}$. So we get
  \[
    \sum_{\sigma \in A_n} \prod_{i = 1}^n A_{i \sigma (i)} = \sum_{\sigma' \in \tau A_n} \prod_{i = 1}^n A_{i\sigma'(i)},
  \]
  But we know that
  \[
    \det A = \text{LHS} - \text{RHS} = 0.
  \]
  So done.
\end{proof}
We have shown that determinants are volume forms, but is this the only volume form? Well obviously not, since $2 \det A$ is also a valid volume form. However, in some sense, all volume forms are ``derived'' from the determinant. Before we show that, we need the following

\begin{lemma}
  Let $d$ be a volume form on $\F^n$. Swapping two entries changes the sign, ie.
  \[
    d (\mathbf{v}_1, \cdots, \mathbf{v}_i, \cdots, \mathbf{v}_j,\cdots, \mathbf{v}_n) = -d(\mathbf{v}_1, \cdots, \mathbf{v}_j, \cdots, \mathbf{v}_i, \cdots, \mathbf{v}_n).
  \]
\end{lemma}

\begin{proof}
  By linearity, we have
  \begin{align*}
    0 &= d(\mathbf{v}_1, \cdots, \mathbf{v}_i + \mathbf{v}_j, \cdots, \mathbf{v}_i + \mathbf{v}_j, \cdots,\mathbf{v}_n) \\
    &= d(\mathbf{v}_1, \cdots, \mathbf{v}_i, \cdots, \mathbf{v}_i, \cdots, \mathbf{v}_n)\\
    &\quad\quad+ d(\mathbf{v}_1, \cdots, \mathbf{v}_i, \cdots, \mathbf{v}_j, \cdots, \mathbf{v}_n)\\
    &\quad\quad+ d(\mathbf{v}_1, \cdots, \mathbf{v}_j, \cdots, \mathbf{v}_i, \cdots, \mathbf{v}_n)\\
    &\quad\quad+ d(\mathbf{v}_1, \cdots, \mathbf{v}_j, \cdots, \mathbf{v}_j, \cdots, \mathbf{v}_n)\\
    &= d(\mathbf{v}_1, \cdots, \mathbf{v}_i, \cdots, \mathbf{v}_j, \cdots, \mathbf{v}_n)\\
    &\quad\quad+ d(\mathbf{v}_1, \cdots, \mathbf{v}_j, \cdots, \mathbf{v}_i, \cdots, \mathbf{v}_n).
  \end{align*}
  So done.
\end{proof}

\begin{cor}
  If $\sigma \in S_n$, then
  \[
    d(\mathbf{v}_{\sigma(1)}, \cdots, \mathbf{v}_{\sigma(n)}) = \varepsilon(\sigma) d(\mathbf{v}_1,\cdots, \mathbf{v}_n)
  \]
  for any $\mathbf{v}_i \in \F^n$.
\end{cor}

\begin{thm}[]
  Let $d$ be any volume form on $\F^n$, and let $A = (A^{(1)}\;\cdots \;A^{(n)}) \in \Mat_n(\F)$. Then
  \[
    d(A^{(1)}, \cdots, A^{(n)}) = (\det A) d(\mathbf{e}_1, \cdots, \mathbf{e}_n),
  \]
  where $\{\mathbf{e}_1, \cdots, \mathbf{e}_n\}$ is the standard basis.
\end{thm}

\begin{proof}
  We can compute
  \begin{align*}
    d(A^{(1)}, \cdots, A^{(n)}) &= d\left(\sum_{i = 1}^n A_{i1} \mathbf{e}_i, A^{(2)}, \cdots, A^{(n)}\right)\\
    &= \sum_{i = 1}^n A_{i1} d(\mathbf{e}_1, A^{(2)}, \cdots, A^{(n)})\\
    &= \sum_{i, j = 1}A_{i1}A_{j2}d(\mathbf{e}_i, \mathbf{e}_j, A^{(3)}, \cdots, A^{(n)})\\
    &= \sum_{i_1, \cdots, i_n} \prod_{j = 1}^n A_{i_j j}d(\mathbf{e}_{i_1}, \cdots, \mathbf{e}_{i_n})
  \end{align*}
  We know that lots of these are zero, since if $i_k = i_j$ for some $k, j$, then the term is zero. So we are just summing over distinct tuples, ie. when there is some $\sigma$ such that $i_j = \sigma(j)$. So we get
  \[
    d(A^{(1)}, \cdots, A^{(n)}) = \sum_{\sigma \in S_n} \prod_{j = 1}^n A_{\sigma(j)j} d(\mathbf{e}_{\sigma(1)}, \cdots, \mathbf{e}_{\sigma(n)}).
  \]
  However, by our corollary up there, this is just
  \[
    d(A^{(1)}, \cdots, A^{(n)}) = \sum_{\sigma \in S_n} \prod_{j = 1}^n A_{\sigma(j)j} \varepsilon(\sigma) d(\mathbf{e}_1, \cdots, \mathbf{e}_n) = (\det A) d(\mathbf{e}_1, \cdots, \mathbf{e}_n).
  \]
  So done.
\end{proof}
We can rewrite the formula as
\[
  d (A \mathbf{e}_1, \cdots, A\mathbf{e}_n) = (\det A)d(\mathbf{e}_1, \cdots, \mathbf{e}_n).
\]
It is not hard to see that the same proof gives for any $\mathbf{v}_1, \cdots, \mathbf{v}_n$, we have
\[
  d(A\mathbf{v}_1, \cdots, A\mathbf{v}_n) = (\det A)d(\mathbf{v}_1, \cdots, \mathbf{v}_n).
\]
So we know that $\det A$ is the volume rescaling factor of an arbitrary parallelopiped, and this is true for \emph{any} volume form $d$.

\begin{thm}
  Let $A, B \in \Mat_n(\F)$. Then $\det(AB) = \det(A)\det(B)$.
\end{thm}

\begin{proof}
  Let $d$ be a non-zero volume form on $\F^n$ (eg. the ``determinant''). Then we can compute
  \[
    d(AB\mathbf{e}_1,\cdots , AB\mathbf{e}_n) = (\det AB) d(\mathbf{e}_1,\cdots, \mathbf{e}_n),
  \]
  but we also have
  \[
    d(AB\mathbf{e}_1, \cdots, AB\mathbf{e}_n) = (\det A) d(B\mathbf{e}_1, \cdots, B\mathbf{e}_n) = (\det A)(\det B)d(\mathbf{e}_1, \cdots, \mathbf{e}_n).
  \]
  Since $d$ is non-zero, we must have $\det AB = \det A \det B$.
\end{proof}

\begin{cor}
  If $A \in \Mat_n(\F)$ is invertible, then $\det A \not= 0$. In fact, when $A$ is invertible, then $\det (A^{-1}) = (\det A)^{-1}$.
\end{cor}

\begin{proof}
  We have
  \[
    1 = \det I = \det(AA^{-1}) = \det A\det A^{-1}.
  \]
  So done.
\end{proof}

\begin{defi}[Singular matrices]
  A matrix $A$ is \emph{singular} if $\det A = 0$. Otherwise, it is \emph{non-singular}.
\end{defi}

\begin{thm}[]
  Let $A \in \Mat_n(\F)$. Then the following are equivalent:
  \begin{enumerate}
    \item $A$ is invertible.
    \item $\det A \not= 0$.
    \item $r(A) = n$.
  \end{enumerate}
\end{thm}

\begin{proof}
  We have proved that (i) $\Rightarrow$ (ii) above, and the rank-nullity theorem implies (iii) $\Rightarrow$ (i). We will prove (ii) $\Rightarrow$ (ii). In fact we will show the contrapositive. Suppose $r(A) < n$. By rank-nullity theorem, $n(A) > 0$. So there is some $\mathbf{x} = \begin{pmatrix}\lambda_1\\\vdots\\\lambda_n\end{pmatrix}$ such that $A\mathbf{x} = \mathbf{0}$. Suppose $\lambda_k \not= 0$. We define $B$ as follows:
  \[
    B =
    \begin{pmatrix}
      1 & & & \lambda_1\\
      & \ddots & & \vdots\\
      & & 1 & \lambda_{k - 1}\\
      & & & \lambda_k\\
      & & & \lambda_{k + 1} & 1\\
      & & & \vdots & & \ddots\\
      & & & \lambda_n & & & 1
    \end{pmatrix}
  \]
  So $AB$ has the $k$th column identically zero. So $\det(AB) = 0$. So it is sufficient to prove that $\det (B) \not= 0$. But $\det B = \lambda_k \not= 0$. So done.
\end{proof}

We are now going to come up with an alternative formula for the derivative (which is probably the one you are familiar with). To do so, we introduce the following notation:
\begin{notation}
  Write $\hat{A}_{ij}$ for the matrix obtained from $A$ by deleting the $i$th row and $j$th column.
\end{notation}

\begin{lemma}
  Let $A \in \Mat_n(\F)$. Then
  \begin{enumerate}
    \item We can expand $\det A$ along the $j$th column by
      \[
        \det A = \sum_{i = 1}^n (-1)^{i + j} A_{ij} \det \hat{A}_{ij}.
      \]
    \item We can expand $\det A$ along the $i$th row by
      \[
        \det A = \sum_{j = 1}^n (-1)^{i + j} A_{ij} \det \hat{A}_{ij}.
      \]
  \end{enumerate}
\end{lemma}
We could prove this directly from the definition, but that is messy and scary, so let's use volume forms instead.

\begin{proof}
  Since $\det A = \det A^T$, (i) and (ii) are equivalent. So it suffices to prove just one of them. We have
  \[
    \det A = d(A^{(1)}, \cdots, A^{(n)}),
  \]
  where $d$ is the volume form induced by the determinant. Then we can write as
  \begin{align*}
    \det A &= d\left(A^{(1)}, \cdots, \sum_{j = 1}^n A_{ij} \mathbf{e}_i, \cdots, A^{(n)}\right)\\
    &= \sum_{i = 1}^n A_{ij} d(A^{(1)}, \cdots, \mathbf{e}_{ij}, \cdots, A^{(n)})
  \end{align*}
  The right is the determinant of a matrix with the $j$th column replaced with $\mathbf{e}_1$. We can move our columns around so that our matrix becomes
  \[
    B =
    \begin{pmatrix}
      \hat{A}_{ij} & 0\\
      \mathrm{stuff} & 1
    \end{pmatrix}
  \]
  We get that $\det B = \det \hat{A}^{ij}$, since the only permutations that give a non-zero sum are those that send $n$ to $n$. In the row and column swapping, we have made $n - j$ column transpositions and $n - i$ row transpositions. So we have
  \begin{align*}
    \det A &= \sum_{i = 1}^n A_{ij} (-1)^{n - j} (-1)^{n - i}\det B\\
    &= \sum_{i = 1}^n A_{ij} (-1)^{i + j} \det \hat{A}_{ij}.
  \end{align*}
\end{proof}
This is not only useful for computing determinants, but also computing inverses.

\begin{defi}[Adjugate matrix]
  Let $A \in \Mat_n(\F)$. The \emph{adjugate matrix} of $A$, written $\adj A$, is the $n\times n$ matrix such that $(\adj A)_{ij} = (-1)^{i + j} \det\hat{A}_{ji}$.
\end{defi}

The relevance is the following result:
\begin{thm}[]
  If $A \in \Mat_n(\F)$, then $A(\adj A) = (\det A) I_n = (\adj A)A$. In particular, if $\det A \not= 0$, then
  \[
    A^{-1} = \frac{1}{\det A}\adj A.
  \]
\end{thm}
Note that this is not an efficient way to compute the inverse.

\begin{proof}
  We compute
  \[
    [(\adj A)A]_{jk} = \sum_{i = 1}^n (\adj A)_{ji} A_{ik} = \sum_{i = 1}^n (-1)^{i + 1} \det\hat{A}_{ij} A_{ik}.\tag{$*$}
  \]
  So if $j = k$, then $[(\adj A)A]_{jk} = \det A$ by the lemma.

  Otherwise, if $j = k$, consider the matrix obtained from $A$ by replacing the $j$th column by the $k$th column. Then the right hand side of $(*)$ is just $\det B$ by the lemma. But we know that if two columns are the same, the determinant is zero. So the right hand side of $(*)$ is zero. So
  \[
    [(\adj A)A]_{jk} = \det A \delta_{jk}
  \]
  The calculation for $[A\adj A] = (\det A) I_n$ can be done in a similar manner, or by considering $(A\adj A)^T = (\adj A)^T A^T = (\adj (A^T)) A^T = (\det A) I_n$.
\end{proof}
Note that the coefficients of $(\adj A)$ are just given by polynomials in the entries of $A$, and so is the determinant. So if $A$ is invertible, then its inverse is given a rational function (ie. ratio of two polynomials) in the entries of $A$.

This is very useful theoretically, but not computationally, since the polynomials are very large. There are better ways computationally, such as Gaussian elimination.

We'll end with a useful tricks to compute the determinant.
\begin{lemma}
  Let $A, B$ be square matrices. Then for any $C$, we have
  \begin{align*}
    \det
    \begin{pmatrix}
      A & C\\
      0 & B
    \end{pmatrix}
    = (\det A) (\det B).
  \end{align*}
\end{lemma}

\begin{proof}
  Suppose $A\in \Mat_k(\F)$, and $B\in \Mat_{\ell}(\F)$, so $C \in \Mat_{l, \ell}(\F)$. let
  \[
    X =
    \begin{pmatrix}
      A & C\\
      0 & B
    \end{pmatrix}.
  \]
  Then by definition, we have
  \[
    \det X = \sum_{\sigma \in S_{k + \ell}}\varepsilon(\sigma) \prod_{i = 1}^{k + \ell} X_{i\sigma(i)}.
  \]
  If $j \leq k$ and $i > k$, then $X_{ij} = 0$. We only want to sum over permutations $\sigma$ such that $\sigma(i) > k$ if $i > k$. So we are permuting the last $j$ things among themselves, and hence the first $k$ things among themselves. So we can decompose this into $\sigma = \sigma_1 \sigma_2$, where $\sigma_1$ is a permutation of $\{1, \cdots, k\}$ and fixes the remaining things, while $\sigma_2$ fixes $\{1, \cdots, k\}$, and permutes the remaining. Then
  \begin{align*}
    \det X &= \sum_{\sigma = \sigma_1\sigma_2}\varepsilon(\sigma_1\sigma_2) \prod_{i = 1}^k X_{i\sigma_1(i)} \prod_{j = 1}^\ell X_{k + j\; \sigma(k + j)}\\
    &= \left(\sum_{\sigma_1 \in S_k} \varepsilon(\sigma_1) \prod_{i = 1}^k A_{i\sigma_1(i)}\right)\left(\sum_{\sigma_2 \in S_\ell} \varepsilon(\sigma_2) \prod_{j = 1}^k B_{j\sigma_2(j)}\right)\\
    &= (\det A)(\det B)
  \end{align*}
\end{proof}

\begin{cor}
  \[
    \det
    \begin{pmatrix}
      A_1 & & & \mathrm{stuff}\\
      & A_2\\
      & & \ddots\\
      0 & & & A_n
    \end{pmatrix} = \prod_{i = 1}^n \det A_i
  \]
\end{cor}

\section{Endomorphisms}
Endomorphisms are linear maps from a vector space $V$ to itself. One might wonder --- why would we want to study these linear maps in particular, when we can just work with arbitrary linear maps from any space to any other space?

When we work with arbitrary linear maps, we are free to choose any basis for the domain, and any basis for the co-domain, since it doesn't make sense to require they have the ``same'' basis. Then we proved that by choosing the right bases, we can put matrices into a nice form with only $1$'s in the diagonal.

However, when working with endomorphisms, we can require ourselves to use the same basis for the domain and co-domain, and there is much more we can say.

\subsection{Invariants}
\begin{defi}[]
  If $V$ is a (finite-dimensional) vector space over $\F$. An \emph{endomorphism} of $V$ is a linear map $\alpha: V \to V$. We write $\End(V)$ for the $\F$-vector space of all such linear maps, and $L$ for the identity map $V \to V$.
\end{defi}
When we think about matrices representing an endomorphism of $V$, we'll use the same basis for the domain and the range. We are going to study some properties of these endomorphisms that are not dependent on the basis we pick, known as \emph{invariants}.

\begin{lemma}
  Suppose $(\mathbf{e}_1, \cdots, \mathbf{e}_n)$ and $(\mathbf{f}_1, \cdots, \mathbf{f}_n)$ are bases for $V$ and $\alpha \in \End(V)$. If $A$ represents $\alpha$ with respect to $(\mathbf{e}_1, \cdots, \mathbf{n})$ and $B$ represents $\alpha$ with respect to $(\mathbf{f}_1,\cdots, \mathbf{f}_n)$, then
  \[
    B = P^{-1}AP,
  \]
  where $P$ is given by
  \[
    \mathbf{f}_i = \sum_{j = 1}^n P_{ji}\mathbf{e}_j.
  \]
\end{lemma}

\begin{proof}
  This is merely a special case of an earlier more general result for arbitrary maps and spaces.
\end{proof}

\begin{defi}[Similar matrices]
  We say matrices $A$ and $B$ are \emph{similar} or \emph{conjugate} if there is some $P$ invertible such that $B = P^{-1}AP$.
\end{defi}

Recall that $\GL_n(\F)$, the group of invertible $n\times n$ matrices. $\GL_n(\F)$ acts on $\Mat_n(\F)$ by conjugation:
\[
  (P, A) \mapsto PAP^{-1}.
\]
We are conjugating it this way so that the associativity axiom holds (otherwise we get a \emph{right} action instead of a \emph{left} action). Then $A$ and $B$ are similar iff they are in the same orbit. Since orbits always partition the set, this is an equivalence relation.

Our main goal is to classify the orbits, ie. find a ``nice'' representative for each orbit.

Our initial strategy is to identify basis-independent invariants for endomorphisms. For example, we will show that the rank, trace, determinant and characteristic polynomial are all such invariants.

Recall that the trace of a matrix $A \in \Mat_n(\F)$ is the sum of the diagonal elements:
\begin{defi}[Trace]
  The \emph{trace} of a matrix of $A \in \Mat_n(\F)$ is defined by
  \[
    \tr A = \sum_{i = 1}^n A_{ii}.
  \]
\end{defi}

We want to show that the trace is an invariant. In fact, we will show a stronger statement (as well as the corresponding statement for determinants):
\begin{lemma}\leavevmode
  \begin{enumerate}
    \item If $A \in \Mat_{m, n}(\F)$ and $B\in \Mat_{n, m}(\F)$, then
      \[
        \tr AB = \tr BA.
      \]
    \item If $A, B \in \Mat_n(\F)$ are similar, then $\tr A = \tr B$.
    \item If $A, B \in \Mat_n(\F)$ are similar, then $\det A = \det B$.
  \end{enumerate}
\end{lemma}

\begin{proof}\leavevmode
  \begin{enumerate}
    \item We have
      \[
        \tr AB = \sum_{i = 1}^m (AB)_{ii} = \sum_{i = 1}^m \sum_{j = 1}^n A_{ij}B_{ji} = \sum_{j = 1}^n \sum_{i = 1}^m B_{ji}A_{ij} = \tr BA.
      \]
    \item Suppose $B = P^{-1}AP$. Then we have
      \[
        \tr B = \tr (P^{-1}(AP)) = \tr ((AP)P^{-1}) = \tr A.
      \]
    \item We have
      \[
        \det (P^{-1}AP) = \det P^{-1} \det A \det P = (\det P)^{-1} \det A \det P = \det A
      \]
  \end{enumerate}
\end{proof}
This allows us to define the trace and determinant of an \emph{endomorphism}.
\begin{defi}[Trace and determinant of endomorphism]
  Let $\alpha \in \End(V)$, and $A$ be a matrix representing $\alpha$ under any basis. Then the \emph{trace} of $\alpha$ is $\tr \alpha = \tr A$, and the \emph{determinant} is $\det \alpha = \det A$.
\end{defi}
The lemma tells us that the determinant and trace are well-defined. We can also define the determinant without reference to a basis, by defining more general volume forms and define the determinant as a scaling factor.

The trace is slightly more tricky to define without basis, but in IB Analysis II, you will find that it is the directional derivative at the origin.

To talk about the characteristic polynomial, we need to know what eigenvalues are.
\begin{defi}[Eigenvalue and eigenvector]
  Let $\alpha \in \End(V)$. Then $\lambda \in \F$ is an \emph{eigenvalue} (or E-value) if there is some $\mathbf{v} \in V\setminus \{0\}$ such that $\alpha \mathbf{v} = \lambda \mathbf{v}$.

  $\mathbf{v}$ is an \emph{eigenvector} if $\alpha(\mathbf{v}) = \lambda \mathbf{v}$ for some $\lambda \in \F$.

  When $\lambda \in \F$, the \emph{$\lambda$-eigenspace}, written $E_\alpha(\lambda)$ or $E(\lambda)$ is the subspace of $V$ containing all the $\lambda$-eigenvectors, ie.
  \[
    E_\alpha(\lambda) = \ker (\lambda \iota - \alpha).
  \]
  where $\iota$ is the identity function.

  The \emph{characteristic polynomial} of $\alpha$ is defined by
  \[
    \chi_\alpha(t) = \det (t\iota - \alpha).
  \]
\end{defi}
Note that $\chi_\alpha(t)$ is a monic polynomial of degree $\dim V$.

We know that $\lambda$ is an eigenvalue of $\alpha$ iff $n(\alpha - \lambda \iota) > 0$ iff $r(\alpha - \lambda \iota) < \dim V$ iff $\chi_\alpha(\lambda) = \det(\lambda \iota - \alpha) = 0$. So the eigenvalues are precisely the roots of the characteristic polynomial.

If $A \in \Mat_n(\F)$, we can define $\chi_A(t) = \det (tI - A)$.
\begin{lemma}
  If $A$ and $B$ are similar, then they have the same characteristic polynomial.
\end{lemma}

\begin{proof}
  \[
    \det (tI - P^{-1}AP)  = \det(P^{-1}(tI - A)P) = \det(tI - A).
  \]
\end{proof}

\begin{lemma}
  Let $\alpha \in \End(V)$ and $\lambda_1, \cdots, \lambda_k$ distinct eigenvalues of $\alpha$. Then
  \[
    E(\lambda_1) + \cdots +  E(\lambda_k) = \bigoplus_{i = 1}^k E(\lambda_i)
  \]
  is a direct sum.
\end{lemma}

\begin{proof}
  Suppose
  \[
    \sum_{i = 1}^k \mathbf{x}_i = \sum_{i = 1}^k \mathbf{y}_i,
  \]
  with $\mathbf{x}_i, \mathbf{y}_i \in E(\lambda_i)$. We want to show that they are equal. We are going to find some clever map that tells us what $\mathbf{x}_i$ and $\mathbf{y}_i$ are. Consider $\beta_j \in \End(V)$ defined by
  \[
    \beta_j = \prod_{r \not= j} (\alpha - \lambda_r \iota).
  \]
  Then
  \begin{align*}
    \beta_j\left(\sum_{i = 1}^i \mathbf{x}_k\right) &= \sum_{i = 1}^k \prod_{r \not= j}(\alpha - \lambda_{r}\iota)(\mathbf{x}_i)\\
    &= \sum_{i = 1}^k \prod_{r\not= j} (\lambda_i - \lambda_r)(\mathbf{x}_i).
    \intertext{Each summand is zero, unless $i \not= j$. So this is equal to}
    \beta_j\left(\sum_{i = 1}^i \mathbf{x}_k\right) &= \prod_{r \not= j}(\lambda_j - \lambda_r) (\mathbf{x}_j).
  \end{align*}
  Similarly, we obtain
  \[
    \beta_j\left(\sum_{i = 1}^i \mathbf{y}_k\right) = \prod_{r \not= j}(\lambda_j - \lambda_r) (\mathbf{y}_j).
  \]
  Since we know that $\sum \mathbf{x}_i = \sum \mathbf{y}_i$, we must have
  \[
    \prod_{r \not= j}(\lambda_j - \lambda_r) \mathbf{x}_j = \prod_{r \not= j} (\lambda_j- \lambda_r)\mathbf{y}_j.
  \]
  Since we know that $\prod_{r \not= j} (\lambda_r - \lambda_j \not= 0$, we must have $\mathbf{x}_i = \mathbf{y}_i$ for all $i$.

  So each expression for $\sum \mathbf{x}_i$ is unique.
\end{proof}
The proof shows that any set of non-zero eigenvectors with distinct eigenvalues is linearly independent.

\begin{defi}[Diagonalizable]
  We say $\alpha \in \End(V)$ is diagonalizable if there is some basis for $V$ such that $\alpha$ is represented by a diagonal matrix, ie. all terms not on the diagonal are zero.
\end{defi}
These are in some sense the nice matrices we like to work with.

\begin{thm}[]
  Let $\alpha \in \End(V)$ and $\lambda_1, \cdots, \lambda_k$ be distinct eigenvalues of $\alpha$. Write $E_i$ for $E(\lambda_i)$. Then the following are equivalent:
  \begin{enumerate}
    \item $\alpha$ is diagonalizable.
    \item $V$ has a basis of eigenvectors for $\alpha$.
    \item $V = \bigoplus_{i = 1}^k E_i$.
    \item $\dim V = \sum_{i = 1}^k \dim E_i$.
  \end{enumerate}
\end{thm}

\begin{proof}\leavevmode
  \begin{itemize}
    \item (i) $\Leftrightarrow$ (ii): Suppose $(\mathbf{e}_1, \cdots, \mathbf{e}_n)$ is a basis for $V$. Then
      \[
        \alpha(\mathbf{e}_i) = A_{ji} \mathbf{e}_j,
      \]
      where $A$ represents $\alpha$. Then $A$ is diagonal iff each $\mathbf{e}_i$ is an eigenvector. So done
    \item (ii) $\Leftrightarrow$ (iii): It is clear that (ii) is true iff $\sum E_i = V$, but we know that $V$ must be a direct sum. So done.
    \item (iii) $\Leftrightarrow$ (iv): This follows from Example Sheet Q10, which says that $V = \oplus_{i = 1}^k E_i$ iff the bases for $E_i$ are disjoint and their union is a basis of $V$.
  \end{itemize}
\end{proof}
\end{document}
