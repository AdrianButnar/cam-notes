\documentclass[a4paper]{article}

\def\npart {IB}
\def\nterm {Michaelmas}
\def\nyear {2015}
\def\nlecturer {S. J. Wadsley}
\def\ncourse {Linear Algebra}
\def\nofficial{https://www.dpmms.cam.ac.uk/~sjw47/LinearAlgebra.pdf}
\def\nlectures{MWF.10}
\def\nnotready {}

\input{header}

\begin{document}
\maketitle
{\small
\noindent Definition of a vector space (over $\R$ or $\C$), subspaces, the space spanned by a subset. Linear independence, bases, dimension. Direct sums and complementary subspaces. \hspace*{\fill} [3]

\vspace{5pt}
\noindent Linear maps, isomorphisms. Relation between rank and nullity. The space of linear maps from $U$ to $V$, representation by matrices. Change of basis. Row rank and column rank.\hspace*{\fill} [4]

\vspace{5pt}
\noindent Determinant and trace of a square matrix. Determinant of a product of two matrices and of the inverse matrix. Determinant of an endomorphism. The adjugate matrix.\hspace*{\fill} [3]

\vspace{5pt}
\noindent Eigenvalues and eigenvectors. Diagonal and triangular forms. Characteristic and minimal polynomials. Cayley-Hamilton Theorem over $\C$. Algebraic and geometric multiplicity of eigenvalues. Statement and illustration of Jordan normal form.\hspace*{\fill} [4]

\vspace{5pt}
\noindent Dual of a finite-dimensional vector space, dual bases and maps. Matrix representation, rank and determinant of dual map.\hspace*{\fill} [2]

\vspace{5pt}
\noindent Bilinear forms. Matrix representation, change of basis. Symmetric forms and their link with quadratic forms. Diagonalisation of quadratic forms. Law of inertia, classification by rank and signature. Complex Hermitian forms.\hspace*{\fill} [4]

\vspace{5pt}
\noindent Inner product spaces, orthonormal sets, orthogonal projection, $V = W \oplus W^\bot$. Gram-Schmidt orthogonalisation. Adjoints. Diagonalisation of Hermitian matrices. Orthogonality of eigenvectors and properties of eigenvalues.\hspace*{\fill} [4]}

\tableofcontents
\setcounter{section}{-1}
\section{Introduction}
Linear algebra is the study of vector spaces and linear maps between vector spaces. Unlike IA vector and matrices, most of the time, we will not assume that we already have a fixed coordinate system. In particular, we no longer treat vectors as a ``list of numbers''. Instead, we will provide an axiomatic treatment of vector spaces and the maps.

\section{Definitions}
\subsection{Definitions and examples}
Intuitively, a vector space $V$ over a field $\F$ (or an $\F$-vector space) is a space with two operations:
\begin{itemize}
  \item We can add two vectors $\mathbf{v}_1, \mathbf{v}_2 \in V$ to obtain $\mathbf{v}_1 + \mathbf{v}_2 \in V$.
  \item We can multiply a scalar $\lambda \in \F$ with a vector $\mathbf{v}\in V$ to obtain $\lambda \mathbf{v} \in V$.
\end{itemize}

Of course, these two operations must satisfy certain axioms before we can call it a vector space. However, before going into these details, we first look at a few examples of vector spaces.

\begin{eg}\leavevmode
  \begin{enumerate}
    \item $\R^n = \{\text{column vectors of length }n\text{ with coefficients in }\R\}$ with the usual addition and scalar multiplication is a vector space.

      An $m\times n$ matrix $A$ with coefficients in $\R$ can be viewed as a linear map from $\R^m$ to $\R^n$ via $\mathbf{v} \mapsto A\mathbf{v}$.

      This is a motivational example for vector spaces. When confused about definitions, we can often think what the definition means in terms of $\R^n$ and matrices to get some intuition.

    \item Let $X$ be a set and define $\R^X = \{f: X\to \R\}$ with addition $(f + g)(x) = f(x) + g(x)$ and scalar multiplication $(\lambda f)(x) = \lambda f(x)$. This is a vector space.

      More generally, if $V$ is a vector space, $X$ is a set, we can define $V^X = \{f: X \to V\}$ with addition and scalar multiplication as above.
    \item Let $[a, b]\subseteq \R$ be a closed interval, then
      \[
        C([a, b], \R) = \{f\in \R^{[a,b]}: f\text{ is continuous}\}
      \]
      is a vector space, with operations as above. We also have
      \[
        C^{\infty}([a, b], \R) = \{f\in \R^{[a,b]}: f\text{ is infinitely differentiable}\}
      \]
    \item The set of $m\times n$ matrices with coefficients in $\R$ is a vector space, using the obvious operations, is a vector space.
  \end{enumerate}
\end{eg}

\begin{notation}
  We will use $\F$ to denote an arbitrary field, usually $\R$ or $\C$.
\end{notation}

We will now properly define a vector space, listing the axioms they are required to satisfy.
\begin{defi}[Vector space]
  An \emph{$\F$-vector space} is an (additive) abelian group $V$ together with a function $\F \times V \to V$, written $(\lambda, \mathbf{v}) \mapsto \lambda \mathbf{v}$, such that
  \begin{enumerate}
    \item $\lambda(\mu \mathbf{v}) = \lambda \mu \mathbf{v}$ for all $\lambda, \mu \in \F$, $\mathbf{v}\in V$ \hfill (associativity)
    \item $\lambda(\mathbf{u} + \mathbf{v}) = \lambda \mathbf{u} + \lambda \mathbf{v}$ for all $\lambda\in \F$, $\mathbf{u}, \mathbf{v}\in V$\hfill (distributivity in $V$)
    \item $(\lambda + \mu) \mathbf{v} = \lambda \mathbf{v} + \mu \mathbf{v}$ for all $\lambda, \mu \in \F$, $\mathbf{v}\in V$ \hfill (distributivity in $\F$)
    \item $1\mathbf{v} = \mathbf{v}$ for all $\mathbf{v}\in V$ \hfill (identity)
  \end{enumerate}

  We always write $\mathbf{0}$ for the identity in $V$, and call this the identity. By abuse of notation, we also write $0$ for the trivial vector space $\{0\}$.
\end{defi}
In a general vector space, there is no notion of ``coordinates'', length, angle or distance.

From the axioms, there are a few results we can immediately prove.
\begin{prop}
  In any vector space $V$, $0\mathbf{v} = \mathbf{0}$ for all $v\in V$, and $(-1)\mathbf{v} = -\mathbf{v}$, where $-\mathbf{v}$ is the additive inverse of $\mathbf{v}$.
\end{prop}
Proof is left as an exercise.

Similar to groups and subgroups, we can have a subspace of a vector space.
\begin{defi}[Subspace]
  If $V$ is an $\F$-vector space, then $U\subseteq V$ is an ($\F$-linear) \emph{subspace} if
  \begin{enumerate}
    \item $\mathbf{u}, \mathbf{v}\in U$ implies $\mathbf{u} + \mathbf{v} \in U$.
    \item $\mathbf{u}\in U, \lambda \in \F$ implies $\lambda u\in U$.
    \item $\mathbf{0}\in U$.
  \end{enumerate}
  These conditions can be expressed more concisely as ``$U$ is non-empty and if $\lambda, \mu\in \F, \mathbf{u}, \mathbf{v}\in U$, then $\lambda \mathbf{u} + \mu \mathbf{v}\in U$''.

  Alternatively, we can write the requirements as $U$ is also a vector space (inheriting the operations from $V$).
\end{defi}

\begin{eg}\leavevmode
  \begin{enumerate}
    \item $\{(x_1, x_2, x_3) \in \R^3: x_1 + x_2 + x_3 = t\}$ is a subspace of $\R^3$ iff $t = 0$.
    \item Let $X$ be a set. We define the \emph{support} of $f$ in $\F^X$ to by $\supp(f) = \{x\in X: f(x) \not= 0\}$. Then the set of functions with finite support forms a vector subspace. This is since $\supp (f + g) \subseteq \supp(f) \cup \supp(g)$, $\supp (\lambda f) = \supp (f)$ (for $\lambda \not= 0$) and $\supp (0) = \emptyset$.
  \end{enumerate}
\end{eg}

If we have two subspaces $U$ and $V$, there are several things we can do with them. For example, we can take the intersection $U\cap V$. We will shortly show that this will be a subspace. However, taking the union will in general not produce a vector space. Instead, we need the sum:

\begin{defi}[Sum of subspaces]
  Suppose $U, W$ are subspaces of an $\F$ vector space $V$. The \emph{sum} of $U$ and $V$ is
  \[
    U + W = \{u + w: u\in U, w\in W\}.
  \]
\end{defi}

\begin{prop}
  Let $U, W$ be subspaces of $V$. Then $U + W$ and $U\cap W$ are subspaces.
\end{prop}

\begin{proof}
  Let $\mathbf{u}_i + \mathbf{w}_i \in U + W$, $\lambda, \mu\in \F$. Then
  \[
    \lambda(\mathbf{u}_1 + \mathbf{w}_1) + \mu(\mathbf{u}_2 + \mathbf{w}_2) = (\lambda\mathbf{u}_1 + \mu\mathbf{u}_2) + (\lambda\mathbf{w}_1 + \mu\mathbf{w}_2) \in U + W.
  \]
  Similarly, if $\mathbf{v}_i \in U\cap W$, then $\lambda \mathbf{v}_1 + \mu \mathbf{v}_2\in U$ and $\lambda \mathbf{v}_1 + \mu \mathbf{v}_2\in W$. So $\lambda \mathbf{v}_1 + \mu \mathbf{v}_2\in U\cap W$.

  Both $U\cap W$ and $U + W$ contain $\mathbf{0}$, and are non-empty. So done.
\end{proof}

\begin{defi}[Quotient spaces*]
  Suppose that $V$ is a vector space, and $U\subseteq V$ is a subspace. Then the quotient group $V/U$ can be made into a vector space called the \emph{quotient space}, where scalar multiplication is given by $(\lambda, v + U) = (\lambda v) + U$.

  This is well defined since if $v + U = w + U\in V/U$, then $v - w \in U$. Hence for $\lambda \in \F$, we have $\lambda v - \lambda w \in U$. So $\lambda v + U = \lambda w + U$.
\end{defi}
\subsection{Linear independence, bases and the Steinitz exchange lemma}

\begin{defi}[Span]
  If $V$ is an $\F$-vector space and $S\subseteq V$, then the \emph{span} of $S$ is
  \[
    \bra S\ket = \left\{\sum_{i = 1}^n \lambda_i S_i : \lambda_i \in \F, S_i \in S, n \geq 0\right\}
  \]
  This is the smallest subspace of $V$ containing $S$.

  Note that the sums must be finite. We will not play with infinite sums, since the notion of convergence is not even well defined in a general vector space.
\end{defi}

\begin{eg}\leavevmode
  \begin{enumerate}
    \item Let $V = \R^3$ and $S = \left\{\begin{pmatrix}1\\0\\0\end{pmatrix}, \begin{pmatrix}0\\1\\1\end{pmatrix}, \begin{pmatrix}1\\2\\2\end{pmatrix}\right\}$. Then
      \[
        \bra S\ket = \left\{
          \begin{pmatrix}
            a\\b\\b\\
          \end{pmatrix}: a, b\in \R
        \right\}.
      \]
      Note that any subset of $S$ of order 2 has the same span as $S$.
    \item Let $X$ be a set, $\delta x: X\to \F$ be the function
      \[
        \delta x(y) =
        \begin{cases}
          1 & y = x\\
          0 & y\not= x
        \end{cases}.
      \]
      Then $\bra \delta x: x \in X\ket$ is the set of all functions with finite support.
  \end{enumerate}
\end{eg}

\begin{defi}[Spanning set]
  Let $V$ be a vector space over $\F$ and $S\subseteq V$. $S$ \emph{spans} $V$ if $\bra S\ket = V$.
\end{defi}

\begin{defi}[Linear independence]
  Let $V$ be a vector space over $\F$ and $S\subseteq V$. Then $S$ is \emph{linearly independent} (LI) if whenever
  \[
    \sum_{i = 1}^n \lambda_i \mathbf{s}_i = 0\text{ with } \lambda_i \in \F, \mathbf{s}_1, \mathbf{s}_2, \cdots, \mathbf{s}_n \in S\text{ distinct},
  \]
  we must have $\lambda_i = 0$ for all $i$.

  If $S$ is not linearly independent, we say it is \emph{linearly dependent} (LD).
\end{defi}

\begin{defi}[Basis]
  Let $V$ be a vector space over $\F$ and $S\subseteq V$. Then $S$ is a \emph{basis} for $V$ if $S$ is linearly independent and spans $V$.
\end{defi}

\begin{defi}[Finite dimensional]
  A vector space is \emph{finite dimensional} if there is a finite basis.
\end{defi}
Ideally, we would want to define the \emph{dimension} as the number of vectors in the basis. However, we must first show that this is well-defined. It is certainly plausible that a vector space has a basis of size $7$ as well as a basis of size $3$. We must show that this can never happen, which is something we'll do soon.

We will first have an example:
\begin{eg}
  Again, let $V = \R^3$ and $S = \left\{\begin{pmatrix}1\\0\\0\end{pmatrix}, \begin{pmatrix}0\\1\\1\end{pmatrix}, \begin{pmatrix}1\\2\\2\end{pmatrix}\right\}$. Then $S$ is linearly dependent since
  \[
    1\begin{pmatrix}1\\0\\0\end{pmatrix} + 2\begin{pmatrix}0\\1\\1\end{pmatrix} + (-1) \begin{pmatrix}1\\2\\2\end{pmatrix} = \mathbf{0}.
  \]
  $S$ also does not span $V$ since $\begin{pmatrix}0\\0\\1\end{pmatrix}\not \in \bra S\ket$.
\end{eg}

Note that no linearly independent set can contain $\mathbf{0}$, as $1\cdot \mathbf{0} = \mathbf{0}$. We also have $\bra \emptyset\ket = \{\mathbf{0}\}$ and $\emptyset$ is a basis for this space.

There is an alternative way in which we can define linear independence.
\begin{lemma}
  $S\subseteq V$ is linearly dependent if and only if there are distinct $s_0, \cdots, s_n \in S$ and $\lambda_1, \cdots, \lambda_n\in \F$ such that
  \[
    \sum_{i = 1}^n \lambda_i s_i = s_0.
  \]
\end{lemma}

\begin{proof}
  If $S$ is linearly dependent, then there is some $\lambda_1, \cdots, \lambda_n \in \F$ all non-zero and $s_1,\cdots, s_n \in S$ such that $\sum \lambda_i s_i = 0$. Then
  \[
    s_1 = \sum_{i = 2}^n -\frac{\lambda_i}{\lambda_1} s_i.
  \]
  Conversely, if $s_0 = \sum_{i = 1}^n \lambda_i s_i$, then
  \[
    (-1)s_0 + \sum_{i = 1}^n \lambda_i s_i = 0.
  \]
\end{proof}

We also have an alternative characterization of what it means to be a basis:
\begin{prop}
  If $S = \{\mathbf{e}_1, \cdots, \mathbf{e}_n\}$ is a subset of $V$ over $\F$, then it is a basis if and only if every $\mathbf{v}\in V$ can be written uniquely as a finite linear combination of elements in $S$, ie. as
  \[
    \mathbf{v} = \sum_{i = 1}^n \lambda_i \mathbf{e}_i.
  \]
\end{prop}

\begin{proof}
  We can view this as a combination of two statements: it can be spanned in at least one way, and it can be spanned in at most one way. We will see that the first part corresponds to $S$ spanning $V$, and the second part corresponds to $S$ being linearly independent.

  In fact, $S$ spanning $V$ is defined exactly to mean that every item $\mathbf{v}\in V$ can be written as a finite linear combination in at least one way.

  Now suppose that $S$ is linearly independent, and we have
  \[
    \mathbf{v} = \sum_{i = 1}^n \lambda_i \mathbf{e}_i = \sum_{i = 1}\mu_i \mathbf{e}_i.
  \]
  Then we have
  \[
    \mathbf{0} = \mathbf{v} - \mathbf{v} = \sum_{i = 1}^n (\lambda_i - \mu_i) \mathbf{e}_i.
  \]
  Linear independence implies that $\lambda_i - \mu_i = 0$ for all $i$. Hence $\lambda_i = \mu_i$. So $\mathbf{v}$ can be expressed in a unique way.

  On the other hand, if $S$ is not linearly independent, then we have
  \[
    \mathbf{0} = \sum_{i = 1}^n \lambda_i \mathbf{e}_i
  \]
  where $\lambda_i \not= 0$ for some $i$. But we also know that
  \[
    \mathbf{0} = \sum_{i = 1}^n 0\cdot \mathbf{e}_i.
  \]
  So there are two ways to write $\mathbf{0}$ as a linear combination. So done.
\end{proof}

Now we come to the key theorem:
\begin{thm}[Steinitz exchange lemma]
  Let $V$ be an $\F$-vector space, and $S = \{\mathbf{e}_1, \cdots, \mathbf{e}_n\}$ a finite linearly independent subset of $V$, and $T$ a spanning subset of $V$. Then there is some $T'\subseteq T$ of order $n$ such that $(T\setminus T') \cup S$ still spans $V$. In particular, $|T| \geq n$.
\end{thm}
In some sense, the final remark is the most important part. It tells us that we cannot have a independent set larger than a spanning set.

This is sometimes stated in the following alternative way for $|T| < \infty$.
\begin{cor}
  If $\{\mathbf{e}_1, \cdots, \mathbf{e}_n\}$ is a linearly independent subset of $V$ and $\{\mathbf{f}_1, \cdots, \mathbf{f}_m\}$ spans $V$, then there is a re-ordering of the $\{\mathbf{f}_i\}$ such that $\{\mathbf{e}_1,\cdots, \mathbf{e}_n, \mathbf{f}_{n + 1}, \cdots, \mathbf{f}_m\}$ spans $V$.
\end{cor}

\begin{proof}
  We are going to prove this by finding an element in $T$ and replacing it with $\mathbf{e}_i$, for each $i$, one by one.

  Suppose that we have found $T_r'\subseteq T$ of order $0 \leq r < n$ such that
  \[
    T_r = (T\setminus T_r') \cup \{\mathbf{e}_1, \cdots, \mathbf{e}_r\}
  \]
  spans $V$.

  (Note that the case $r = 0$ is trivial, since we can take $T_r' = \emptyset$, and the case $r = n$ is the theorem which we want to achieve.)

  Suppose we have these. Since $T_r$ spans $V$, we can write
  \[
    \mathbf{e}_{r + 1} = \sum_{i = 1}^k \lambda_i \mathbf{t}_i,\quad \lambda_i \in \F, \mathbf{t}_i \in T_r.
  \]
  We know that the $\mathbf{e}_i$ are linearly independent, so not all $\mathbf{t}_i$'s are $\mathbf{e}_i$'s. So there is some $j$ such that $t_j \in (T\setminus T_r')$.  We can write this as
  \[
    \mathbf{t}_j = \frac{1}{\lambda_j} \mathbf{e}_{r + 1} + \sum_{i \not= j} -\frac{\lambda_i}{\lambda_j} \mathbf{t}_i.
  \]
  We let $T_{r + 1}' = T_r' \cup \{\mathbf{t}_j\}$ of order $r + 1$, and
  \[
    T_{r + 1} = (T\setminus T_{r + 1}') \cup \{\mathbf{e}_1, \cdots, \mathbf{e}_{r + 1}\} = (T_r \setminus \{\mathbf{t}_j\}\} \cup \{\mathbf{e}_{r + 1}\}
  \]
  Since $\mathbf{t}_j$ is in the span of $T_r\cup \{\mathbf{e}_{r + 1}\}$, we have $\mathbf{t}_j \in \bra T_{r + 1}\ket$. So
  \[
    V \supseteq \bra T_{r + 1}\ket \supseteq \bra T_r \ket = V.
  \]
  So $\bra T_{r + 1}\ket = V$.

  Hence we can inductively find $T_n$.
\end{proof}
Note that while we proved this for finite sets, if we assume the Axiom of Choice, we can prove the same for infinite sets of any cardinality by big scary things like Zorn's lemma and transfinite induction, but these are out of the scope of this course, and we needn't care.

\begin{cor}
  Suppose $V$ is a vector space over $\F$ with a basis of order $n$. Then
  \begin{enumerate}
    \item Every basis of $V$ has order $n$.
    \item Any linearly independent set of order $n$ is a basis.
    \item Every spanning set of order $n$ is a basis.
    \item Every finite spanning set contains a basis.
    \item Every linearly independent subset of $V$ can be extended to basis.
  \end{enumerate}
\end{cor}

\begin{proof}
  let $S = \{\mathbf{e}_1, \cdots, \mathbf{e}_n\}$ be the basis for $V$.
  \begin{enumerate}
    \item Suppose $T$ is another basis. Since $S$ is independent and $T$ is spanning, $|T| \geq |S|$. Since $T$ is linearly independent, every finite subset of $T$ is independent. Also, $S$ is spanning. So every finite subset of $T$ has order at most $|S|$. So $|T| \leq |S|$. So $|T| = |S|$.

    \item Suppose now that $T$ is a linearly independent subset of order $n$, but $\bra T\ket \not= V$. Then there is some $\mathbf{v} \in V\setminus \bra T\ket$. We now show that $T\cup \{\mathbf{v}\}$ is independent. Indeed, if
      \[
        \lambda_0 \mathbf{v} + \sum_{i = 1}^m \lambda_i \mathbf{t}_i = 0
      \]
      with $\lambda_i \in \F$, $\mathbf{t}_1, \cdots, \mathbf{t}_m\in T$ distinct, then
      \[
        \lambda_0 \mathbf{v} = \sum_{i = 1}^m (-\lambda_i) t_i.
      \]
      Then $\lambda_0 \mathbf{v} \in \bra T\ket$. So $\lambda_0= 0$. As $T$ is linearly independent, we have $\lambda_0 = \cdots = \lambda_m = 0$. So $T\cup \{\mathbf{v}\}$ is a linearly independent subset of size $> n$. This is a contradiction since $S$ is a spanning set of size $n$.

    \item Let $T$ be a spanning set of order $n$. if $T$ were linearly dependent, then there is some $\mathbf{t}_0, \cdots, \mathbf{t}_m \in T$ distinct and $\lambda_1, \cdots, \lambda_m \in \F$ such that
      \[
        \mathbf{t}_0 = \sum \lambda_i \mathbf{t}_i.
      \]
      So $\mathbf{t}_0 \in \bra T\setminus \{\mathbf{t}_0\}\ket$, ie. $\bra T\setminus \{\mathbf{t}_0\} \ket = V$. So $T\setminus \{\mathbf{t}_0\}$ is a spanning set of order $n - 1$, which is a contradiction.

    \item Suppose $T$ is any finite spanning set. Let $T' \subseteq T$ be a spanning set of least possible size. This exists because $T$ is finite. If $|T'|$ has size $n$, then done by (iii). Otherwise by the Steinitz exchange lemma, it has size $|T'| > n$. So $T'$ must be linearly dependent because $S$ is spanning.  So there is some $\mathbf{t}_0, \cdots, \mathbf{t}_m \in T$ distinct and $\lambda_1, \cdots, \lambda_m \in \F$ such that $\mathbf{t}_0 = \sum \lambda_i \mathbf{t}_i$. Then $T'\setminus \{\mathbf{t}_0\}$ is a smaller spanning set. Contradiction.

    \item Suppose $T$ is a linearly independent set. Since $S$ spans, there is some $S' \subseteq S$ of order $|T|$ such that $(S\setminus S')\cup T$ spans $V$ by the Steinitz exchange lemma. So by (ii), $(S\setminus S')\cup T$ is a basis of $V$ containing $T$.
  \end{enumerate}
\end{proof}
Finally, we can define dimension.
\begin{defi}[Dimension]
  If $V$ is a vector space $\F$ iwht finite basis $S$, then the \emph{dimension} of $V$, written
  \[
    \dim V = \dim_{\F}V = |S|.
  \]
\end{defi}
By the corollary, $\dim V$ does not depend on the choice of $S$. However, it does depend on $\F$. For example, $\dim_\C \C = 1$ (since $\{1\}$ is a basis), but $\dim_\R \C = 2$ (since $\{1, i\}$ is a basis).

After defining the dimension, we can prove a few things about dimensions.
\begin{lemma}
  If $V$ is a finite dimensional vector space over $\F$, $U\subseteq V$ is a proper subspace, then $U$ is finite dimensional and $\dim U < \dim $.
\end{lemma}

\begin{proof}
  Every linearly independent subset of $V$ has size at most $\dim V$. So let $S \subseteq U$ be a linearly independent subset of largest size. We want to show that $S$ spans $U$ and $|S| < \dim V$.

  If $\mathbf{v}\in V\setminus \bra S\ket$, then $S\cup \{\mathbf{v}\}$ is linearly independent. So $\mathbf{v}\not\in U$ by maximality of $S$. This means that $\bra S\ket = U$.

  Since $U\not= V$, there is some $\mathbf{v}\in V\setminus U = V\setminus \bra S\ket$. So $S\cup \{\mathbf{v}\}$ is a linearly independent subset of order $|S| + 1$. So $|S| + 1 \leq \dim V$. In particular, $\dim U = |S| < \dim V$.
\end{proof}

\begin{prop}
  If $U, W$ are subspaces of a finite dimensional vector space $V$, then
  \[
    \dim (U + W) = \dim U + \dim W - \dim (U\cap W).
  \]
\end{prop}
The proof is not hard, as long as we manage to pick the right basis to do the proof. This is our slogan:
\begin{center}
  When you choose a basis, always choose the right basis.
\end{center}
We need a basis for all four of them, and we want to compare the basis. So we want to pick bases that are compatible.

\begin{proof}
  let $R = \{\mathbf{v}_1, \cdots, \mathbf{v}_r\}$ be a basis for $U\cap W$. This is a linearly independent subset of $U$. So we can extend it to be a basis of $U$ by
  \[
    S = \{\mathbf{v}_1, \cdots, \mathbf{v}_r, \mathbf{u}_{r + 1}, \cdots, \mathbf{u}_s\}.
  \]
  Similarly, for $W$, we can obtain a basis
  \[
    T = \{\mathbf{v}_1, \cdots, \mathbf{v}_r, \mathbf{w}_{r + 1}, \cdots, \mathbf{w}_t\}.
  \]
  We want to show that $\dim (U + W) = s + t - r$. It is sufficient to prove that $S\cup T$ is a basis for $U + W$.

  We first show spanning. Suppose $\mathbf{u} + \mathbf{w} \in U + W$, $\mathbf{u}\in U, \mathbf{w}\in W$.  Then $\mathbf{u}\in \bra S\ket$ and $\mathbf{w}\in \bra T\ket$. So $\mathbf{u} + \mathbf{w} \in \bra S\cup T\ket$. So $U + W = \bra S \cup T\ket$.

  To show linear independence, suppose we have a linear relation
  \[
    \sum_{i = 1}^r \lambda_i \mathbf{v}_i + \sum_{j = r + 1}^s \mu_j \mathbf{u}_j + \sum_{k = r + 1}^t \nu_k \mathbf{w}_k = 0.
  \]
  So
  \[
    \sum \lambda_i \mathbf{v}_i + \sum \mu_j \mathbf{u}_j = - \sum \nu_k \mathbf{w}_k.
  \]
  Since the left hand side is something in $U$, and the right hand side is something in $W$, they both lie in $U\cap W$.

  Since $S$ is a basis of $U$, there is only one way of writing the left hand vector as a sum of $\mathbf{v}_i$ and $\mathbf{u}_j$. However, since $R$ is a basis of $U\cap W$, we can write the left hand vector just as a sum of $\mathbf{v}_i$'s. So we must have $\mu_j = 0$ for all $j$. Then we have
  \[
    \sum \lambda_i \mathbf{v}_i + \sum \nu_k \mathbf{w}_k = \mathbf{0}.
  \]
  Finally, since $T$ is linearly independent, $\lambda_i = \nu_k = 0$ for all $i, k$. So $S\cup T$ is linearly independent.
\end{proof}

\begin{prop}(non-examinable)
  If $V$ is a finite dimensional vector space $\F$ and $U\cup V$ is a subspace, then
  \[
    \dim V = \dim U + \dim V/U.
  \]
\end{prop}
We can view this as a linear algebra version of Lagrange's theorem. Combined with the first isomorphism theorem for vector spaces, this gives the rank-nullity theorem.

\begin{proof}
  Let $\{\mathbf{u}_1, \cdots, \mathbf{u}_m\}$ be a basis for $U$ and extend this to a basis $\{\mathbf{u}_1, \cdots, \mathbf{u}_m,\allowbreak \mathbf{v}_{m + 1}, \cdots, \mathbf{v}_n\}$ for $V$. We want to show that $\{\mathbf{v}_{m + 1} + U, \cdots, \mathbf{v}_n + U\}$ is a basis for $V/U$.

  It is easy to see that this spans $V/U$. If $v + U \in V/U$, then we can write
  \[
    \mathbf{v} = \sum \lambda_i \mathbf{u}_i + \sum \mu_i \mathbf{v}_i.
  \]
  Then
  \[
    \mathbf{v} + U = \sum \mu_i (\mathbf{v}_i + U) + \sum \lambda_i (\mathbf{u}_i + U) =  \sum \mu_i (\mathbf{v}_i + U).
  \]
  So done.

  To show that they are linearly independent, suppose that
  \[
    \sum \lambda_i (\mathbf{v}_i + U) = \mathbf{0} + U = U.
  \]
  Then this requires
  \[
    \sum \lambda_i \mathbf{v}_i \in U.
  \]
  Then we can write this as a linear combination of the $\mathbf{u}_i$'s. So
  \[
    \sum \lambda_i \mathbf{v}_i = \sum \mu_j \mathbf{u}_j
  \]
  for some $\mu_j$. Since $\{\mathbf{u}_1, \cdots, \mathbf{u}_m, \mathbf{v}_{n + 1}, \cdots, \mathbf{v}_n\}$ is a basis for $V$, we must have $\lambda_i = \mu_j = 0$ for all $i, j$. So $\{\mathbf{v}_i + U\}$ is linearly independent.
\end{proof}
\subsection{Direct sums}
We are going to define direct sums in many ways in order to confuse students.
\begin{defi}[(Internal) direct sum]
  Suppose $V$ is a vector space over $\F$ and $U, W\subseteq V$ are subspaces. We say that $V$ is the \emph{(internal) direct sum} of $U$ and $W$ if
  \begin{enumerate}
    \item $U + W = V$
    \item $U \cap W = 0$.
  \end{enumerate}
  We write $V = U\oplus W$.

  Equivalently, this requires that every $\mathbf{v}\in V$ can be written uniquely as $\mathbf{u} + \mathbf{w}$ with $\mathbf{u}\in U, \mathbf{w}\in W$. We say that $U$ and $W$ are \emph{complementary subspaces} of $V$.
\end{defi}

\begin{eg}
  Let $V = \R^2$, and $U = \bra \begin{pmatrix}0\\1\end{pmatrix}\ket$. Then $\bra \begin{pmatrix}1\\1\end{pmatrix}\ket$ and $\bra \begin{pmatrix}1\\0\end{pmatrix}\ket$ are both complementary subspaces to $U$ in $V$.
\end{eg}

\begin{defi}[(External) direct sum]
  If $U, W$ are vector spaces over $\F$, the \emph{(external) direct sum} is $U\oplus W = \{(\mathbf{u}, \mathbf{v}): \mathbf{u}\in U, \mathbf{w}\in W\}$, with addition and scalar multiplication componentwise:
  \[
    (\mathbf{u}_1, \mathbf{w}_1) + (\mathbf{u}_2, \mathbf{w}_2) = (\mathbf{u}_1 + \mathbf{u}_2, \mathbf{w}_1 + \mathbf{w}_2),\quad \lambda (\mathbf{u}, \mathbf{w}) = (\lambda \mathbf{u}, \lambda \mathbf{w}).
  \]
\end{defi}
The difference between these two definitions is that the first is decomposing $V$ into smaller spaces, while the second is building a bigger space based on two spaces.

Note, however, that the external direct sum $U\oplus W$ is the internal direct sum of $U$ and $W$ viewed as subspaces of $U\oplus W$, ie. as the internal direct sum of $\{(\mathbf{u}, \mathbf{0}): \mathbf{u}\in U\}$ and $\{(\mathbf{0}, \mathbf{v}): \mathbf{v}\in V\}$.

\begin{defi}[(Multiple) (internal) direct sum]
  If $U_1, \cdots, U_n\subseteq V$ are subspaces of $V$, then $V$ is the \emph{(internal) direct sum}
  \[
    V = U_1 \oplus \cdots \oplus U_n = \bigoplus_{i = 1}^n U_i,
  \]
  if every $\mathbf{v}\in V$ can be written uniquely as $\mathbf{v} = \sum \mathbf{u}_i$ with $\mathbf{u}_i \in U_i$.

  This can be extended to an infinite sum with the same definition, just noting that the sum $\mathbf{v} = \sum \mathbf{u}_i$ has to be finite.
\end{defi}
For more details, see Example Sheet 1 Q. 10, where we prove in particular that $\dim V = \sum \dim U_i$.

\begin{defi}[(Multiple) (external) direct sum]
  If $U_1, \cdots, U_n$ are vector spaces over $\F$, the external direct sum is
  \[
    U_1 \oplus \cdots \oplus U_n = \bigoplus_{i = 1}^n U_i = \{(\mathbf{u}_1, \cdots, \mathbf{u}_n): \mathbf{u}_i \in U_i\},
  \]
  with pointwise operations.

  This can be made into an infinite sum if we require that all but finitely many of the $\mathbf{u}_i$ have to be zero.
\end{defi}

\section{Linear maps}
\subsection{Definitions and examples}
\begin{defi}[Linear map]
  Let $U, V$ be vector spaces over $\F$. Then $\alpha: U\to V$ is a \emph{linear map} if
  \begin{enumerate}
    \item $\alpha(\mathbf{u}_1 + \mathbf{u}_2) = \alpha(\mathbf{u}_1) + \alpha(\mathbf{u}_2)$ for all $\mathbf{u}_i \in U$.
    \item $\alpha(\lambda \mathbf{u}) = \lambda \alpha (\mathbf{u})$ for all $\lambda \in \F, \mathbf{u}\in U$.
  \end{enumerate}
  We write $\mathcal{L}(U, V)$ for the set of linear maps $U\to V$.
\end{defi}
There are a few things we should take note of:
\begin{itemize}
  \item If we are lazy, we can combine the two requirements to the single requirement that
    \[
      \alpha (\lambda \mathbf{u}_1 + \mu \mathbf{u}_2) = \lambda \alpha(\mathbf{u}_1) + \mu \alpha(\mathbf{u}_2).
    \]
  \item It is easy to see that if $\alpha$ is linear, then it is a group homomorphism (if we view vector spaces as groups). In particular, $\alpha (\mathbf{0}) = \mathbf{0}$.
  \item If we want to stress the field $\F$, we say that $\alpha$ is $\F$-linear. For example, complex conjugation is a map $\C \to \C$ that is $\R$-linear but not $\C$-linear.
\end{itemize}

\begin{eg}\leavevmode
  \begin{enumerate}
    \item Let $A$ be an $n\times m$ matrix with coefficients in $\F$. We will write $A\in M_{n, m}(\F)$. Then $\alpha: \F^m \to \F^n$ defined by $\mathbf{v}\to A\mathbf{v}$ is linear.

      Recall matrix multiplication is is defined by: if $A_{ij}$ is the $ij$th coefficient of $A$, then the $i$th coefficient of $A\mathbf{v}$ is $A_{ij}\mathbf{v}_j$. So we have
      \begin{align*}
        \alpha(\lambda \mathbf{u} + \mu \mathbf{v})_i &= \sum_{j = 1}^m A_{ij}(\lambda \mathbf{u} + \mu \mathbf{v})_j \\
        &= \lambda \sum_{j = 1}^m A_{ij}u_j + \mu \sum_{j = 1}^m A_{ij} v_j \\
        &= \lambda \alpha(\mathbf{u})_i + \mu \alpha(\mathbf{v})_i.
      \end{align*}
      So $\alpha$ is linear.
    \item Let $X$ be a set and $g\in \F^X$. Then we define $m_g: \F^x \to \F^x$ by $m_g(f)(x) = g(x) f(x)$. Then $m_g$ is linear. For example, $f(x) \mapsto 2x^2 f(x)$ is linear.
    \item Integration $I: (C([a, b]), \R) \to (C([a, b]), \R)$ defined by $f\mapsto \int_a^x f(t) \;\d t$ is linear.
    \item Differentiation $D: (C^\infty ([a, b]), \R) \to (C^\infty ([a, b]), \R)$ by $ f\mapsto f'$ is linear.
    \item If $\alpha, \beta\in \mathcal{L}(U, V)$, then $\alpha + \beta$ defined by $(\alpha +  \beta)(\mathbf{u}) = \alpha(\mathbf{u}) + \beta(\mathbf{u})$ is linear.

      Also, if $\lambda \in \F$, then $\lambda \alpha$ defined by $(\lambda \alpha)(\mathbf{u}) = \lambda (\alpha (\mathbf{u}))$ is also linear.

      In this way, $\mathcal{L}(U, V)$ is also a vector space over $\F$.
    \item Composition of linear maps is linear. Using this, we can show that many things are linear, like differentiating twice, or adding and then multiplying linear maps.
  \end{enumerate}
\end{eg}

Just like everything else, we want to define isomorphisms.
\begin{defi}[Isomorphism]
  We say a linear map $\alpha: U\to V$ is an \emph{isomorphism} if there is some $\beta: V\to U$ (also linear) such that $\alpha \circ \beta = \id_V$ and $\beta\circ \alpha = \id_U$.

  If there exists an isomorphism $U\to V$, we say $U$ and $V$ are \emph{isomorphic}, and write $U\cong V$.
\end{defi}

\begin{lemma}
  If $U$ and $V$ are vector spaces over $\F$ and $\alpha: U\to V$, then $\alpha$ is an isomorphism iff $\alpha$ is a bijective linear map.
\end{lemma}

\begin{proof}
  If $\alpha$ is an isomorphism, then it is clearly bijective since it has an inverse function.

  Suppose $\alpha$ is a linear bijection. Then as a function, it has an inverse $\beta: V\to U$. We want to show that this is linear. Let $\mathbf{v}_1, \mathbf{v}_2 \in V$, $\lambda, \mu \in \F$. We have
  \[
    \alpha \beta(\lambda \mathbf{v}_1 + \mu \mathbf{v}_2) = \lambda \mathbf{v}_1 + \mu \mathbf{v}_2 = \lambda \alpha \beta (\mathbf{v}_1) + \mu \alpha \beta (\mathbf{v}_2) = \alpha (\lambda \beta(\mathbf{v}_1) + \mu \beta (\mathbf{v}_2)).
  \]
  Since $\alpha$ is injective, we have
  \[
    \beta(\lambda \mathbf{v}_1 + \mu \mathbf{v}_2) = \lambda \beta (\mathbf{v}_1) + \mu \beta (\mathbf{v}_2).
  \]
  So $\beta$ is linear.
\end{proof}

\begin{defi}[Image and kernel]
  Let $\alpha: U\to V$ be a linear map. Then the \emph{image} of $\alpha$ is
  \[
    \im \alpha = \{\alpha (\mathbf{u}): \mathbf{u}\in U\}.
  \]
  The \emph{kernel} of $\alpha$ is
  \[
    \ker \alpha = \{\mathbf{u}: \alpha (\mathbf{u}) = \mathbf{0}\}.
  \]
\end{defi}
It is easy to show that these are subspaces of $V$ and $U$ respectively.

\begin{eg}\leavevmode
  \begin{enumerate}
    \item Let $A\in M_{m, n}(\F)$ and $\alpha: \F^n \to \F^m$ be the linear map $\mathbf{v}\mapsto A\mathbf{v}$. Then the system of linear equations
      \[
        \sum_{j = 1}^m A_{ij}x_j = b_i,\quad 1 \leq i \leq n
      \]
      has a solution iff $(b_1, \cdots, b_n) \in \im \alpha$.

      The kernel of $\alpha$ contains all solution to $\sum_j A_{ij}x_j = 0$.
    \item Let $\beta: C^{\infty}(\R, \R) \to C^{\infty}(\R, \R)$ that sends
      \[
        \beta(f)(t) = f''(t) + p(t) f'(t) + q(t) f(t).
      \]
      for some $p, q\in C^{\infty}(\R, \R)$.

      Then if $y(t) \in \im \beta$, then there is a solution (in $C^\infty (\R, \R)$) to
      \[
        f''(t) + p(t) f'(t) + q(t) f(t) = y(t).
      \]
      Similarly, $\ker \beta$ contains the solutions to the homogeneous equation
      \[
        f''(t) + p(t) f'(t) + q(t) f(t) = 0.
      \]
  \end{enumerate}
\end{eg}
\end{document}
