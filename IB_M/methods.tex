\documentclass[a4paper]{article}

\def\npart {IB}
\def\nterm {Michaelmas}
\def\nyear {2015}
\def\nlecturer {D. B. Skinner}
\def\ncourse {Methods}
\def\nofficial {http://www.damtp.cam.ac.uk/user/dbs26/1Bmethods.html}
\def\nnotready {}

\input{header}

\begin{document}
\maketitle
{\small
\noindent\textbf{Self-adjoint ODEs}\\
Periodic functions. Fourier series: definition and simple properties; Parseval's theorem. Equations of second order. Self-adjoint differential operators. The Sturm-Liouville equation; eigenfunctions and eigenvalues; reality of eigenvalues and orthogonality of eigenfunctions; eigenfunction expansions (Fourier series as prototype), approximation in mean square, statement of completeness.\hspace*{\fill} [5]

\vspace{10pt}
\noindent\textbf{PDEs on bounded domains: separation of variables}\\
Physical basis of Laplace's equation, the wave equation and the diffusion equation. General method of separation of variables in Cartesian, cylindrical and spherical coordinates. Legendre's equation: derivation, solutions including explicit forms of $P_0$, $P_1$ and $P_2$, orthogonality. Bessel's equation of integer order as an example of a self-adjoint eigenvalue problem with non-trivial weight.

\vspace{5pt}
\noindent Examples including potentials on rectangular and circular domains and on a spherical domain (axisymmetric case only), waves on a finite string and heat flow down a semi-infinite rod.\hspace*{\fill} [5]

\vspace{10pt}
\noindent\textbf{Inhomogeneous ODEs: Green's functions}\\
Properties of the Dirac delta function. Initial value problems and forced problems with two fixed end points; solution using Green's functions. Eigenfunction expansions of the delta function and Green's functions.\hspace*{\fill} [4]

\vspace{10pt}
\noindent\textbf{Fourier transforms}\\
Fourier transforms: definition and simple properties; inversion and convolution theorems. The discrete Fourier transform. Examples of application to linear systems. Relationship of transfer function to Green's function for initial value problems.\hspace*{\fill} [4]

\vspace{10pt}
\noindent\textbf{PDEs on unbounded domains}\\
Classification of PDEs in two independent variables. Well posedness. Solution by the method of characteristics. Green's functions for PDEs in 1, 2 and 3 independent variables; fundamental solutions of the wave equation, Laplace's equation and the diffusion equation. The method of images. Application to the forced wave equation, Poisson's equation and forced diffusion equation. Transient solutions of diffusion problems: the error function.\hspace*{\fill} [6]}

\tableofcontents

\setcounter{section}{-1}
\section{Introduction}
In the previous courses, the (partial) differential equations we have seen are mostly linear. For example, we have Laplace's equation:
\[
  \frac{\partial^2 \phi}{\partial x^2} + \frac{\partial \phi}{\partial y^2} = 0,
\]
and the heat equation:
\[
  \frac{\partial \phi}{\partial t} = \kappa \left(\frac{\partial^2 \phi}{\partial x^2} + \frac{\partial^2 \phi }{\partial y^2}\right).
\]
The Schr\"odinger' equation in quantum mechanics is also linear:
\[
  i\hbar \frac{\partial \Phi}{\partial t}= -\frac{\hbar^2}{2m}\frac{\partial^2 \phi}{\partial x^2} + V(x) \Phi(x).
\]
By being linear, these equations have the property that if $\phi_1, \phi_2$ are solutions, then so are $\lambda_1 \phi_1 + \lambda_2 \phi_2$ (for any constants $\lambda_i$).

Why are all these linear? In general, if we just randomly write down a differential equation, most likely it is not going to be linear. So why are all these equations of physics linear?

The answer is that the real world is \emph{not} linear in general. However, often we are not looking for a completely accurate and precise description of the universe. When we have low energy/speed/whatever, we can often quite accurately approximate reality by a linear equation. For example, the equation of general relativity is very complicated and nowhere near being linear, but for small masses and velocities, they reduce to Newton's law of gravitation, which is linear.

The only exception to this seems to be Schr\"odinger's equation. While there are many theories and equations that superseded the Schr\"odinger equation, these are all still linear in nature. It seems that linearity is the thing that underpins quantum mechanics.

Due to the prevalence of linear equations, it is rather important that we understand these equations well, and this is our primary objective of the course.

\section{Vector spaces}
It is often convenient to express our concepts and items in terms of vector spaces. We will start with the relevant definitions.
\begin{defi}[Vector space]
  A \emph{vector space} over $\C$ (or $\R$) is a set $V$ with an operation $+$ which obeys
  \begin{enumerate}
    \item $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$\hfill (commutativity)
    \item $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$\hfill (associativity)
    \item There is some $\mathbf{0}\in V$ such that $\mathbf{0} + \mathbf{u} = \mathbf{u}$ for all $\mathbf{u}$\hfill (identity)
  \end{enumerate}
  We can also multiply vectors by a scalars $\lambda\in \C$, which satisfies
  \begin{enumerate}
    \item $\lambda(\mu \mathbf{v}) = (\lambda \mu) \mathbf{v}$ \hfill (associativity)
    \item $\lambda(\mathbf{u} + \mathbf{v}) = \lambda \mathbf{u} + \lambda \mathbf{v}$ \hfill (distributive in $V$)
    \item $(\lambda + \mu)\mathbf{u} = \lambda \mathbf{u} + \lambda \mathbf{v}$ \hfill (distributive in $\C$)
    \item $1\mathbf{v} = \mathbf{v}$ \hfill (identity)
  \end{enumerate}
\end{defi}
It is often useful to give a vector space an extra structure known as an inner product.
\begin{defi}[Inner product]
  An \emph{inner product} on $V$ is a map $(\cdot, \cdot): V\times V \to \C$ that satisfies
  \begin{enumerate}
    \item $(\mathbf{u}, \lambda \mathbf{v}) = \lambda (\mathbf{u}, \mathbf{v})$ \hfill(linearity in second argument)
    \item $(\mathbf{u}, \mathbf{v} + \mathbf{w}) = (\mathbf{u}, \mathbf{v}) + (\mathbf{u}, \mathbf{w})$ \hfill (additivity)
    \item $(\mathbf{u}, \mathbf{v}) = (\mathbf{v}, \mathbf{u})^*$ \hfill (conjugate symmetry)
    \item $(\mathbf{u}, \mathbf{u}) \geq 0$, with equality iff $\mathbf{u} = \mathbf{0}$ \hfill (positivity)
  \end{enumerate}
  Note that the positivity condition makes sense since conjugate symmetry entails that $(\mathbf{u}, \mathbf{u}) \in \R$.

  The inner product in turn defines a norm $\|\mathbf{u}\| = (\mathbf{u}, \mathbf{u})$ that provides the notion of length and distance.
\end{defi}
It is important to note that we only have linearity in the \emph{second argument}. For the first argument, we have $(\lambda \mathbf{u}, \mathbf{v}) = (\mathbf{v}, \lambda \mathbf{u})^* = \lambda^* (\mathbf{v}, \mathbf{u})^* = \lambda^* (\mathbf{u}, \mathbf{v})$.

\begin{defi}[Basis]
  A set of vectors $\{\mathbf{v}_1, \mathbf{v}_2, \cdots, \mathbf{v}_n\}$ form a \emph{basis} of $V$ iff any $\mathbf{u}\in V$ can be uniquely written as a linear combination
  \[
    \mathbf{u} = \sum_{i = 1}^n \lambda_i \mathbf{v}_i
  \]
  for some scalars $\lambda_i$. The \emph{dimension} of a vector space is the number of basis vectors in its basis.

  A basis is \emph{orthogonal} (with respect to the inner product) if $(\mathbf{v}_i, \mathbf{v}_j) = 0$ whenever $i\not = j$.

  A basis is \emph{orthonormal} (with respect to the inner product) if $(\mathbf{v}_i, \mathbf{v}_i) = 1$ for all $i$.
\end{defi}

Given an orthonormal basis, we can use the inner product to find the expansion of any $\mathbf{u}\in V$ in terms of the basis, for if
\[
  \mathbf{u} = \sum_i \lambda_i \mathbf{v}_i,
\]
taking the inner product with $\mathbf{v}_j$ gives
\[
  (\mathbf{v}_j, \mathbf{u}) = \left(\mathbf{v}_j, \sum_i \lambda_i \mathbf{v}_i\right) = \sum_i \lambda_i (\mathbf{v}_j, \mathbf{v}_i) = \lambda_j,
\]
using additivity and linearity. So we can recover the values of $\lambda_i$.

We have seen all these so far in IA Vectors and Matrices, where a vector is a list of finitely many numbers. However, \emph{functions} can also be thought of as elements of an (infinite dimensional) vector space.

Suppose we have $f, g: \Omega \to \C$. Then we can define the sum $f + g$ by $(f + g)(x) = f(x) + g(x)$. Given scalar $\lambda$, we can also define $(\lambda f)(x) = \lambda f(x)$.

This also makes intuitive sense. We can simply view a functions as a list of numbers, where we list out the values of $f$ at each point. The list could be infinite, but a list nonetheless.

Often, we don't want to look at the set of \emph{all} functions. That would be too huge and uninteresting. A natural class of functions to consider would be the set of solutions to some particular differential solution. However, this doesn't always work. For this class to actually be a vector space, the sum of two solutions (and the scalar multiple of a solution) must also be a solution. This is exactly the requirement that the differential equation is linear. Hence, the set of solutions to a linear differential equation would form a vector space. Linearity pops up again.

Now what about the inner product? A natural definition is
\[
  (f, g) = \int_{\Sigma} f(x)^* g(x) \;\d \mu,
\]
where $\mu$ is some measure. For example, we could integrate $\d x$, or $\d x^2$. This measure specifies how much weighting we give to each point $x$.

Why does this definition make sense? Recall that the usual inner product on finite-dimensional vector spaces is $\sum v_i^* w_i$. Here we are just summing the different components of $v$ and $w$. Recall that we said we can think of the function $f$ as a list of all its values, and this integral is just the sum of all components of $f$ and $g$.

\begin{eg}
  Let $\Sigma = [a, b]$. Then we could take
  \[
    (f, g) = \int_a^b f(x)^* g(x)\;\d x.
  \]
  Alternatively, let $\Sigma = D^2 \subseteq \R^2$ be the unit disk. Then we could have
  \[
    (f, g) = \int_0^1 \int_0^{2\pi}f(r, \theta)^* g(r, \theta)\;\d \theta\; r\;\d r
  \]
\end{eg}
Note that we were careful and said that $\d \mu$ is ``some measure''. We will later see cases where this isn't necessarily just something simple like $\d x\;\d y$.

If $\Sigma$ has a boundary, we will often want to restrict our functions to take particular values on the boundary, known as boundary conditions. Often, we want the boundary conditions to preserve linearity. We call these nice boundary conditions \emph{homogeneous} conditions.

\begin{eg}
  Let $\Sigma = [a, b]$. We could require that $f(a) + 7 f'(b) = 0$, or maybe $f(a) + 3 f''(a) = 0$. These are examples of homogeneous boundary conditions. On the other hand, the requirement $f(a) = 1$ is \emph{not} homogeneous.
\end{eg}

\section{Fourier series}
\begin{defi}[Periodic function]
  A function $f$ is \emph{periodic} if there is some fixed $R$ such that $f(x + R) = f(x)$ for all $x$. A nicer definition, though would be to think of this as a function $f: S^1 \to \C$ from unit circle to $\C$.
\end{defi}
Fourier one day decided that $e^{in\theta}$ (for $n \in Z$) is a good choice of ``basis functions'' for periodic functions. We have
\[
  (e^{im \theta}, e^{in\theta}) = \int_{-\pi}^{\pi} e^{-im\theta} e^{in\theta}\;\d \theta = \int_{-\pi}^\pi e^{i(n - m)\theta}\;\d \theta =
  \begin{cases}
    2\pi & n = m\\
    0    & n\not= m
  \end{cases} = 2\pi \delta_{nm}
\]
Hence the set $\{\frac{1}{\sqrt{2\pi}} e^{in\theta}: n\in \Z\}$ is orthonormal on $\Sigma = S^1$.

Fourier's idea was to use this as a basis for \emph{any} periodic function. Fourier claimed that any $f: S^1 \to \C$ can be expanded in this basis:
\[
  f(\theta) = \sum_{n \in \Z}\hat{f}_n e^{in\theta},
\]
where
\[
  \hat{f}_n = \frac{1}{2\pi} (e^{in\theta}, f) = \frac{1}{2\pi}\int_{-\pi}^\pi e^{-in\theta} f(\theta)\;\d \theta.
\]
However, this is an infinite sum, and there is no guarantee that it converges. Indeed, it sometimes does not.
\end{document}
