\documentclass[a4paper]{article}

\def\npart {III}
\def\nterm {Lent}
\def\nyear {2017}
\def\nlecturer {C. J. B. Brookes}
\def\ncourse {Algebras}
\def\nlectures {TTS.10}

\input{header}

\begin{document}
\maketitle
{\small
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
The aim of the course is to give an introduction to algebras. The emphasis will be on non-commutative examples that arise in representation theory (of groups and Lie algebras) and the theory of algebraic D-modules, though you will learn something about commutative algebras in passing.

Topics I hope to fit in are:

Artinian algebras. Examples, group algebras of finite groups, crossed products. Structure theory. Artin--Wedderburn theorem. Projective modules. Blocks.

Noetherian algebras. Examples, quantum plane and quantum torus, differential operator algebras, enveloping algebras of finite dimensional Lie algebras. Structure theory. Injective hulls, uniform dimension and Goldie's theorem.

Hochschild chain and cochain complexes. Hochschild homology and cohomology. Gerstenhaber algebras.

$K_0$ and $K_1$.

Deformation of algebras. Quantum co-ordinate algebras and quantum enveloping algebras.

Coalgebras, bialgebras and Hopf algebras. Quantum groups.

\subsubsection*{Pre-requisites}
It will be assumed that you have attended a first course on ring theory, eg IB Groups, Rings and Modules. Experience of other algebraic courses such as II Representation Theory, Galois Theory or Number Fields, or III Lie algebras will be helpful but not necessary.
}
\tableofcontents

\setcounter{section}{-1}
\section{Introduction}
We start with the definition of an algebra. Throughout the course, $k$ will be a field.
\begin{defi}[$k$-algebra]\index{$k$-algebra}\index{algebra}
  A (unital) associative $k$-algebra is a $k$-vector space $A$ together with an associative product $m: A \otimes A \to A$, written $(x, y) \mapsto xy$, which is $k$-bilinear, and a $k$-linear map $u: k \to A$ such that $u(1)$ is the identity of the multiplication of $A$.
\end{defi}
We will mostly talk about $k$-algebras, but other algebraic structures will appear, eg. Lie algebras.

\begin{eg}
  Let $K/k$ be a finite field extension. Then $K$ is a $k$-algebra.
\end{eg}

\begin{eg}
  The $n\times n$ matrices $M_n(k)$ over $k$ are non-commutative $k$-algebras.
\end{eg}

\begin{eg}
  The quaternions $\H$ is an $\R$-algebra, with an $\R$-basis $1, i, j, k$, and multiplication given by
  \[
    i^2 = j^2 = k^2 = 1,\quad ij = k,\quad ji = -k.
  \]
  This is in fact a \term{division algebra} (or \term{skew fields}), ie. the non-zero elements have multiplicative inverse.
\end{eg}

\begin{eg}
  Let $G$ be a finite group. Then the group algebra
  \[
    kG = \left\{\sum \lambda_g g: g \in G, \lambda_g \in k\right\}
  \]
  with the obvious multiplication induced by the group operation is a $k$-algebra.

  These are the associative algebras underlying the representation theory of finite groups.
\end{eg}

Most of the time, we will just care about algebras that are finite-dimensional as $k$-algebras. However, the results actually hold for more general algebras, known as \emph{Artinian algebras}.

These algebras are defined by some finiteness condition on the ideals.

\begin{defi}[Ideal]\index{ideal}
  A \emph{left ideal} of $A$ is a $k$-subspace of $A$ such that if $x \in A$ and $y \in I$, then $xy \in I$. A \emph{right ideal} is defined similarly, and an \emph{ideal} is something that is both a left ideal and a right ideal.
\end{defi}
Since the multiplication is not necessarily commutative, we have to make the distinction between left and right things. Most of the time, we just talk about the left case, as the other case is entirely analogous.

The definition we want is the following:
\begin{defi}[Artinian algebra]\index{Artinian algebra}\index{algebra!Artinian}
  An algebra $A$ is \emph{left Artinian} if it satisfies the \term{descending chain condition} (\term{DCC}) on left ideals, ie. if we have a descending chain of left ideals
  \[
    I_1 \geq I_2 \geq I_3 \geq \cdots,
  \]
  then there is some $N$ such that $I_{N + m} = I_{N}$ for all $m \geq 0$.

  We say an algebra is \emph{Artinian} if it is both left and right Artinian.
\end{defi}

\begin{eg}
  Any finite-dimensional algebra is Artinian.
\end{eg}

The main classification theorem for Artinian algebras we will prove is the following result:
\begin{thm}[Artin--Wedderburn theorem]\index{Artin--Wedderburn theorem}
  Let $A$ be a left-Artinian algebra such that the intersection of the maximal left ideals is zero, then $A$ is the direct sum of finitely many matrix algebras over division algebras.
\end{thm}
When we actually get to the theorem, we will rewrite this in a way that seems a bit more natural.

This theorem is useful for representation theory, as the group algebra of a finite group is a finite-dimensional algebra, hence Artinian.

After studying Artinian rings, we'll talk about Noetherian algebras.
\begin{defi}[Noetherian algebra]\index{Noetherian algebra}\index{algebra!Noetherian}
  An algebra is \emph{left Noetherian} if it satisfies the \term{ascending chain condition} (\term{ACC}) on left ideals, ie. if
  \[
    I_1 \leq I_2 \leq I_3 \leq \cdots
  \]
  is an ascending chain of left ideals, then there is some $N$ such that $I_{N + m} = I_N$ for all $m \geq 0$.

  Similarly, we say an algebra is \emph{Noetherian} if it is both left and right Noetherian.
\end{defi}

We can again look at some examples.
\begin{eg}
  Again all finite-dimensional algebras are Noetherian.
\end{eg}

\begin{eg}
  In the commutative case, Hilbert's basis theorem tells us a polynomial algebra $k[X_1, \cdots, X_k]$ in finitely many variables is Noetherian. Similarly, the power series rings $k[[X_1, \cdots, X_n]]$ are Noetherian.
\end{eg}

\begin{eg}
  The \term{universal enveloping algebra} of a finite-dimensional Lie algebra are the (associative!) algebras that underpin the representation theory of these Lie algebras.
\end{eg}

\begin{eg}
  Some differential operator algebras are Noetherian. We assume $\Char k = 0$. Consider the polynomial ring $k[X]$. We have operators ``multiplication by $X$'' and ``differentiate with respect to $X$'' on $k[X]$. We can form the algebra $k[X, \frac{\partial}{\partial x}]$ of differential operators on $k[X]$. This is called the \term{Weyl algebra} $A_1$. This is a non-commutative Noetherian algebra.
\end{eg}

\begin{eg}
  Some group algebras are Noetherian. Clearly all group algebras of finite groups are Noetherian, but the group algebras of certain infinite groups are Noetherian. For example, we can take
  \[
    G = \left\{
      \begin{pmatrix}
        1 & \lambda & \mu\\
        0 & 1 & \nu\\
        0 & 0 & 0
      \end{pmatrix}: \lambda, \mu, \nu \in \Z
    \right\}.
  \]
\end{eg}

We'll see that left Artinian implies left Noetherian. There is a general theory of Noetherian algebras, but is not as useful as that in commutative algebra.

In the commutative case, we often look at $\Spec A$, the set of prime ideals of $A$. However, sometimes in the non-commutative there are few prime ideals, and so $\Spec$ is not going to keep us busy.
\begin{eg}
  In the Weyl algebra $A_1$, the only prime ideals are $0$ and $A_1$.
\end{eg}

We will prove a theorem of Goldie:
\begin{thm}[Goldie's theorem]\index{Goldie's theorem}
  Let $A$ be a left Noetherian such that the intersection of prime ideals is zero. Then $A$ has a \term{full ring of quotients} which is the finite sum of matrix algebras over division algebras, where the full ring of quotients is what we obtain by adding inverses to everything that is not a zero divisor.
\end{thm}

Some types of Noetherian algebras can be thought of as non-commutative polynomial algebras and non-commutative power series, ie. they are \emph{deformations} of the analogous commutative algebra. For example, we say $A_1$ is a deformation of the polynomial algebra $k[X, Y]$, where instead of having $XY - YX = 0$, we have $XY - YX = 1$. This also applies to enveloping algebras and Iwasawa algebras. We study when one can deform the multiplication so that it remains associative, and this is bound up with the cohomology theory of associative algebras --- \emph{Hochschild cohomology}. The Hochschild complex has additional algebraic structure, and this will allow us to understand how we can deform the algebra.

If time remains, we'll talk about about bialgebras, Hopf algebras and quantum groups. In a bialgebra, one also has a comultiplication map $A \to A \otimes A$, which in representation theory is crucial in saying how to regard a tensor product of two representations as a representation.

We finish with a little bit of history. In 1890's, Hilbert proved some big theorems in complex polynomial algebras. In 1920's, Noether came and abstracted out the key properties that made Hilbert's work work, and came up with the notion of the Noetherian property. In 1930s and 1940s, the development of standard commutative algebra. In 1945 came Hochschild, and in 1960s came Goldie. In 1960 we had Gerstenhaber, who told us about the structure of the Hochschild complex. In the 1960s and 1970s, the study of enveloping algebras of differential operators started, and from the 1980s we started studying quantum groups, whose motivation came from mathematical physics, but got the algebraists excited.

\section{Artinian algebras}
\subsection{Artinian algebras}
We start with some general definitions. Recall the definitions of right and left Artinian algebras from the introduction. Most examples we'll meet are in fact finite-dimensional vector spaces over $k$. However, we can also look at some perverse examples:

\begin{eg}
  Let
  \[
    A = \left\{
      \begin{pmatrix}
        r & s\\
        0 & t
      \end{pmatrix}: r \in \Q, s, t \in \R
    \right\}
  \]
  Then this is left Artinian but not right Artinian.
\end{eg}

As in the case of commutative algebra, we can study the modules of an algebra.
\begin{defi}[Module]\index{module}\index{bimodule}
  Let $A$ be an algebra. A \emph{left $A$-module} is a $k$-vector space $M$ and a map
  \[
    \begin{tikzcd}[cdmap]
      A \times M \ar[r] & M\\
      (x, m) \ar[r, maps to] & xm
    \end{tikzcd}
  \]
  such that
  \begin{align*}
    (x + y)m &= xm + ym\\
    (xy)m &= x(ym).
  \end{align*}
  Right $A$-modules are defined similarly.

  An \emph{$A\mdash A$-bimodule} is a vector space $M$ that is both a left $A$-module and a right $A$-module, such that the two actions commute --- for $a, b \in A$ and $x \in M$, we have
  \[
    a(xb) = (ax)b.
  \]
\end{defi}

\begin{eg}
  The algebra $A$ itself is a left $A$-module. We write this as $_A A$, and call this the \term{left regular representation}. Similarly, the right action is denoted $A_A$. These two actions are compatible by associativity, so $A$ is an $A\mdash A$-bimodule.
\end{eg}

If we write $\End_k(A)$ for the $k$-linear maps $A \to A$, then $\End_k$ is naturally a $k$-algebra by composition, and we have a $k$-algebra homomorphism $A \to \End_k(A)$ that sends $a \in A$ to multiplication by $a$.

However, in the right case, we do not get such a map. Instead, what we get is a map $A \to \End_k(A)^\op$, where
\begin{defi}[Opposite algebra]\index{opposite algebra}\index{algebra!opposite}{$A^\op$}
  Let $A$ be a $k$-algebra. We define the \emph{opposite algebra} $A^\op$ where $A^\op$ has the same underlying vector space, but we define the multiplication
  \[
    x \cdot y = yx.
  \]
  Here on the left we have the multiplication in $A^\op$ and on the right we have the multiplication in $A$.
\end{defi}
In general, a left $A$-module is a right $A^\op$-module.

As in the case of ring theory, we can talk about prime ideals. However, we will adopt a slightly different definition:
\begin{defi}[Prime ideal]\index{prime ideal}\index{ideal!prime}
  An ideal $P$ is \emph{prime} if it is a proper ideal, and if $I$ and $J$ are ideals with $IJ \subseteq P$, then either $I \subseteq P$ or $J \subseteq P$.
\end{defi}
It is an exercise to check that this coincides in the commutative case with the definition using elements.

\begin{defi}[Annihilator]\index{annihilator}
  Let $M$ be a left $A$-module and $m \in M$. We define the \emph{annihilators} to be
  \begin{align*}
    \Ann(m) &= \{a \in A: am = 0\}\\
    \Ann(M) &= \{a \in A: am = 0\text{ for all }m \in M\} = \bigcap_{m \in M} \Ann(m).
  \end{align*}
\end{defi}
Note that $\Ann(m)$ is a left ideal of $A$, and is in fact the kernel of the $A$-module homomorphism $A \to M$ given by $x \mapsto xm$. We'll denote the image of this map by $Am$, a left submodule of $M$, and we have
\[
  \frac{A}{\Ann(m)} \cong Am.
\]
On the other hand, it is easy to see that $\Ann(M)$ is an fact a (two-sided) ideal.

\begin{defi}[Simple module]\index{simple module}\index{irreducible module}\index{module!simple}\index{module!irreducible}
  A non-zero module $M$ is \emph{simple} or \emph{irreducible} if the only submodules of $M$ are $0$ and $M$.
\end{defi}

It is easy to see that
\begin{prop}
  Let $A$ be an algebra and $I$ a left ideal. Then $I$ is a maximal left ideal iff $A/I$ is simple.
\end{prop}

\begin{eg}
  $\Ann(m)$ is a maximal left ideal iff $Am$ is irreducible.
\end{eg}

\begin{prop}
  Let $A$ be an algebra and $M$ a simple module. Then $M \cong A/I$ for some (maximal) left ideal $I$ of $A$.
\end{prop}

\begin{proof}
  Pick an arbitrary element $m \in M$, and define the $A$-module homomorphism $\varphi: A \to M$ by $\varphi(a) = am$. Then the image is a non-trivial submodule, and hence must be $M$. Then by the first isomorphism theorem, we have $M \cong A/\ker I$.
\end{proof}

Before we start doing anything, we note the following convenient lemma:
\begin{lemma}
  Let $M$ be a finitely-generated $A$ module. Then $M$ has a maximal proper submodule $M'$.
\end{lemma}

\begin{proof}
  Let $m_1, \cdots, m_k \in M$ be a minimal generating set. Then in particular $N = \bra m_1, \cdots, m_{k - 1}\ket$ is a proper submodule of $M$. Moreover, a submodule of $M$ containing $N$ is proper iff it does not contain $m_k$, and this property is preserved under increasing unions. So by Zorn's lemma, there is a maximal proper submodule.
\end{proof}

\begin{defi}[Jacobson radical]\index{Jacobson radical}\index{$J(A)$}
  The \index{Jacobson radical} $J(A)$ of $A$ is the intersection of all maximal left ideals.
\end{defi}
This is in fact an ideal, and not just a left one, because
\[
  J(A) = \bigcap \{\text{maximal left ideals}\} = \bigcap_{m \in M\text{ simple}} \Ann(m) = \bigcap_{M\text{ simple}} \Ann(M),
\]
which we have established is an ideal. However, it still looks as if this definition depends on the sidedness. However, we have the following result:
\begin{lemma}[Nakayama lemma]\index{Nakayama lemma}
  The following are equivalent for a left ideal $I$ of $A$.
  \begin{enumerate}
    \item $I < J(A)$
    \item For any finitely-generated left $A$-module $m$, we have $IM = M$ implies $M = 0$, where $IM$ is the module generated by elements of the form $xm$, with $x \in I$ and $m \in M$.
    \item $G = \{1 + x: x \in I\} = 1 + I$ is a subgroup of the unit group of $A$.
  \end{enumerate}
\end{lemma}
Note that this shows that the Jacobson radical is the largest ideal satisfying (iii), which is something that does not depend on handedness.

\begin{proof}\leavevmode
  \begin{itemize}
    \item (i) $\Rightarrow$ (ii): Suppose $I < J(A)$ and $M \not= 0$ is a finitely-generated $A$-module, and we'll see that $IM \lneq M$.

      Let $N$ be a maximal submodule of $M$. Then $M/N$ is a simple module, so for any $\bar{m} \in M/N$, we know $\Ann(\bar{m})$ is a maximal left ideal. So $J(A) \leq \Ann(M/N)$. So $J(A) N \leq N \lneq M$.
    \item (ii) $\Rightarrow$ (iii): Assume (ii). We let $x \in I$ and set $y = 1 + x$. Hence $1 = y - x \in Ay + I$. Since $Ay + I$ is a left ideal, we know $Ay + I = A$. In other words, we know
      \[
        I \left(\frac{A}{Ay}\right) = \frac{A}{Ay}.
      \]
      Now using $2$ on the finitely-generated module $A/Ay$ (it is in fact generated by $1$), we know that $A/Ay = 0$. So $A = Ay$. So there exists $z \in A$ such that $1 = zy = z(1 + x)$. So $(1 + x)$ has a left inverse, and in fact $z$ is in $G$, since we can write $z = 1 - zx$. This is enough to show that $G$ is a group.
    \item (iii) $\Rightarrow$ (i): Suppose $I$ is a maximal left ideal of $A$. Let $x \in I$. If $x \not \in I_1$, then $I_1 + Ax = A$ by maximality of $I$. So $1 = y + zx$ for some $y \in I$ and $z \in A$. So $y = 1 - zx \in G$. So $y$ is invertible. But $y \in I_1$. So $I_1 = A$. This is a contradiction. So we found that $I < I_1$, and this is true for all maximal left ideals $I_1$. Hence $I \leq J(A)$.
  \end{itemize}
\end{proof}

\begin{defi}[Semisimple algebra]\index{semisimple algebra}\index{algebra!semisimple}
  An algebra is \emph{semisimple} if $J(A) = 0$.
\end{defi}

\begin{eg}
  For any $A$, we know $A/J(A)$ is always semisimple.
\end{eg}

\begin{eg}
  Consider $M_n(k)$. We let $e_i$ be the matrix with $1$ in the $(i, i)$th entry and zero everywhere else. This is idempotent, ie. $e_i^2 = e_i$. It is also straightforward to check that
  \[
    A e_i =
    \left\{
      \begin{pmatrix}
        0 & \cdots & 0 & a_1 & 0 & \cdots & 0\\
        0 & \cdots & 0 & a_2 & 0 & \cdots & 0\\
        \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
        0 & \cdots & 0 & a_n & 0 & \cdots & 0
      \end{pmatrix}
    \right\}
  \]
  The non-zero column is, of course, the $i$th column. Similarly, $e_i A$ is the matrices that are zero apart from in the $i$th row. These are in fact all left and all right ideals respectively. So the only $2$-ideals are $0$ and $A$.

  Also, we note that $A_A$ is a direct sum of the simple left modules $A e_i$ and $A_A$ is the direct sum of the simple right modules $e_i A$.
\end{eg}

\begin{defi}[Simple algebra]\index{simple algebra}\index{algebra!simple}
  An algebra is \emph{simple} if the only ideals are $0$ and $A$.
\end{defi}
Thus, we can observe that $M_n(k)$ is a simple algebra.

\begin{defi}[Completely reducible]\index{completely reducible}
  A module $M$ of $A$ is \emph{completely reducible} iff it is a sum of simple modules.
\end{defi}

\begin{prop}
  Let $M$ be an $A$-module. Then the following are equivalent:
  \begin{enumerate}
    \item $M$ is completely reducible.
    \item $M$ is the direct sum of simple modules.
    \item Every module of $M$ has a \term{complement}, ie. for any submodule $N$ of $M$, there is a complement $N'$ such that $M = N \oplus N'$.
  \end{enumerate}
\end{prop}

\begin{proof}\leavevmode
  \begin{itemize}
    \item (i) $\Rightarrow$ (ii): Let $M$ be completely reducible, and consider the set
      \[
        \left\{\{S_\alpha \leq M\} : S_\alpha\text{ are simple},\; \sum S_\alpha\text{ is a direct sum}\right\}.
      \]
      Notice this set is closed under increasing unions, since the property of being a direct sum is only checked on finitely many elements. So by Zorn's lemma, it has a maximal element, and let $N$ be the sum of the elements.

      Suppose this were not all of $M$. Then there is some $S \leq M$ such that $S \not\subseteq N$. Then $S \cap N \leq S$. By simplicity, they intersect trivially. So $S + N$ is a direct sum, which is a contradiction. So we must have $N = M$, and $M$ is the direct sum of simple modules.
    \item (ii) $\Rightarrow$ (i) is trivial.
    \item (i) $\Rightarrow$ (iii): Let $N \leq M$ be a submodule, and consider
      \[
        \left\{\{S_\alpha \leq M\} : S_\alpha\text{ are simple},\; N + \sum S_\alpha\text{ is a direct sum}\right\}.
      \]
      Again this set has a maximal element, and let $P$ be the direct sum of those $S_\alpha$. Again if $P \oplus N$ is not all of $M$, then pick an $S \leq M$ simple such that $S$ is not contained in $P \oplus N$. Then again $S \oplus P \oplus N$ is a direct sum, which is a contradiction.
    \item (iii) $\Rightarrow$ (i): Let $x \in M$. We show that there is some simple $S \leq M$ such that $x \in S$. Let $N$ be a maximal submodule that does not contain $x$, and let $S$ be its complement. If $S$ is not simple, then there is some non-trivial $0 < P < S$. Then $N \oplus P$ has a complement, say $Q$. Then we can write
      \[
        \begin{array}{ccccccc}
          M &=& N &\oplus& P &\oplus& Q\\
          x &=& n &+& p &+& q
        \end{array}
      \]
      Since $x \not\in N$, we may wlog $p \not= 0$. Then $N \oplus Q$ does not contain $x$, and is strictly larger than $N$, a contradiction. So $S$ must be simple, and is a simple module containing $x$.
  \end{itemize}
\end{proof}

Using these different characterizations, we can prove that completely reducible modules are closed under the familiar ex operations.
\begin{prop}
  Sums, submodules and quotients of completely reducible modules are completely reducible.
\end{prop}

\begin{proof}
  It is clear by definition that sums of completely reducible modules are completely reducible.

  To see that submodules of completely reducible modules are completely reducible, let $M$ be completely reducible, and $N \leq M$. Then for each $x \in N$, there is some simple submodule $S \leq M$ containing $x$. Since $S \cap N \leq S$ and contains $x$, it must be $S$, ie. $S \subseteq N$. So $N$ is the sum of simple modules.

  Finally, to see quotients are completely reducible, if $M$ is completely reducible and $N$ is a submodule, then we can write
  \[
    M = N \oplus P
  \]
  for some $P$. Then $M/N \cong P$, and $P$ is completely reducible.
\end{proof}

We will show that every left Artinian algebra is completely reducible over itself iff it is completely irreducible. We can in fact prove a more general fact for $A$-modules. To do so, we need a generalization of the Jacobson radical.

\begin{defi}[Radical]\index{radical}
  For a module $M$, we write $\Rad(M)$ for the intersection of maximal submodules of $M$, and call it the \emph{radical} of $M$.
\end{defi}
Thus, we have $\Rad(_A A) = J(A) = \Rad(A_A)$. % why

\begin{prop}
  Let $M$ be an $A$-module satisfying the descending chain condition on submodules. Then $M$ is completely reducible iff $\Rad(M) = 0$.
\end{prop}

\begin{proof}
  It is clear that if $M$ is completely reducible, then $\Rad(M) = 0$, as for any $x \in M$, there is a simple submodule containing $x$, and its complement has to be maximal.

  Conversely, if $\Rad(M) = 0$, we note that since $M$ satisfies the descending chain condition on submodules, there must be a \emph{finite} collection $M_1, \cdots, M_n$ of maximal submodules whose intersection vanish. Then consider the map
  \[
    \begin{tikzcd}[cdmap]
      M \ar[r] & \displaystyle\bigoplus_{i = 1}^n \frac{M}{M_i}\\
      x \ar[r, maps to]& (x + M_1, x + M_2, \cdots, x + M_n)
    \end{tikzcd}
  \]
  The kernel of this map is the intersection of the $M_i$, which is trivial. So this embeds $M$ as a submodule of $\bigoplus \frac{M}{M_i}$. But each $\frac{M}{M_i}$ is simple, so $M$ is a submodule of a semisimple algebra, hence semisimple.
\end{proof}

\begin{cor}
  If $A$ is a semi-simple left Artinian algebra, then $_AA$ is completely reducible.
\end{cor}

\begin{cor}
  If $A$ is a semi-simple left Artinian algebra, then every left $A$-module is completely reducible.
\end{cor}

\begin{proof}
  Every $A$-module $M$ is a quotient of sums of $_AA$. Explicitly, we have a map
  \[
    \begin{tikzcd}[cdmap]
      \displaystyle\bigoplus_{m \in M} {}_A A \ar[r] & M\\
      (a_m) \ar[r, maps to] & \sum a_m m
    \end{tikzcd}
  \]
  Then this map is clearly surjective, and thus $M$ is a quotient of $\bigoplus_M {}_AA$.
\end{proof}

If $A$ is not semi-simple, then it turns out it is rather easy to figure out radical of $M$, at least if $M$ is finitely-generated.
\begin{lemma}
  Let $A$ be left Artinian, and $M$ a finitely generated left $A$-module, then $J(A) M = \Rad(M)$.
\end{lemma}

\begin{proof}
  Let $M'$ be a maximal submodule of $M$. Then $M/M'$ is simple, and is in fact $A/I$ for some maximal left ideal $I$. Then we have
  \[
    J(A) \left(\frac{M}{M'}\right) = 0,
  \]
  since $J(A) < I$. Therefore $J(A) M \leq M'$. So $J(A)M \leq \Rad(M)$.

  Conversely, we know $\frac{M}{J(A) M}$ is an $A/J(A)$-module, and is hence completely reducible as $A/J(A)$ is semi-simple (and left Artinian). Since an $A$-submodule of $\frac{M}{J(A) M}$ is the same as an $A/J(A)$-submodule, it follows that it is completely reducible as an $A$-module as well. So
  \[
    \Rad\left(\frac{M}{J(A)M}\right) = 0,
  \]
  and hence $\Rad(M) \leq J(A) M$.
\end{proof}

\begin{prop}
  Let $A$ be left Artinian. Then
  \begin{enumerate}
    \item $J(A)$ is nilpotent, ie. there exists some $r$ such that $J(A)^r = 0$.
    \item If $M$ is a finitely-generated left $A$-module, then it is both left Artinian and left Noetherian.
    \item $A$ is left Noetherian.
  \end{enumerate}
\end{prop}

\begin{proof}\leavevmode
  \begin{enumerate}
    \item Since $A$ is left-Artinian, and $\{J(A)^r: r \in \N\}$ is a descending chain of ideals, it must eventually be constant. So $J(A)^r = J(A)^{2r}$ for some $r$. If this is non-zero, then again using the descending chain condition, we see there is a left ideal $I$ with $J(A)^r I \not= 0$ that is minimal with this property (one such ideal exists, say $J(A)$ itself).

      Now pick $x \in I$ with $J(A)^r x \not = 0$. In particular, $\not= 0$. Then by minimality, we must have $I = J(A)^r x$, as we know $J(A)^r x \subseteq I$. So there exists some $a \in J(A)^r$ with $x = ax$. So
      \[
        (1 - a) x = 0.
      \]
      But $1 - a$ is a unit. So $x = 0$. This is a contradiction. So $J(A)^r = 0$.
    \item Let $M_i = J(A)^i M$. Then $M_i/M_{i + 1}$ is annihilated by $J(A)$, and hence completely reducible (it is a module over semi-simple $A/J(A)$). Since $M$ is a finitely generated left $A$-module for a left Artinian algebra, it satisfies the descending chain condition for submodules (exercise), and hence so does $M_i/M_{i + 1}$. % fill in exercise

      So we know $M_i/M_{i + 1}$ is a finite sum of simple modules, and therefore satisfies the ascending chain condition. So $M_i/M_{i + 1}$ is left Noetherian, and hence $M$ is (exercise).

    \item Follows from (ii) since $A$ is a finitely-generated left $A$-module.
  \end{enumerate}
\end{proof}

A last little lemma before doing Artin--Wedderburn:
\begin{lemma}
  Let $A$ be left Artinian and $J(A)$ is finitely-generated as a left $A$-module, and $_AA$ is completely reducible, then $A$ is semi-simple.
\end{lemma}

\begin{proof}
  If $_AA$ is completely reducible, then $J(A)$ has a complement. Write
  \[
    _A A = J(A) \oplus \frac{_AA}{J(A)}.
  \]
  Multiplying on the left by $J(A)$ gives $J(A) = J(A)^2$. Since $J(A)$ is nilpotent, this implies we must have $J(A) = 0$. % or by nakamyama?
\end{proof}

\subsection{Artin--Wedderburn theorem}
We are going to state the Artin--Wedderburn theorem for right things, because this makes the notation easier for us.
\begin{thm}[Artin--Wedderburn theorem]\index{Artin--Wedderburn theorem}
  Let $A$ be a semisimple right Artinian algebra. Then
  \[
    A = \bigoplus_{i = 1}^r M_{n_i}(D_i),
  \]
  for some division algebra $D_i$, and these factors are uniquely determined.
  
  $A$ has exactly $r$ isomorphism classes of simple (right) modules $S_i$, and
  \[
    \End_A (S_i) = \{\text{$A$-module homomorphisms $S_i \to S_i$}\} \cong D_i,
  \]
  and
  \[
    \dim_{D_i}(S_i) = n_i.
  \]
  If $A$ is simple, then $r = 1$.
\end{thm}
If we had the left version instead, then we need to insert $\op$'s somewhere.

To prove this, we use that $\End_A(A_A) \cong A$, where the endomorphism given by $a \in A$ is left-multiplication by $A$.

Artin--Wedderburn is an easy consequence of two trivial lemma. The first is Shur's lemma:
\begin{lemma}[Schur's lemma]\index{Schur's lemma}
  Let $M_1, M_2$ be simple right $A$-modules. Then either $M_1 \cong M_2$, or $\Hom_A(M_1, M_2) = 0$. If $M$ is a simple $A$-module, then $\End_A(M)$ is a division algebra.
\end{lemma}

\begin{proof}
  A non-zero $A$-module homomorphism $M_1 \to M_2$ must be injective, as the kernel is submodule. Similarly, the image has to be the whole thing since the image is a submodule. So this must be an isomorphism, and in particular has an inverse. So the last part follows as well.
\end{proof}

The proof of Artin--Wedderburn relies on the isomorphism
\[
  \End_A(A_A) \cong A_A.
\]
To see this isomorphism, note that any endomorphism of $A_A$ is uniquely determined by the image of $1$, and this gives a map $\End_A(A_A) \mapsto A_A$. Conversely, any element $x \in A_A$ determines an endomorphism of $A_A$ given by left-multiplication by $x$. This is indeed a homomorphism since $A_A$ is a \emph{right} module.

This is really all we need, but we have the following slightly more general result:
\begin{lemma}\leavevmode
  \begin{enumerate}
    \item If $M$ is a right $A$-module and $e$ is an idempotent in $A$, ie. $e^2 = e$, then $Me \cong \Hom_A(eA, M)$.
    \item We have
      \[
        eAe \cong \End_A(eA).
      \]
      In particular, we can take $e = 1$, and recover $\End_A(A_A) \cong A$.
  \end{enumerate}
\end{lemma}

\begin{proof}\leavevmode
  \begin{enumerate}
    \item We define maps
      \[
        \begin{tikzcd}[cdmap]
          me \ar[r, maps to] & (ex \mapsto mex)\\
          Me \ar[r, "f_1", yshift=2] & \Hom(eA, M) \ar[l, "f_2", yshift=-2]\\
          \alpha(e) & \alpha \ar[l, maps to]
        \end{tikzcd}
      \]
      We note that $\alpha(e) = \alpha(e^2) = \alpha(e) e \in Me$. So this is well-defined. By inspection, these maps are inverse to each other. So we are done.

      Note that we might worry that we have to pick representatives $me$ and $ex$ for the map $f_1$, but in fact we can also write it as $f(a)(y) = ay$, since $e$ is idempotent. So we are safe.
    \item Immediate from above by putting $M = eA$.
  \end{enumerate}
\end{proof}

\begin{lemma}
  Let $M$ be a completely reducible right $A$-module. We write
  \[
    M = \bigoplus S_i^{n_i},
  \]
  where $\{S_i\}$ are distinct simple $A$-modules. Write $D_i = \End_A(S_i)$, which we already know is a division algebra. Then
  \[
    \End_A (S_i^{n_i}) \cong M_{n_i} (D_i),
  \]
  and
  \[
    \End_A(M) = \bigoplus M_{n_i} (D_i)
  \]
\end{lemma}

\begin{proof}
  The result for $\End_A(S_i^{n_i})$ is just the familiar fact that a homomorphism $S^n \to S^m$ is given by an $m \times n$ matrix of maps $S \to S$ (in the case of vector spaces over a field $k$, we have $\End(k) \cong k$, so they are matrices with entries in $k$). Then by Schur's lemma, we have
  \[
    \End_A(M) = \bigoplus_i \End_A(M_i) \cong M_{n_i}(D_i).
  \]
\end{proof}

We now prove Artin--Wedderburn.
\begin{proof}[Proof of Artin--Wedderburn]
  If $A$ is semi-simple, then it is completely reducible as a right $A$-module. So we have
  \[
    A \cong \End(A_A) \cong \bigoplus M_{n_i}(D_i).
  \]
  We now decompose each $M_{n_i}(D_i)$ into a sum of simple modules. We know each $M_{n_i}(D_i)$ is a non-trivial $M_{n_i}(D_i)$ module in the usual way, and the action of the other summands is trivial. We can simply decompose each $M_{n_i}(D_i)$ as the sum of submodules of the form
  \[
    \left\{
      \begin{pmatrix}
        0 & 0 & \cdots & 0 & 0\\
        \vdots & \vdots & \ddots & \vdots & \vdots\\
        0 & 0 & \cdots & 0 & 0\\
        a_1 & a_2 & \cdots & a_{n_i - 1} & a_{n_i}\\
        \vdots & \vdots & \ddots & \vdots & \vdots\\
        0 & 0 & \cdots & 0 & 0 \\
      \end{pmatrix}
    \right\}
  \]
  and there are $n_i$ components. We immediately see that if we write $S_i$ for this submodule, then we have
  \[
    \dim_{D_i}(S_i) = n_i.
  \]
  Finally, we have to show that every simple module $S$ of $A$ is one of the $S_i$. We simply have to note that if $S$ is a simple $A$-module, then there is a non-trivial map $f: A \to S$ (say by picking $x \in S$ and defining $f(a) = xa$). Then in the decomposition of $A$ into a direct sum of simple submouldes, there must be one factor $S_i$ such that $f|_{S_i}$ is non-trivial. Then by Shur's lemma, this is in fact an isomorphism $S_i \cong S$.
\end{proof}

This was for semi-simple algebras. For a general right Artinian algebra, we know that $A/J(A)$ is semi-simple and inherits the Artinian property. Then Artin--Wedderburn applies to $A/J(A)$.

Note that if $A$ is a finite-dimensional $k$-algebra, then it is certainly Artinian. So we must have that the division algebras $D_i$ appearing in Artin--Wedderburn are also finite-dimensional $k$-vector spaces.

Now pick an arbitrary $x \in D_i$. Then the sub-algebra of $D_i$ generated by $x$ must be commutative. So it is in fact a subfield, and finite dimensionality means it is algebraic over $k$. If $k$ is algebraically closed, then $x$ must be in $k$, and so we've shown that these $D_i$ must be $k$. Thus we get

\begin{cor}
  If $k$ is algebraically closed and $A$ is a finite-dimensional semi-simple $k$-algebra, then
  \[
    A \cong \bigoplus M_{n_i}(k).
  \]
\end{cor}
This is true, for example, when $k = \C$.

We shall end by applying our results to group algebras. Recall the following definition:
\begin{defi}[Group algebra]\index{group algebra}\index{$kG$}
  Let $G$ be a group and $k$ a field. The \emph{group algebra} of $G$ over $k$ is
  \[
    kG = \left\{\sum \lambda_g g: g \in G, \lambda_g \in k\right\}.
  \]
  This has a bilinear multiplication given by the obvious formula
  \[
    (\lambda_g g) (\mu_h h) = \lambda_g \mu_h (gh).
  \]
\end{defi}

The first thing to note is that group algebras are almost always semi-simple.
\begin{thm}[Maschke's theorem]\index{Maschke's theorem}
  Let $G$ be a finite group and $p \nmid |G|$, where $p = \Char k$, so that $|G|$ is invertible in $k$, then $kG$ is semi-simple.
\end{thm}

\begin{proof}
  We show that any submodule $V$ of a $kG$-module $U$ has a complement. Let $\pi: U \to V$ be any $k$-vector space projection, and define a new map
  \[
    \pi' = \frac{1}{|G|} \sum_{g \in G} g\pi g^{-1}: U \to V.
  \]
  It is easy to see that this is a $kG$-module homomorphism $U \to V$, and is a projection. So we have
  \[
    U = V \oplus \ker \pi',
  \]
  and this gives a $kG$-module complement.
\end{proof}

There is a converse to Maschke's theorem:
\begin{thm}
  Let $G$ be finite and $kG$ semi-simple. Then $\Char k \nmid |G|$.
\end{thm}

\begin{proof}
  We note that there is a simple $kG$-module $S$, given by the trivial module. This is a one-dimensional $k$ vector space. We have
  \[
    D = \End_{kG}(S) = k.
  \]
  Now suppose $kG$ is semi-simple. Then by Artin--Wedderburn, there must be only one summand of $S$ in $kG$.

  Consider the following two ideals of $kG$: we let
  \[
    I_1 = \left\{\sum \lambda_g g \in kG: \sum \lambda_g = 0\right\}.
  \]
  This is in fact a two-sided ideal of $kG$. We also have the center of the algebra, given by
  \[
    I_2 = \left\{\lambda \sum g \in kG: \lambda \in k\right\}.
  \]
  Now if $\Char k \mid |G|$, then we find that $I_2 \subseteq I_2$. Then we can write
  \[
    kG = \frac{kG}{I_1} \oplus I_1 = \frac{kG}{I_1} \oplus I_2 \oplus \cdots,
  \]
  and we know $\frac{kG}{I_1}$ has dimension $1$. So $kG$ has two summands. This gives a contradiction. So we must have $\Char k \nmid |G|$.
\end{proof}

We can do a bit more of representation theory. Recall that when $k$ is algebraically closed and has characteristic zero, then the number of simple $kG$-modules is the number of conjugacy classes of $G$. There is a more general result for a general characteristic $p$ field:
\begin{thm}
  Let $k$ be algebraically closed, and $G$ be finite. Then the number of simple $kG$ modules (up to isomorphism) is equal to the number of conjugacy classes of elements of order not divisible by $p$. These are known as the \term{$p$-regular elements}.
\end{thm}

We immediately deduce that
\begin{cor}
  If $|G| = p^r$ for some $r$ and $p$ is prime, then the trivial module is the only simple $kG$ module, when $\Char k = p$.
\end{cor}
Note that we can prove this directly rather than using the theorem, by showing that $I = \ker (kG \to k)$ is a nilpotent ideal, and annihilates all simple modules. % exercise

\begin{proof}[Proof sketch of theorem]
  The number of simple $kG$ is just the number of simple $kG/J(kG)$ module. % why?
  There is a useful trick to figure out the number of simple $A$-modules for a given semi-simple $A$. Suppose we have a decomposition
  \[
    A \cong \bigoplus_{i = 1}^r M_{n_i}(k).
  \]
  Then we know $r$ is the number of simple $A$-modules. We now consider $[A, A]$, the $k$-subspace generated by elements of the form $xy - yx$. Then we see that
  \[
    \frac{A}{[A, A]} \cong \bigoplus_{i = 1}^r \frac{M_{n_i}(k)}{[M_{n_i}(k), M_{n_i}(k)]}.
  \]
  Now by linear algebra, we know $[M_{n_i}(k), M_{n_i}(k)]$ is trace zero matrices, and so we know
  \[
    \dim_k \frac{M_{n_i}(k)}{[M_{n_i}(k), M_{n_i}(k)]} = 1.
  \]
  Hence we know
  \[
    \dim \frac{A}{[A, A]} = r.
  \]
  Thus we need to compute
  \[
    \dim_k \frac{kG/J(kG)}{[kG/J(kG), kG/J(kG)]}
  \]
  We then note the following facts:
  \begin{enumerate}
    \item For a general algebra $A$, we have
      \[
        \frac{A/J(A)}{[A/J(A), A/J(A)]} \cong \frac{A}{[A, A] + J(A)}.
      \]
    \item Let $g_1, \cdots, g_m$ be conjugacy class representatives of $G$. Then
      \[
        \{g_i + [kG, kG]\}
      \]
      forms a $k$-vector space basis of $kG/[kG, kG]$. % exercise!
    \item If $g_1, \cdots, g_r$ is a set of representatives of $p$-regular conjugacy classes, then
      \[
        \left\{g_i + \Big([kG, kG] + J(kG)\Big)\right\}
      \]
      form a basis of $kG/([kG, kG] + J(kG))$. % not exercise!
  \end{enumerate}
  Hence the result follows.
\end{proof}
One may find it useful to note that $[kG, kG] + J(kG)$ consists of the elements in $kG$ such that $x^{p^s} \in [kG, kG]$ for some $s$.

Note that $A/[A, A]$ is the $0$th Hochschild homology group of the algebra $A$. In the usual proof of this result for the the characteristic $0$ case, instead of lookinng at $A/[A, A]$, we look at the center $Z(A)$. This center is the $0$th Hochschild cohomology group of $A$.

\subsection{Cross products}
Number theorists are often interested in representations of Galois groups and $kG$-modules where $k$ is an algebraic number field, eg. $\Q$. The $D_i$'s appearing in Artin--Wedderburn may be non-commutative. We met the quaternions $\H$ in the introduction. These are examples of a general construction.

Let $B$ be a $k$-algebra, and $G$ a group. The elements of the crossed product are of the form
\[
  \sum \lambda_g g: \lambda_g \in B.
\]
We have to define an associative product. Bilinearity of the product implies it is enough to define how to multiply
\[
  (\lambda_g g)(\mu_h h).
\]
Consider
\[
  Bg = \{\lambda_g g: \lambda_g \in B\},
\]
then this is a left $B$-module in an obvious way. We get a right $B$-module by
\[
  g \cdot \lambda = \lambda^g g,
\]
where
\[
  \begin{tikzcd}[cdmap]
    \phi_g: B \ar[r] & B\\
    \lambda \ar[r, maps to] & \lambda^g
  \end{tikzcd}
\]
is a $k$-algebra isomorphism of $B$. Then we can define
\[
  (\lambda_g g)\cdot (\mu_h h) = \lambda_g \mu^g_h g\cdot h,
\]
and
\[
  g \cdot h = \Psi(g, h) gh,
\]
where $\Psi: G \times G \to B$, and the $gh$ on the right is the group multiplication. So at the end, we get
\[
  \lambda_g g g \cdot \mu_h h = \lambda_g \mu_h^g \Psi(g, h) gh.
\]
The associativity forces $\Psi$ to satisfy the condition that it must be a $2$-cycle.

In this case, we can construct $\H$ as $\C G$.
\printindex
\end{document}
