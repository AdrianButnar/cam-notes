\documentclass[a4paper]{article}

\def\npart {III}
\def\nterm {Lent}
\def\nyear {2017}
\def\nlecturer {C. J. B. Brookes}
\def\ncourse {Algebras}
\def\nlectures {TTS.10}

\input{header}

\begin{document}
\maketitle
{\small
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
The aim of the course is to give an introduction to algebras. The emphasis will be on non-commutative examples that arise in representation theory (of groups and Lie algebras) and the theory of algebraic D-modules, though you will learn something about commutative algebras in passing.

Topics I hope to fit in are:

Artinian algebras. Examples, group algebras of finite groups, crossed products. Structure theory. Artin-Wedderburn theorem. Projective modules. Blocks.

Noetherian algebras. Examples, quantum plane and quantum torus, differential operator algebras, enveloping algebras of finite dimensional Lie algebras. Structure theory. Injective hulls, uniform dimension and Goldie's theorem.

Hochschild chain and cochain complexes. Hochschild homology and cohomology. Gerstenhaber algebras.

$K_0$ and $K_1$.

Deformation of algebras. Quantum co-ordinate algebras and quantum enveloping algebras.

Coalgebras, bialgebras and Hopf algebras. Quantum groups.

\subsubsection*{Pre-requisites}
It will be assumed that you have attended a first course on ring theory, eg IB Groups, Rings and Modules. Experience of other algebraic courses such as II Representation Theory, Galois Theory or Number Fields, or III Lie algebras will be helpful but not necessary.
}
\tableofcontents

\setcounter{section}{-1}
\section{Introduction}
We start with the definition of an algebra. Throughout the course, $k$ will be a field.
\begin{defi}[$k$-algebra]\index{$k$-algebra}\index{algebra}
  A (unital) associative $k$-algebra is a $k$-vector space $A$ together with an associative product $m: A \otimes A \to A$, written $(x, y) \mapsto xy$, which is $k$-bilinear, and a $k$-linear map $u: k \to A$ such that $u(1)$ is the identity of the multiplication of $A$.
\end{defi}
We will mostly talk about $k$-algebras, but other algebraic structures will appear, eg. Lie algebras.

\begin{eg}
  Let $K/k$ be a finite field extension. Then $K$ is a $k$-algebra.
\end{eg}

\begin{eg}
  The $n\times n$ matrices $M_n(k)$ over $k$ are non-commutative $k$-algebras.
\end{eg}

\begin{eg}
  The quaternions $\H$ is an $\R$-algebra, with an $\R$-basis $1, i, j, k$, and multiplication given by
  \[
    i^2 = j^2 = k^2 = 1,\quad ij = k,\quad ji = -k.
  \]
  This is in fact a \term{division algebra} (or \term{skew fields}), ie. the non-zero elements have multiplicative inverse.
\end{eg}

\begin{eg}
  Let $G$ be a finite group. Then the group algebra
  \[
    kG = \left\{\sum \lambda_g g: g \in G, \lambda_g \in k\right\}
  \]
  with the obvious multiplication induced by the group operation is a $k$-algebra.

  These are the associative algebras underlying the representation theory of finite groups.
\end{eg}

Most of the time, we will just care about algebras that are finite-dimensional as $k$-algebras. However, the results actually hold for more general algebras, known as \emph{Artinian algebras}.

These algebras are defined by some finiteness condition on the ideals.

\begin{defi}[Ideal]\index{ideal}
  A \emph{left ideal} of $A$ is a $k$-subspace of $A$ such that if $x \in A$ and $y \in I$, then $xy \in I$. A \emph{right ideal} is defined similarly, and an \emph{ideal} is something that is both a left ideal and a right ideal.
\end{defi}
Since the multiplication is not necessarily commutative, we have to make the distinction between left and right things. Most of the time, we just talk about the left case, as the other case is entirely analogous.

The definition we want is the following:
\begin{defi}[Artinian algebra]\index{Artinian algebra}\index{algebra!Artinian}
  An algebra $A$ is \emph{left Artinian} if it satisfies the \term{descending chain condition} (\term{DCC}) on left ideals, ie. if we have a descending chain of left ideals
  \[
    I_1 \geq I_2 \geq I_3 \geq \cdots,
  \]
  then there is some $N$ such that $I_{N + m} = I_{N}$ for all $m \geq 0$.

  We say an algebra is \emph{Artinian} if it is both left and right Artinian.
\end{defi}

\begin{eg}
  Any finite-dimensional algebra is Artinian.
\end{eg}

The main classification theorem for Artinian algebras we will prove is the following result:
\begin{thm}[Artin-Wedderburn theorem]\index{Artin-Wedderburn theorem}
  Let $A$ be a left-Artinian algebra such that the intersection of the maximal left ideals is zero, then $A$ is the direct sum of finitely many matrix algebras over division algebras.
\end{thm}
When we actually get to the theorem, we will rewrite this in a way that seems a bit more natural.

This theorem is useful for representation theory, as the group algebra of a finite group is a finite-dimensional algebra, hence Artinian.

After studying Artinian rings, we'll talk about Noetherian algebras.
\begin{defi}[Noetherian algebra]\index{Noetherian algebra}\index{algebra!Noetherian}
  An algebra is \emph{left Noetherian} if it satisfies the \term{ascending chain condition} (\term{ACC}) on left ideals, ie. if
  \[
    I_1 \leq I_2 \leq I_3 \leq \cdots
  \]
  is an ascending chain of left ideals, then there is some $N$ such that $I_{N + m} = I_N$ for all $m \geq 0$.

  Similarly, we say an algebra is \emph{Noetherian} if it is both left and right Noetherian.
\end{defi}

We can again look at some examples.
\begin{eg}
  Again all finite-dimensional algebras are Noetherian.
\end{eg}

\begin{eg}
  In the commutative case, Hilbert's basis theorem tells us a polynomial algebra $k[X_1, \cdots, X_k]$ in finitely many variables is Noetherian. Similarly, the power series rings $k[[X_1, \cdots, X_n]]$ are Noetherian.
\end{eg}

\begin{eg}
  The \term{universal enveloping algebra} of a finite-dimensional Lie algebra are the (associative!) algebras that underpin the representation theory of these Lie algebras.
\end{eg}

\begin{eg}
  Some differential operator algebras are Noetherian. We assume $\Char k = 0$. Consider the polynomial ring $k[X]$. We have operators ``multiplication by $X$'' and ``differentiate with respect to $X$'' on $k[X]$. We can form the algebra $k[X, \frac{\partial}{\partial x}]$ of differential operators on $k[X]$. This is called the \term{Weyl algebra} $A_1$. This is a non-commutative Noetherian algebra.
\end{eg}

\begin{eg}
  Some group algebras are Noetherian. Clearly all group algebras of finite groups are Noetherian, but the group algebras of certain infinite groups are Noetherian. For example, we can take
  \[
    G = \left\{
      \begin{pmatrix}
        1 & \lambda & \mu\\
        0 & 1 & \nu\\
        0 & 0 & 0
      \end{pmatrix}: \lambda, \mu, \nu \in \Z
    \right\}.
  \]
\end{eg}

We'll see that left Artinian implies left Noetherian. There is a general theory of Noetherian algebras, but is not as useful as that in commutative algebra.

In the commutative case, we often look at $\Spec A$, the set of prime ideals of $A$. However, sometimes in the non-commutative there are few prime ideals, and so $\Spec$ is not going to keep us busy.
\begin{eg}
  In the Weyl algebra $A_1$, the only prime ideals are $0$ and $A_1$.
\end{eg}

We will prove a theorem of Goldie:
\begin{thm}[Goldie's theorem]\index{Goldie's theorem}
  Let $A$ be a left Noetherian such that the intersection of prime ideals is zero. Then $A$ has a \term{full ring of quotients} which is the finite sum of matrix algebras over division algebras, where the full ring of quotients is what we obtain by adding inverses to everything that is not a zero divisor.
\end{thm}

Some types of Noetherian algebras can be thought of as non-commutative polynomial algebras and non-commutative power series, ie. they are \emph{deformations} of the analogous commutative algebra. For example, we say $A_1$ is a deformation of the polynomial algebra $k[X, Y]$, where instead of having $XY - YX = 0$, we have $XY - YX = 1$. This also applies to enveloping algebras and Iwasawa algebras. We study when one can deform the multiplication so that it remains associative, and this is bound up with the cohomology theory of associative algebras --- \emph{Hochschild cohomology}. The Hochschild complex has additional algebraic structure, and this will allow us to understand how we can deform the algebra.

If time remains, we'll talk about about bialgebras, Hopf algebras and quantum groups. In a bialgebra, one also has a comultiplication map $A \to A \otimes A$, which in representation theory is crucial in saying how to regard a tensor product of two representations as a representation.

We finish with a little bit of history. In 1890's, Hilbert proved some big theorems in complex polynomial algebras. In 1920's, Noether came and abstracted out the key properties that made Hilbert's work work, and came up with the notion of the Noetherian property. In 1930s and 1940s, the development of standard commutative algebra. In 1945 came Hochschild, and in 1960s came Goldie. In 1960 we had Gerstenhaber, who told us about the structure of the Hochschild complex. In the 1960s and 1970s, the study of enveloping algebras of differential operators started, and from the 1980s we started studying quantum groups, whose motivation came from mathematical physics, but got the algebraists excited.

\section{Artinian algebras}
We start with some general definitions. Recall the definitions of right and left Artinian algebras from the introduction. Most examples we'll meet are in fact finite-dimensional vector spaces over $k$. However, we can also look at some preverse examples:

\begin{eg}
  Let
  \[
    A = \left\{
      \begin{pmatrix}
        r & s\\
        0 & t
      \end{pmatrix}: r \in \Q, s, t \in \R
    \right\}
  \]
  Then this is left Artinian but not right Artinian.
\end{eg}

As in the case of commutative algebra, we can study the modules of an algebra.
\begin{defi}[Module]\index{module}\index{bimodule}
  Let $A$ be an algebra. A \emph{left $A$-module} is a $k$-vector space $M$ and a map
  \[
    \begin{tikzcd}[cdmap]
      A \times M \ar[r] & M\\
      (x, m) \ar[r, maps to] & xm
    \end{tikzcd}
  \]
  such that
  \begin{align*}
    (x + y)m &= xm + ym\\
    (xy)m &= x(ym).
  \end{align*}
  Right $A$-modules are defined similarly.

  An \emph{$A\mdash A$-bimodule} is a vector space $M$ that is both a left $A$-module and a right $A$-module, such that the two actions commute --- for $a, b \in A$ and $x \in M$, we have
  \[
    a(xb) = (ax)b.
  \]
\end{defi}

\begin{eg}
  The algebra $A$ itself is a left $A$-module. We write this as $_A A$, and call this the \term{left regular representation}. Similarly, the right action is denoted $A_A$. These two actions are compatible by associativity, so $A$ is an $A\mdash A$-bimodule.
\end{eg}

If we write $\End_k(A)$ for the $k$-linear maps $A \to A$, then $\End_k$ is naturally a $k$-algebra by composition, and we have a $k$-algebra homomorphism $A \to \End_k(A)$ that sends $a \in A$ to multiplication by $a$.

However, in the right case, we do not get such a map. Instead, what we get is a map $A \to \End_k(A)^\op$, where
\begin{defi}[Opposite algebra]\index{opposite algebra}\index{algebra!opposite}{$A^\op$}
  Let $A$ be a $k$-algebra. We define the \emph{opposite algebra} $A^\op$ where $A^\op$ has the same underlying vector space, but we define the multiplication
  \[
    x \cdot y = yx.
  \]
  Here on the left we have the multiplication in $A^\op$ and on the right we have the multiplication in $A$.
\end{defi}
In general, a left $A$-module is a right $A^\op$-module.

As in the case of ring theory, we can talk about prime ideals. However, we will adopt a slightly different definition:
\begin{defi}[Prime ideal]\index{prime ideal}\index{ideal!prime}
  An ideal $P$ is \emph{prime} if it is a proper ideal, and if $I$ and $J$ are ideals with $IJ \subseteq P$, then either $I \subseteq P$ or $J \subseteq P$.
\end{defi}
It is an exercise to check that this coincides in the commutative case with the definition using elements.

\begin{defi}[Annihilator]\index{annihilator}
  Let $M$ be a left $A$-module and $m \in M$. We define the \emph{annihilators} to be
  \begin{align*}
    \Ann(m) &= \{a \in A: am = 0\}\\
    \Ann(M) &= \{a \in A: am = 0\text{ for all }m \in M\} = \bigcap_{m \in M} \Ann(m).
  \end{align*}
\end{defi}
Note that $\Ann(m)$ is a left ideal of $A$, and is in fact the kernel of the $A$-module homomorphism $A \to M$ given by $x \mapsto xm$. We'll denote the image of this map by $Am$, a left submodule of $M$, and we have
\[
  \frac{A}{\Ann(m)} \cong Am.
\]
On the other hand, it is easy to see that $\Ann(M)$ is an fact a (two-sided) ideal.

\begin{defi}[Simple module]\index{simple module}\index{irreducible module}\index{module!simple}\index{module!irreducible}
  A non-zero module $M$ is \emph{simple} or \emph{irreducible} if the only submodules of $M$ are $0$ and $M$.
\end{defi}

It is easy to see that
\begin{prop}
  Let $A$ be an algebra and $I$ a left ideal. Then $I$ is a maximal left ideal iff $A/I$ is simple.
\end{prop}

\begin{eg}
  $\Ann(m)$ is a maximal left ideal iff $Am$ is irreducible.
\end{eg}

\begin{prop}
  Let $A$ be an algebra and $M$ a simple module. Then $M \cong A/I$ for some (maximal) left ideal $I$ of $A$.
\end{prop}

\begin{proof}
  Pick an arbitrary element $m \in M$, and define the $A$-module homomorphism $\varphi: A \to M$ by $\varphi(a) = am$. Then the image is a non-trivial submodule, and hence must be $M$. Then by the first isomorphism theorem, we have $M \cong A/\ker I$.
\end{proof}

Before we start doing anything, we note the following convenient lemma:
\begin{lemma}
  Let $M$ be a finitely-generated $A$ module. Then $M$ has a maximal proper submodule $M'$.
\end{lemma}

\begin{proof}
  Let $m_1, \cdots, m_k \in M$ be a minimal generating set. Then in particular $N = \bra m_1, \cdots, m_{k - 1}\ket$ is a proper submodule of $M$. Moreover, a submodule of $M$ containing $N$ is proper iff it does not contain $m_k$, and this property is preserved under increasing unions. So by Zorn's lemma, there is a maximal proper submodule.
\end{proof}

\begin{defi}[Jacobson radical]\index{Jacobson radical}\index{$J(A)$}
  The \index{Jacobson radical} $J(A)$ of $A$ is the intersection of all maximal left ideals.
\end{defi}
This is in fact an ideal, and not just a left one, because
\[
  J(A) = \bigcap \{\text{maximal left ideals}\} = \bigcap_{m \in M\text{ simple}} \Ann(m) = \bigcap_{M\text{ simple}} \Ann(M),
\]
which we have established is an ideal. However, it still looks as if this definition depends on the sidedness. However, we have the following result:
\begin{lemma}[Nakayama lemma]\index{Nakayama lemma}
  The following are equivalent for a left ideal $I$ of $A$.
  \begin{enumerate}
    \item $I < J(A)$
    \item For any finitely-generated left $A$-module $m$, we have $IM = M$ implies $M = 0$, where $IM$ is the module generated by elements of the form $xm$, with $x \in I$ and $m \in M$.
    \item $G = \{1 + x: x \in I\} = 1 + I$ is a subgroup of the unit group of $A$.
  \end{enumerate}
\end{lemma}
Note that this shows that the Jacobson radical is the largest ideal satisfying (iii), which is something that does not depend on handedness.

\begin{proof}\leavevmode
  \begin{itemize}
    \item (i) $\Rightarrow$ (ii): Suppose $I < J(A)$ and $M \not= 0$ is a finitely-generated $A$-module, and we'll see that $IM \lneq M$.

      Let $N$ be a maximal submodule of $M$. Then $M/N$ is a simple module, so for any $\bar{m} \in M/N$, we know $\Ann(\bar{m})$ is a maximal left ideal. So $J(A) \leq \Ann(M/N)$. So $J(A) N \leq N \lneq M$.
    \item (ii) $\Rightarrow$ (iii): Assume (ii). We let $x \in I$ and set $y = 1 + x$. Hence $1 = y - x \in Ay + I$. Since $Ay + I$ is a left ideal, we know $Ay + I = A$. In other words, we know
      \[
        I \left(\frac{A}{Ay}\right) = \frac{A}{Ay}.
      \]
      Now using $2$ on the finitely-generated module $A/Ay$ (it is in fact generated by $1$), we know that $A/Ay = 0$. So $A = Ay$. So there exists $z \in A$ such that $1 = zy = z(1 + x)$. So $(1 + x)$ has a left inverse, and in fact $z$ is in $G$, since we can write $z = 1 - zx$. This is enough to show that $G$ is a group.
    \item (iii) $\Rightarrow$ (i): Suppose $I$ is a maximal left ideal of $A$. Let $x \in I$. If $x \not \in I_1$, then $I_1 + Ax = A$ by maximality of $I$. So $1 = y + zx$ for some $y \in I$ and $z \in A$. So $y = 1 - zx \in G$. So $y$ is invertible. But $y \in I_1$. So $I_1 = A$. This is a contradiction. So we found that $I < I_1$, and this is true for all maximal left ideals $I_1$. Hence $I \leq J(A)$.
  \end{itemize}
\end{proof}

\begin{defi}[Semisimple algebra]\index{semisimple algebra}\index{algebra!semisimple}
  An algebra is \emph{semisimple} if $J(A) = 0$.
\end{defi}

\begin{eg}
  For any $A$, we know $A/J(A)$ is always semisimple.
\end{eg}

\begin{eg}
  Consider $M_n(k)$. We let $e_i$ be the matrix with $1$ in the $(i, i)$th entry and zero everywhere else. This is idempotent, ie. $e_i^2 = e_i$. It is also straightforward to check that
  \[
    A e_i =
    \left\{
      \begin{pmatrix}
        0 & \cdots & 0 & a_1 & 0 & \cdots & 0\\
        0 & \cdots & 0 & a_2 & 0 & \cdots & 0\\
        \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
        0 & \cdots & 0 & a_n & 0 & \cdots & 0
      \end{pmatrix}
    \right\}
  \]
  The non-zero column is, of course, the $i$th column. Similarly, $e_i A$ is the matrices that are zero apart from in the $i$th row. These are in fact all left and all right ideals. So the only $2$-ideals are $0$ and $A$.

  Also, we note that $A_A$ is a direct sum of the simple left modules $A e_i$ and $A_A$ is the direct sum of the simple right modules $e_i A$.
\end{eg}

\begin{defi}[Simple algebra]\index{simple algebra}\index{algebra!simple}
  An algebra is \emph{simple} if the only ideals are $0$ and $A$.
\end{defi}
Thus, we can observe that $M_n(k)$ is a simple algebra.

\begin{defi}[Completely reducible]\index{completely reducible}
  A module $M$ of $A$ is \emph{completely reducible} iff it is a sum of simple modules.
\end{defi}

\begin{prop}
  Let $M$ be an $A$-module. Then the following are equivalent:
  \begin{enumerate}
    \item $M$ is completely reducible.
    \item $M$ is the direct sum of simple modules.
    \item Every module of $M$ has a \term{complement}, ie. for any submodule $N$ of $M$, there is a complement $N'$ such that $M = N \oplus N'$.
  \end{enumerate}
\end{prop}

\begin{proof}\leavevmode
  \begin{itemize}
    \item (i) $\Rightarrow$ (ii): Let $M$ be completely reducible, and consider the set
      \[
        \left\{\{S_\alpha \leq M\} : S_\alpha\text{ are simple},\; \sum S_\alpha\text{ is a direct sum}\right\}.
      \]
      Notice this set is closed under increasing unions, since the property of being a direct sum is only checked on finitely many elements. So by Zorn's lemma, it has a maximal element, and let $N$ be the sum of the elements.

      Suppose this were not all of $M$. Then there is some $S \leq M$ such that $S \not\subseteq N$. Then $S \cap N \leq S$. By simplicity, they intersect trivially. So $S + N$ is a direct sum, which is a contradiction. So we must have $N = M$, and $M$ is the direct sum of simple modules.
    \item (ii) $\Rightarrow$ (i) is trivial.
    \item (i) $\Rightarrow$ (iii): Let $N \leq M$ be a submodule, and consider
      \[
        \left\{\{S_\alpha \leq M\} : S_\alpha\text{ are simple},\; N + \sum S_\alpha\text{ is a direct sum}\right\}.
      \]
      Again this set has a maximal element, and let $P$ be the direct sum of those $S_\alpha$. Again if $P \oplus N$ is not all of $M$, then pick an $S \leq M$ simple such that $S$ is not contained in $P \oplus N$. Then again $S \oplus P \oplus N$ is a direct sum, which is a contradiction.
    \item (iii) $\Rightarrow$ (i): Let $x \in M$. We show that there is some simple $S \leq M$ such that $x \in S$. Let $N$ be a maximal submodule that does not contain $x$, and let $S$ be its complement. If $S$ is not simple, then there is some non-trivial $0 < P < S$. Then $N \oplus P$ has a complement, say $Q$. Then we can write
      \[
        \begin{array}{ccccccc}
          M &=& N &\oplus& P &\oplus& Q\\
          x &=& n &+& p &+& q
        \end{array}
      \]
      Since $x \not\in N$, we may wlog $p \not= 0$. Then $N \oplus Q$ does not contain $x$, and is strictly larger than $N$, a contradiction. So $S$ must be simple, and is a simple module containing $x$.
  \end{itemize}
\end{proof}

Using these different characterizations, we can prove that completely reducible modules are closed under the familiar ex operations.
\begin{prop}
  Sums, submodules and quotients of completely reducible modules are completely reducible.
\end{prop}

\begin{proof}
  It is clear by definition that sums of completely reducible modules are completely reducible. 

  To see that submodules of completely reducible modules are completely reducible, let $M$ be completely reducible, and $N \leq M$. Then for each $x \in N$, there is some simple submodule $S \leq M$ containing $x$. Since $S \cap N \leq S$ and contains $x$, it must be $S$, ie. $S \subseteq N$. So $N$ is the sum of simple modules.
  
  Finally, to see quotients are completely reducible, if $M$ is completely reducible and $N$ is a submodule, then we can write
  \[
    M = N \oplus P
  \]
  for some $P$. Then $M/N \cong P$, and $P$ is completely reducible.
\end{proof}

We will show that every left Artinian algebra is completely reducible over itself iff it is completely irreducible. We can in fact prove a more general fact for $A$-modules. To do so, we need a generalization of the Jacobson radical.

\begin{defi}[Radical]\index{radical}
  For a module $M$, we write $\Rad(M)$ for the intersection of maximal submodules of $M$, and call it the \emph{radical} of $M$.
\end{defi}
Thus, we have $\Rad(_A A) = J(A) = \Rad(A_A)$. % why

\begin{prop}
  Let $M$ be an $A$-module satisfying the descending chain condition on submodules. Then $M$ is completely reducible iff $\Rad(M) = 0$.
\end{prop}

\begin{proof}
  It is clear that if $M$ is completely reducible, then $\Rad(M) = 0$, as for any $x \in M$, there is a simple submodule containing $x$, and its complement has to be maximal.

  Conversely, if $\Rad(M) = 0$, we note that since $M$ satisfies the descending chain condition on submodules, there must be a \emph{finite} collection $M_1, \cdots, M_n$ of maximal submodules whose intersection vanish. Then consider the map
  \[
    \begin{tikzcd}[cdmap]
      M \ar[r] & \displaystyle\bigoplus_{i = 1}^n \frac{M}{M_i}\\
      x \ar[r, maps to]& (x + M_1, x + M_2, \cdots, x + M_n)
    \end{tikzcd}
  \]
  The kernel of this map is the intersection of the $M_i$, which is trivial. So this embeds $M$ as a submodule of $\bigoplus \frac{M}{M_i}$. But each $\frac{M}{M_i}$ is simple, so $M$ is a submodule of a semisimple algebra, hence semisimple.
\end{proof}

\begin{cor}
  If $A$ is a semi-simple left Artinian algebra, then $_AA$ is completely reducible.
\end{cor}

\begin{cor}
  If $A$ is a semi-simple left Artinian algebra, then every left $A$-module is completely reducible.
\end{cor}

\begin{proof}
  Every $A$-module $M$ is a quotient of sums of $_AA$. Explicitly, we have a map
  \[
    \begin{tikzcd}[cdmap]
      \displaystyle\bigoplus_{m \in M} {}_A A \ar[r] & M\\
      (a_m) \ar[r, maps to] & \sum a_m m
    \end{tikzcd}
  \]
  Then this map is clearly surjective, and thus $M$ is a quotient of $\bigoplus_M {}_AA$.
\end{proof}

If $A$ is not semi-simple, then it turns out it is rather easy to figure out radical of $M$, at least if $M$ is finitely-generated.
\begin{lemma}
  Let $A$ be left Artinian, and $M$ a finitely generated left $A$-module, then $J(A) M = \Rad(M)$.
\end{lemma}

\begin{proof}
  Let $M'$ be a maximal submodule of $M$. Then $M/M'$ is simple, and is in fact $A/I$ for some maximal left ideal $I$. Then we have
  \[
    J(A) \left(\frac{M}{M'}\right) = 0,
  \]
  since $J(A) < I$. Therefore $J(A) M \leq M'$. So $J(A)M \leq \Rad(M)$.

  Conversely, we know $\frac{M}{J(A) M}$ is an $A/J(A)$-module, and is hence completely reducible as $A/J(A)$ is semi-simple (and left Artinian). Since an $A$-submodule of $\frac{M}{J(A) M}$ is the same as an $A/J(A)$-submodule, it follows that it is completely reducible as an $A$-module as well. So
  \[
    \Rad\left(\frac{M}{J(A)M}\right) = 0,
  \]
  and hence $\Rad(M) \leq J(A) M$.
\end{proof}

\begin{prop}
  Let $A$ be left Artinian. Then
  \begin{enumerate}
    \item $J(A)$ is nilpotent, ie. there exists some $r$ such that $J(A)^r = 0$.
    \item If $M$ is a finitely-generated left $A$-module, then it is both left Artinian and left Noetherian.
    \item $A$ is left Noetherian.
  \end{enumerate}
\end{prop}

\begin{proof}\leavevmode
  \begin{enumerate}
    \item Since $A$ is left-Artinian, and $\{J(A)^r: r \in \N\}$ is a descending chain of ideals, it must eventually be constant. So $J(A)^r = J(A)^{2r}$ for some $r$. If this is non-zero, then again using the descending chain condition, we see there is a left ideal $I$ with $J(A)^r I \not= 0$ that is minimal with this property (one such ideal exists, say $J(A)$ itself).

      Now pick $x \in I$ with $J(A)^r x \not = 0$. In particular, $\not= 0$. Then by minimality, we must have $I = J(A)^r x$, as we know $J(A)^r x \subseteq I$. So there exists some $a \in J(A)^r$ with $x = ax$. So
      \[
        (1 - a) x = 0.
      \]
      But $1 - a$ is a unit. So $x = 0$. This is a contradiction. So $J(A)^r = 0$.
    \item Let $M_i = J(A)^i M$. Then $M_i/M_{i + 1}$ is annihilated by $J(A)$, and hence completely reducible (it is a module over semi-simple $A/J(A)$). Since $M$ is a finitely generated left $A$-module for a left Artinian algebra, it satisfies the descending chain condition for submodules (exercise), and hence so does $M_i/M_{i + 1}$. % fill in exercise

      So we know $M_i/M_{i + 1}$ is a finite sum of simple modules, and therefore satifies the ascending chain condition. So $M_i/M_{i + 1}$ is left Noetherian, and hence $M$ is (exercise).

    \item Follows from (ii) since $A$ is a finitely-generated left $A$-module.
  \end{enumerate}
\end{proof}

A last little lemma before doing Artin-Wedderburn:
\begin{lemma}
  Let $A$ be left Artinian and $J(A)$ is finitely-generated as a left $A$-module, and $_AA$ is completely reducible, then $A$ is semi-simple.
\end{lemma}

\begin{proof}
  If $_AA$ is completely reducible, then $J(A)$ has a complement. Write
  \[
    _A A = J(A) \oplus \frac{_AA}{J(A)}.
  \]
  Multiplying on the left by $J(A)$ gives $J(A) = J(A)^2$. Since $J(A)$ is nilpotent, this implies we must have $J(A) = 0$. % or by nakamyama?
\end{proof}

We are going to state the Artin-Wedderburn theorem for right things, because this makes the notation easier for us.
\begin{thm}[Artin-Wedderburn theorem]\index{Artin-Wedderburn theorem}
  Let $A$ be a semisimple right Artinian algebra. Then
  \[
    A = \bigoplus_{i = 1}^r A_i,
  \]
  where each $A_i = M_{n_i}(D_i)$ for some division algebra $D_i$, and the $A_i$ are uniquely determined, and $A$ has exactly $r$ isomorphism classes of simple (right) modules $M_i$, and
  \[
    \End_A (M_i) = \{\text{$A$-module homomorphisms $M_i \to M_i$}\} \cong D_i,
  \]
  and
  \[
    \dim_{D_i}(M_i) = n_i.
  \]
  If $A$ is simple, then $r = 1$.
\end{thm}
If we had the left version instead, then we need to insert $\op$'s somewhere.

To prove this, we use that $\End_A(A_A) \cong A$, where the endomorphism given by $a \in A$ is left-mutiplication by $A$.

We start off with Schur's lemma:
\begin{lemma}[Schur's lemma]\index{Schur's lemma}
  Let $M_1, M_2$ be simple right $A$-modules. Then eitehr $M_1 \cong M_2$, or $\Hom_A(M_1, M_2) = 0$. If $M$ is a simple $A$-module, then $\End_A(M)$ is a division algebra.
\end{lemma}

\begin{proof}
  A non-zero $A$-module homomorphism $M_1 \to M_2$ must either be injective, as the kernel is submodule. Similarly, the image has to be the whole thing since the image is a submodule. So this must be an isomorphism, and in particular has an inverse. So the last part follows as well.
\end{proof}

The proof of Artin-Wedderburn relies on considering $\End_A(A_A)$ in two different ways. We know $A_A$ is a cylic right $A$-module generated by $1$. So an endomorphisms of $A_A$ is determined by the image of $1$. Conversely, given any element $x\in A_A$, we have an $A$-module homomorphism sending $1$ to $x$, given by left-multiplication by $x$. This is indeed a homomorphism since $A_A$ is a \emph{right} module. So we have
\[
  \End_A(A_A) \cong A.
\]
\begin{lemma}\leavevmode
  \begin{enumerate}
    \item If $M$ is a right $A$-module and $e$ is an idemptotent in $A$, ie. $e^2 = e$, then $Me \cong \Hom_A(eA, M)$.
    \item We have
      \[
        eAe \cong \End_A(eA).
      \]
      In particular, we can take $e = 1$, and recover $\End_A(A_A) \cong A$.
  \end{enumerate}
\end{lemma}

\begin{proof}\leavevmode
  \begin{enumerate}
    \item We define maps
      \[
        \begin{tikzcd}[cdmap]
          me \ar[r, maps to] & (ex \mapsto mex)\\
          Me \ar[r, "f_1", yshift=2] & \End(eA, M) \ar[l, "f_2", yshift=-2]\\
          \alpha(e) & \alpha \ar[l, maps to]
        \end{tikzcd}
      \]
      We note that $\alpha(e) = \alpha(e^2) = \alpha(e) e \in Me$. So this is well-defined. by inspection, these maps are inverse to each other. So we are done.

      Note that we might worry that we have to pick representatives $me$ and $ex$ for the map $f_1$, but in fact we can also write it as $f(a)(y) = ay$, since $e$ is idempotent. So we are safe.
    \item Immediate from above by putting $M = eA$.
  \end{enumerate}
\end{proof}

\begin{lemma}
  Let $M$ be a finite direct sum of $A$-modules, say
  \[
    M = \bigoplus M_i,
  \]
  with each $M_i$ being a direct sum of $n_i$ simple modules all isomorphic to some $S_i$:
  \[
    M_i = M_{i1} \oplus M_{i2} \oplus \cdots \oplus M_{in_2},
  \]
  and $S_i \not\cong S_j$.

  We let $D_i = \End_A(S_i)$, which we have seen is a division algebra. Then we have
  \[
    \End_A (M_i) \cong M_{n_i} (D_i),
  \]
  and
  \[
    \End_A(M) = \bigoplus M_{n_i} (D_i)
  \]
  is semisimple.
\end{lemma}

\begin{proof}
  We choose isomorphisms $\Theta_{ij}: M_{ij} \to S_i$. Given $\lambda \in \End_A(M_i)$, we can define $\lambda_{jk} \in D_i$ as the composite map
  \[
    \begin{tikzcd}
      S_i \ar[r, "\theta_{ik}^{-1}"] & M_{ik} \ar[r, hook] & M_i \ar[r, "\lambda"] & M_i \ar[r, "\mathrm{proj}_i"] & M_{ij} \ar[r, "\theta_{ij}"] & S_i
    \end{tikzcd}
  \]
  Then the map $\lambda \mapsto (\lambda_{jk})$ is an injective map $\End_A(M_i) \to M_{n_i}(D_i)$. Conversely, given $(\lambda_{jk})$, we can construct $\lambda$ as the sum of the composite maps
  \[
    \begin{tikzcd}
      M_i \ar[r, two heads] & M_{ik} \ar[r, "\theta_{ik}"] & S_i \ar[r, "\lambda_{jk}"] & S_i \ar[r, "\theta_{ij}^{-1}"] & M_{ij} \ar[r, hook] & M_i.
    \end{tikzcd}
  \]
  This gives an inverse , and so we ahve $\End_A(M_i) \cong M_{n_i}(D_i)$. Then by Schur's lemma, we have
  \[
    \End_A(M) = \bigoplus_i \End_A(M_i) \cong M_{n_i}(D_i).
  \]
\end{proof}

We now prove Artin-Wedderburn.
\begin{proof}[Proof of Artin-Weddenburn]
  We have
  \[
    \End(A_A) \cong A \cong \oplus M_{n_i}(D_i).
  \]
\end{proof}
Note that $n_i = \dim_{D_i} S_i$.

This was for semi-simple rings. For a general right Artinian algebra, we know that $A/J(A)$ is semi-simple and inherits the Artinian property. Then Artin-Weddenburn applies to $A/J(A)$.

Note that if $A$ is a finite-dimensional $k$-algebra, then it is certainly Artinian. So we must have that the division algebras $D_i$ appearing in Artin-Weddenburn are also finite-dimensional $k$-vector spaces. So if $x \in D_i$, the it must be algebraic over $k$, and it generates a subfield of $D_i$ which is algebraic over $k$. If $k$ is algebraically closed, then $x$ must be in $k$, and so we've shown that these $D_i$ must be $k$. Thus we get

\begin{cor}
  If $k$ is algebraically closed and $A$ is a finite-dimensional semi-simple $k$-algebra, then
  \[
    A \cong \bigoplus M_{n_i}(k).
  \]
\end{cor}
This is true, for example, when $k = \C$.

The group algebra of a finite group $G$ is
\[
  kG = \left\{\sum \lambda_g g: g \in G, \lambda_g \in k\right\}.
\]
This has a bilinear multiplication given by the obvious formula
\[
  (\lambda_g g) (\mu_h h) = \lambda_g \mu_h (gh).
\]
\begin{thm}[Maschke's theorem]
  Let $G$ be a finite group and $p \nmid |G|$, where $p = \Char k$, so that $|G|$ is invertible in $k$, then $kG$ is semi-simple.
\end{thm}

\begin{proof}
  We show that any submodule $V$ of a $kG$-module $U$ has a complement. Let $\pi: U \to V$ be a $k$-vector space projection, and thus
  \[
    U = V \oplus \ker \pi
  \]
  as $k$ vector spaces. We need to construct a $kG$-module complement instead. We set
  \[
    \pi' = \frac{1}{|G|} \sum_{g \in G} g\pi g^{-1}: U \to V.
  \]
  It is easy to see that this is a $kG$-module homomorphism $U \to V$, and is a projection. So we have
  \[
    U = V \oplus \ker \pi',
  \]
  and this is a $kG$-module complement.
\end{proof}

\begin{thm}
  Let $G$ be finite and $kG$ semi-simple. Then $\Char k \nmid |G|$.
\end{thm}

\begin{proof}
  Suppose $p \mid |G|$. Then we have a simple $kG$ module $S$ which is the trivial module. It is one-dimensional with basis $u$, and $ug = u$ for all $g \in G$. We have
  \[
    \End_k(S) \cong k.
  \]
  If $kG$ were semi-simple, then the number of times $S$ appears in the decomposition of $kG$ as a sum of simples is $1$, which is the dimension of the simple module over $\End_k(S)$.
\end{proof}
\printindex
\end{document}
