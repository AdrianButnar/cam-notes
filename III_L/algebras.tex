\documentclass[a4paper]{article}

\def\npart {III}
\def\nterm {Lent}
\def\nyear {2017}
\def\nlecturer {C. J. B. Brookes}
\def\ncourse {Algebras}
\def\nlectures {TTS.10}

\input{header}

\newcommand\GKdim{\mathrm{GK}\mdash\mathrm{dim}}

\begin{document}
\maketitle
{\small
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
The aim of the course is to give an introduction to algebras. The emphasis will be on non-commutative examples that arise in representation theory (of groups and Lie algebras) and the theory of algebraic D-modules, though you will learn something about commutative algebras in passing.

Topics I hope to fit in are:

Artinian algebras. Examples, group algebras of finite groups, crossed products. Structure theory. Artin--Wedderburn theorem. Projective modules. Blocks.

Noetherian algebras. Examples, quantum plane and quantum torus, differential operator algebras, enveloping algebras of finite dimensional Lie algebras. Structure theory. Injective hulls, uniform dimension and Goldie's theorem.

Hochschild chain and cochain complexes. Hochschild homology and cohomology. Gerstenhaber algebras.

$K_0$ and $K_1$.

Deformation of algebras. Quantum co-ordinate algebras and quantum enveloping algebras.

Coalgebras, bialgebras and Hopf algebras. Quantum groups.

\subsubsection*{Pre-requisites}
It will be assumed that you have attended a first course on ring theory, eg IB Groups, Rings and Modules. Experience of other algebraic courses such as II Representation Theory, Galois Theory or Number Fields, or III Lie algebras will be helpful but not necessary.
}
\tableofcontents

\setcounter{section}{-1}
\section{Introduction}
We start with the definition of an algebra. Throughout the course, $k$ will be a field.
\begin{defi}[$k$-algebra]\index{$k$-algebra}\index{algebra}
  A (unital) associative $k$-algebra is a $k$-vector space $A$ together with an associative product $m: A \otimes A \to A$, written $(x, y) \mapsto xy$, which is $k$-bilinear, and a $k$-linear map $u: k \to A$ such that $u(1)$ is the identity of the multiplication of $A$.
\end{defi}
We will mostly talk about $k$-algebras, but other algebraic structures will appear, eg. Lie algebras.

\begin{eg}
  Let $K/k$ be a finite field extension. Then $K$ is a $k$-algebra.
\end{eg}

\begin{eg}
  The $n\times n$ matrices $M_n(k)$ over $k$ are non-commutative $k$-algebras.
\end{eg}

\begin{eg}
  The quaternions $\H$ is an $\R$-algebra, with an $\R$-basis $1, i, j, k$, and multiplication given by
  \[
    i^2 = j^2 = k^2 = 1,\quad ij = k,\quad ji = -k.
  \]
  This is in fact a \term{division algebra} (or \term{skew fields}), ie. the non-zero elements have multiplicative inverse.
\end{eg}

\begin{eg}
  Let $G$ be a finite group. Then the group algebra
  \[
    kG = \left\{\sum \lambda_g g: g \in G, \lambda_g \in k\right\}
  \]
  with the obvious multiplication induced by the group operation is a $k$-algebra.

  These are the associative algebras underlying the representation theory of finite groups.
\end{eg}

Most of the time, we will just care about algebras that are finite-dimensional as $k$-algebras. However, the results actually hold for more general algebras, known as \emph{Artinian algebras}.

These algebras are defined by some finiteness condition on the ideals.

\begin{defi}[Ideal]\index{ideal}
  A \emph{left ideal} of $A$ is a $k$-subspace of $A$ such that if $x \in A$ and $y \in I$, then $xy \in I$. A \emph{right ideal} is defined similarly, and an \emph{ideal} is something that is both a left ideal and a right ideal.
\end{defi}
Since the multiplication is not necessarily commutative, we have to make the distinction between left and right things. Most of the time, we just talk about the left case, as the other case is entirely analogous.

The definition we want is the following:
\begin{defi}[Artinian algebra]\index{Artinian algebra}\index{algebra!Artinian}
  An algebra $A$ is \emph{left Artinian} if it satisfies the \term{descending chain condition} (\term{DCC}) on left ideals, ie. if we have a descending chain of left ideals
  \[
    I_1 \geq I_2 \geq I_3 \geq \cdots,
  \]
  then there is some $N$ such that $I_{N + m} = I_{N}$ for all $m \geq 0$.

  We say an algebra is \emph{Artinian} if it is both left and right Artinian.
\end{defi}

\begin{eg}
  Any finite-dimensional algebra is Artinian.
\end{eg}

The main classification theorem for Artinian algebras we will prove is the following result:
\begin{thm}[Artin--Wedderburn theorem]\index{Artin--Wedderburn theorem}
  Let $A$ be a left-Artinian algebra such that the intersection of the maximal left ideals is zero, then $A$ is the direct sum of finitely many matrix algebras over division algebras.
\end{thm}
When we actually get to the theorem, we will rewrite this in a way that seems a bit more natural.

This theorem is useful for representation theory, as the group algebra of a finite group is a finite-dimensional algebra, hence Artinian.

After studying Artinian rings, we'll talk about Noetherian algebras.
\begin{defi}[Noetherian algebra]\index{Noetherian algebra}\index{algebra!Noetherian}
  An algebra is \emph{left Noetherian} if it satisfies the \term{ascending chain condition} (\term{ACC}) on left ideals, ie. if
  \[
    I_1 \leq I_2 \leq I_3 \leq \cdots
  \]
  is an ascending chain of left ideals, then there is some $N$ such that $I_{N + m} = I_N$ for all $m \geq 0$.

  Similarly, we say an algebra is \emph{Noetherian} if it is both left and right Noetherian.
\end{defi}

We can again look at some examples.
\begin{eg}
  Again all finite-dimensional algebras are Noetherian.
\end{eg}

\begin{eg}
  In the commutative case, Hilbert's basis theorem tells us a polynomial algebra $k[X_1, \cdots, X_k]$ in finitely many variables is Noetherian. Similarly, the power series rings $k[[X_1, \cdots, X_n]]$ are Noetherian.
\end{eg}

\begin{eg}
  The \term{universal enveloping algebra} of a finite-dimensional Lie algebra are the (associative!) algebras that underpin the representation theory of these Lie algebras.
\end{eg}

\begin{eg}
  Some differential operator algebras are Noetherian. We assume $\Char k = 0$. Consider the polynomial ring $k[X]$. We have operators ``multiplication by $X$'' and ``differentiate with respect to $X$'' on $k[X]$. We can form the algebra $k[X, \frac{\partial}{\partial x}]$ of differential operators on $k[X]$. This is called the \term{Weyl algebra} $A_1$. This is a non-commutative Noetherian algebra.
\end{eg}

\begin{eg}
  Some group algebras are Noetherian. Clearly all group algebras of finite groups are Noetherian, but the group algebras of certain infinite groups are Noetherian. For example, we can take
  \[
    G = \left\{
      \begin{pmatrix}
        1 & \lambda & \mu\\
        0 & 1 & \nu\\
        0 & 0 & 0
      \end{pmatrix}: \lambda, \mu, \nu \in \Z
    \right\}.
  \]
\end{eg}

We'll see that left Artinian implies left Noetherian. There is a general theory of Noetherian algebras, but is not as useful as that in commutative algebra.

In the commutative case, we often look at $\Spec A$, the set of prime ideals of $A$. However, sometimes in the non-commutative there are few prime ideals, and so $\Spec$ is not going to keep us busy.
\begin{eg}
  In the Weyl algebra $A_1$, the only prime ideals are $0$ and $A_1$.
\end{eg}

We will prove a theorem of Goldie:
\begin{thm}[Goldie's theorem]\index{Goldie's theorem}
  Let $A$ be a left Noetherian such that the intersection of prime ideals is zero. Then $A$ has a \term{full ring of quotients} which is the finite sum of matrix algebras over division algebras, where the full ring of quotients is what we obtain by adding inverses to everything that is not a zero divisor.
\end{thm}

Some types of Noetherian algebras can be thought of as non-commutative polynomial algebras and non-commutative power series, ie. they are \emph{deformations} of the analogous commutative algebra. For example, we say $A_1$ is a deformation of the polynomial algebra $k[X, Y]$, where instead of having $XY - YX = 0$, we have $XY - YX = 1$. This also applies to enveloping algebras and Iwasawa algebras. We study when one can deform the multiplication so that it remains associative, and this is bound up with the cohomology theory of associative algebras --- \emph{Hochschild cohomology}. The Hochschild complex has additional algebraic structure, and this will allow us to understand how we can deform the algebra.

If time remains, we'll talk about about bialgebras, Hopf algebras and quantum groups. In a bialgebra, one also has a comultiplication map $A \to A \otimes A$, which in representation theory is crucial in saying how to regard a tensor product of two representations as a representation.

We finish with a little bit of history. In 1890's, Hilbert proved some big theorems in complex polynomial algebras. In 1920's, Noether came and abstracted out the key properties that made Hilbert's work work, and came up with the notion of the Noetherian property. In 1930s and 1940s, the development of standard commutative algebra. In 1945 came Hochschild, and in 1960s came Goldie. In 1960 we had Gerstenhaber, who told us about the structure of the Hochschild complex. In the 1960s and 1970s, the study of enveloping algebras of differential operators started, and from the 1980s we started studying quantum groups, whose motivation came from mathematical physics, but got the algebraists excited.

\section{Artinian algebras}
\subsection{Artinian algebras}
We start with some general definitions. Recall the definitions of right and left Artinian algebras from the introduction. Most examples we'll meet are in fact finite-dimensional vector spaces over $k$. However, we can also look at some perverse examples:

\begin{eg}
  Let
  \[
    A = \left\{
      \begin{pmatrix}
        r & s\\
        0 & t
      \end{pmatrix}: r \in \Q, s, t \in \R
    \right\}
  \]
  Then this is left Artinian but not right Artinian.
\end{eg}

As in the case of commutative algebra, we can study the modules of an algebra.
\begin{defi}[Module]\index{module}\index{bimodule}
  Let $A$ be an algebra. A \emph{left $A$-module} is a $k$-vector space $M$ and a map
  \[
    \begin{tikzcd}[cdmap]
      A \times M \ar[r] & M\\
      (x, m) \ar[r, maps to] & xm
    \end{tikzcd}
  \]
  such that
  \begin{align*}
    (x + y)m &= xm + ym\\
    (xy)m &= x(ym).
  \end{align*}
  Right $A$-modules are defined similarly.

  An \emph{$A\mdash A$-bimodule} is a vector space $M$ that is both a left $A$-module and a right $A$-module, such that the two actions commute --- for $a, b \in A$ and $x \in M$, we have
  \[
    a(xb) = (ax)b.
  \]
\end{defi}

\begin{eg}
  The algebra $A$ itself is a left $A$-module. We write this as $_A A$, and call this the \term{left regular representation}. Similarly, the right action is denoted $A_A$. These two actions are compatible by associativity, so $A$ is an $A\mdash A$-bimodule.
\end{eg}

If we write $\End_k(A)$ for the $k$-linear maps $A \to A$, then $\End_k$ is naturally a $k$-algebra by composition, and we have a $k$-algebra homomorphism $A \to \End_k(A)$ that sends $a \in A$ to multiplication by $a$.

However, in the right case, we do not get such a map. Instead, what we get is a map $A \to \End_k(A)^\op$, where
\begin{defi}[Opposite algebra]\index{opposite algebra}\index{algebra!opposite}{$A^\op$}
  Let $A$ be a $k$-algebra. We define the \emph{opposite algebra} $A^\op$ where $A^\op$ has the same underlying vector space, but we define the multiplication
  \[
    x \cdot y = yx.
  \]
  Here on the left we have the multiplication in $A^\op$ and on the right we have the multiplication in $A$.
\end{defi}
In general, a left $A$-module is a right $A^\op$-module.

As in the case of ring theory, we can talk about prime ideals. However, we will adopt a slightly different definition:
\begin{defi}[Prime ideal]\index{prime ideal}\index{ideal!prime}
  An ideal $P$ is \emph{prime} if it is a proper ideal, and if $I$ and $J$ are ideals with $IJ \subseteq P$, then either $I \subseteq P$ or $J \subseteq P$.
\end{defi}
It is an exercise to check that this coincides in the commutative case with the definition using elements.

\begin{defi}[Annihilator]\index{annihilator}
  Let $M$ be a left $A$-module and $m \in M$. We define the \emph{annihilators} to be
  \begin{align*}
    \Ann(m) &= \{a \in A: am = 0\}\\
    \Ann(M) &= \{a \in A: am = 0\text{ for all }m \in M\} = \bigcap_{m \in M} \Ann(m).
  \end{align*}
\end{defi}
Note that $\Ann(m)$ is a left ideal of $A$, and is in fact the kernel of the $A$-module homomorphism $A \to M$ given by $x \mapsto xm$. We'll denote the image of this map by $Am$, a left submodule of $M$, and we have
\[
  \frac{A}{\Ann(m)} \cong Am.
\]
On the other hand, it is easy to see that $\Ann(M)$ is an fact a (two-sided) ideal.

\begin{defi}[Simple module]\index{simple module}\index{irreducible module}\index{module!simple}\index{module!irreducible}
  A non-zero module $M$ is \emph{simple} or \emph{irreducible} if the only submodules of $M$ are $0$ and $M$.
\end{defi}

It is easy to see that
\begin{prop}
  Let $A$ be an algebra and $I$ a left ideal. Then $I$ is a maximal left ideal iff $A/I$ is simple.
\end{prop}

\begin{eg}
  $\Ann(m)$ is a maximal left ideal iff $Am$ is irreducible.
\end{eg}

\begin{prop}
  Let $A$ be an algebra and $M$ a simple module. Then $M \cong A/I$ for some (maximal) left ideal $I$ of $A$.
\end{prop}

\begin{proof}
  Pick an arbitrary element $m \in M$, and define the $A$-module homomorphism $\varphi: A \to M$ by $\varphi(a) = am$. Then the image is a non-trivial submodule, and hence must be $M$. Then by the first isomorphism theorem, we have $M \cong A/\ker I$.
\end{proof}

Before we start doing anything, we note the following convenient lemma:
\begin{lemma}
  Let $M$ be a finitely-generated $A$ module. Then $M$ has a maximal proper submodule $M'$.
\end{lemma}

\begin{proof}
  Let $m_1, \cdots, m_k \in M$ be a minimal generating set. Then in particular $N = \bra m_1, \cdots, m_{k - 1}\ket$ is a proper submodule of $M$. Moreover, a submodule of $M$ containing $N$ is proper iff it does not contain $m_k$, and this property is preserved under increasing unions. So by Zorn's lemma, there is a maximal proper submodule.
\end{proof}

\begin{defi}[Jacobson radical]\index{Jacobson radical}\index{$J(A)$}
  The \index{Jacobson radical} $J(A)$ of $A$ is the intersection of all maximal left ideals.
\end{defi}
This is in fact an ideal, and not just a left one, because
\[
  J(A) = \bigcap \{\text{maximal left ideals}\} = \bigcap_{m \in M\text{ simple}} \Ann(m) = \bigcap_{M\text{ simple}} \Ann(M),
\]
which we have established is an ideal. However, it still looks as if this definition depends on the sidedness. However, we have the following result:
\begin{lemma}[Nakayama lemma]\index{Nakayama lemma}
  The following are equivalent for a left ideal $I$ of $A$.
  \begin{enumerate}
    \item $I < J(A)$
    \item For any finitely-generated left $A$-Module $m$, we have $IM = M$ implies $M = 0$, where $IM$ is the module generated by elements of the form $xm$, with $x \in I$ and $m \in M$.
    \item $G = \{1 + x: x \in I\} = 1 + I$ is a subgroup of the unit group of $A$.
  \end{enumerate}
\end{lemma}
Note that this shows that the Jacobson radical is the largest ideal satisfying (iii), which is something that does not depend on handedness.

\begin{proof}\leavevmode
  \begin{itemize}
    \item (i) $\Rightarrow$ (ii): Suppose $I < J(A)$ and $M \not= 0$ is a finitely-generated $A$-module, and we'll see that $IM \lneq M$.

      Let $N$ be a maximal submodule of $M$. Then $M/N$ is a simple module, so for any $\bar{m} \in M/N$, we know $\Ann(\bar{m})$ is a maximal left ideal. So $J(A) \leq \Ann(M/N)$. So $J(A) M \leq N \lneq M$.
    \item (ii) $\Rightarrow$ (iii): Assume (ii). We let $x \in I$ and set $y = 1 + x$. Hence $1 = y - x \in Ay + I$. Since $Ay + I$ is a left ideal, we know $Ay + I = A$. In other words, we know
      \[
        I \left(\frac{A}{Ay}\right) = \frac{A}{Ay}.
      \]
      Now using $2$ on the finitely-generated module $A/Ay$ (it is in fact generated by $1$), we know that $A/Ay = 0$. So $A = Ay$. So there exists $z \in A$ such that $1 = zy = z(1 + x)$. So $(1 + x)$ has a left inverse, and in fact $z$ is in $G$, since we can write $z = 1 - zx$. This is enough to show that $G$ is a group.
    \item (iii) $\Rightarrow$ (i): Suppose $I_1$ is a maximal left ideal of $A$. Let $x \in I$. If $x \not \in I_1$, then $I_1 + Ax = A$ by maximality of $I$. So $1 = y + zx$ for some $y \in I$ and $z \in A$. So $y = 1 - zx \in G$. So $y$ is invertible. But $y \in I_1$. So $I_1 = A$. This is a contradiction. So we found that $I < I_1$, and this is true for all maximal left ideals $I_1$. Hence $I \leq J(A)$.
  \end{itemize}
\end{proof}

\begin{defi}[Semisimple algebra]\index{semisimple algebra}\index{algebra!semisimple}
  An algebra is \emph{semisimple} if $J(A) = 0$.
\end{defi}

\begin{eg}
  For any $A$, we know $A/J(A)$ is always semisimple.
\end{eg}

\begin{eg}
  Consider $M_n(k)$. We let $e_i$ be the matrix with $1$ in the $(i, i)$th entry and zero everywhere else. This is idempotent, ie. $e_i^2 = e_i$. It is also straightforward to check that
  \[
    A e_i =
    \left\{
      \begin{pmatrix}
        0 & \cdots & 0 & a_1 & 0 & \cdots & 0\\
        0 & \cdots & 0 & a_2 & 0 & \cdots & 0\\
        \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
        0 & \cdots & 0 & a_n & 0 & \cdots & 0
      \end{pmatrix}
    \right\}
  \]
  The non-zero column is, of course, the $i$th column. Similarly, $e_i A$ is the matrices that are zero apart from in the $i$th row. These are in fact all left and all right ideals respectively. So the only $2$-ideals are $0$ and $A$.

  Also, we note that $A_A$ is a direct sum of the simple left modules $A e_i$ and $A_A$ is the direct sum of the simple right modules $e_i A$.
\end{eg}

\begin{defi}[Simple algebra]\index{simple algebra}\index{algebra!simple}
  An algebra is \emph{simple} if the only ideals are $0$ and $A$.
\end{defi}
Thus, we can observe that $M_n(k)$ is a simple algebra.

\begin{defi}[Completely reducible]\index{completely reducible}
  A module $M$ of $A$ is \emph{completely reducible} iff it is a sum of simple modules.
\end{defi}

\begin{prop}
  Let $M$ be an $A$-module. Then the following are equivalent:
  \begin{enumerate}
    \item $M$ is completely reducible.
    \item $M$ is the direct sum of simple modules.
    \item Every module of $M$ has a \term{complement}, ie. for any submodule $N$ of $M$, there is a complement $N'$ such that $M = N \oplus N'$.
  \end{enumerate}
\end{prop}

\begin{proof}\leavevmode
  \begin{itemize}
    \item (i) $\Rightarrow$ (ii): Let $M$ be completely reducible, and consider the set
      \[
        \left\{\{S_\alpha \leq M\} : S_\alpha\text{ are simple},\; \sum S_\alpha\text{ is a direct sum}\right\}.
      \]
      Notice this set is closed under increasing unions, since the property of being a direct sum is only checked on finitely many elements. So by Zorn's lemma, it has a maximal element, and let $N$ be the sum of the elements.

      Suppose this were not all of $M$. Then there is some $S \leq M$ such that $S \not\subseteq N$. Then $S \cap N \leq S$. By simplicity, they intersect trivially. So $S + N$ is a direct sum, which is a contradiction. So we must have $N = M$, and $M$ is the direct sum of simple modules.
    \item (ii) $\Rightarrow$ (i) is trivial.
    \item (i) $\Rightarrow$ (iii): Let $N \leq M$ be a submodule, and consider
      \[
        \left\{\{S_\alpha \leq M\} : S_\alpha\text{ are simple},\; N + \sum S_\alpha\text{ is a direct sum}\right\}.
      \]
      Again this set has a maximal element, and let $P$ be the direct sum of those $S_\alpha$. Again if $P \oplus N$ is not all of $M$, then pick an $S \leq M$ simple such that $S$ is not contained in $P \oplus N$. Then again $S \oplus P \oplus N$ is a direct sum, which is a contradiction.
    \item (iii) $\Rightarrow$ (i): Let $x \in M$. We show that there is some simple $S \leq M$ such that $x \in S$. Let $N$ be a maximal submodule that does not contain $x$, and let $S$ be its complement. If $S$ is not simple, then there is some non-trivial $0 < P < S$. Then $N \oplus P$ has a complement, say $Q$. Then we can write
      \[
        \begin{array}{ccccccc}
          M &=& N &\oplus& P &\oplus& Q\\
          x &=& n &+& p &+& q
        \end{array}
      \]
      Since $x \not\in N$, we may wlog $p \not= 0$. Then $N \oplus Q$ does not contain $x$, and is strictly larger than $N$, a contradiction. So $S$ must be simple, and is a simple module containing $x$.
  \end{itemize}
\end{proof}

Using these different characterizations, we can prove that completely reducible modules are closed under the familiar ex operations.
\begin{prop}
  Sums, submodules and quotients of completely reducible modules are completely reducible.
\end{prop}

\begin{proof}
  It is clear by definition that sums of completely reducible modules are completely reducible.

  To see that submodules of completely reducible modules are completely reducible, let $M$ be completely reducible, and $N \leq M$. Then for each $x \in N$, there is some simple submodule $S \leq M$ containing $x$. Since $S \cap N \leq S$ and contains $x$, it must be $S$, ie. $S \subseteq N$. So $N$ is the sum of simple modules.

  Finally, to see quotients are completely reducible, if $M$ is completely reducible and $N$ is a submodule, then we can write
  \[
    M = N \oplus P
  \]
  for some $P$. Then $M/N \cong P$, and $P$ is completely reducible.
\end{proof}

We will show that every left Artinian algebra is completely reducible over itself iff it is completely irreducible. We can in fact prove a more general fact for $A$-modules. To do so, we need a generalization of the Jacobson radical.

\begin{defi}[Radical]\index{radical}
  For a module $M$, we write $\Rad(M)$ for the intersection of maximal submodules of $M$, and call it the \emph{radical} of $M$.
\end{defi}
Thus, we have $\Rad(_A A) = J(A) = \Rad(A_A)$. % why

\begin{prop}
  Let $M$ be an $A$-module satisfying the descending chain condition on submodules. Then $M$ is completely reducible iff $\Rad(M) = 0$.
\end{prop}

\begin{proof}
  It is clear that if $M$ is completely reducible, then $\Rad(M) = 0$, as for any $x \in M$, there is a simple submodule containing $x$, and its complement has to be maximal.

  Conversely, if $\Rad(M) = 0$, we note that since $M$ satisfies the descending chain condition on submodules, there must be a \emph{finite} collection $M_1, \cdots, M_n$ of maximal submodules whose intersection vanish. Then consider the map
  \[
    \begin{tikzcd}[cdmap]
      M \ar[r] & \displaystyle\bigoplus_{i = 1}^n \frac{M}{M_i}\\
      x \ar[r, maps to]& (x + M_1, x + M_2, \cdots, x + M_n)
    \end{tikzcd}
  \]
  The kernel of this map is the intersection of the $M_i$, which is trivial. So this embeds $M$ as a submodule of $\bigoplus \frac{M}{M_i}$. But each $\frac{M}{M_i}$ is simple, so $M$ is a submodule of a semisimple algebra, hence semisimple.
\end{proof}

\begin{cor}
  If $A$ is a semi-simple left Artinian algebra, then $_AA$ is completely reducible.
\end{cor}

\begin{cor}
  If $A$ is a semi-simple left Artinian algebra, then every left $A$-module is completely reducible.
\end{cor}

\begin{proof}
  Every $A$-module $M$ is a quotient of sums of $_AA$. Explicitly, we have a map
  \[
    \begin{tikzcd}[cdmap]
      \displaystyle\bigoplus_{m \in M} {}_A A \ar[r] & M\\
      (a_m) \ar[r, maps to] & \sum a_m m
    \end{tikzcd}
  \]
  Then this map is clearly surjective, and thus $M$ is a quotient of $\bigoplus_M {}_AA$.
\end{proof}

If $A$ is not semi-simple, then it turns out it is rather easy to figure out radical of $M$, at least if $M$ is finitely-generated.
\begin{lemma}
  Let $A$ be left Artinian, and $M$ a finitely generated left $A$-module, then $J(A) M = \Rad(M)$.
\end{lemma}

\begin{proof}
  Let $M'$ be a maximal submodule of $M$. Then $M/M'$ is simple, and is in fact $A/I$ for some maximal left ideal $I$. Then we have
  \[
    J(A) \left(\frac{M}{M'}\right) = 0,
  \]
  since $J(A) < I$. Therefore $J(A) M \leq M'$. So $J(A)M \leq \Rad(M)$.

  Conversely, we know $\frac{M}{J(A) M}$ is an $A/J(A)$-module, and is hence completely reducible as $A/J(A)$ is semi-simple (and left Artinian). Since an $A$-submodule of $\frac{M}{J(A) M}$ is the same as an $A/J(A)$-submodule, it follows that it is completely reducible as an $A$-module as well. So
  \[
    \Rad\left(\frac{M}{J(A)M}\right) = 0,
  \]
  and hence $\Rad(M) \leq J(A) M$.
\end{proof}

\begin{prop}
  Let $A$ be left Artinian. Then
  \begin{enumerate}
    \item $J(A)$ is nilpotent, ie. there exists some $r$ such that $J(A)^r = 0$.
    \item If $M$ is a finitely-generated left $A$-module, then it is both left Artinian and left Noetherian.
    \item $A$ is left Noetherian.
  \end{enumerate}
\end{prop}

\begin{proof}\leavevmode
  \begin{enumerate}
    \item Since $A$ is left-Artinian, and $\{J(A)^r: r \in \N\}$ is a descending chain of ideals, it must eventually be constant. So $J(A)^r = J(A)^{2r}$ for some $r$. If this is non-zero, then again using the descending chain condition, we see there is a left ideal $I$ with $J(A)^r I \not= 0$ that is minimal with this property (one such ideal exists, say $J(A)$ itself).

      Now pick $x \in I$ with $J(A)^r x \not = 0$. In particular, $\not= 0$. Then by minimality, we must have $I = J(A)^r x$, as we know $J(A)^r x \subseteq I$. So there exists some $a \in J(A)^r$ with $x = ax$. So
      \[
        (1 - a) x = 0.
      \]
      But $1 - a$ is a unit. So $x = 0$. This is a contradiction. So $J(A)^r = 0$.
    \item Let $M_i = J(A)^i M$. Then $M_i/M_{i + 1}$ is annihilated by $J(A)$, and hence completely reducible (it is a module over semi-simple $A/J(A)$). Since $M$ is a finitely generated left $A$-module for a left Artinian algebra, it satisfies the descending chain condition for submodules (exercise), and hence so does $M_i/M_{i + 1}$. % fill in exercise

      So we know $M_i/M_{i + 1}$ is a finite sum of simple modules, and therefore satisfies the ascending chain condition. So $M_i/M_{i + 1}$ is left Noetherian, and hence $M$ is (exercise).

    \item Follows from (ii) since $A$ is a finitely-generated left $A$-module.
  \end{enumerate}
\end{proof}

A last little lemma before doing Artin--Wedderburn:
\begin{lemma}
  Let $A$ be left Artinian and $J(A)$ is finitely-generated as a left $A$-module, and $_AA$ is completely reducible, then $A$ is semi-simple.
\end{lemma}

\begin{proof}
  If $_AA$ is completely reducible, then $J(A)$ has a complement. Write
  \[
    _A A = J(A) \oplus \frac{_AA}{J(A)}.
  \]
  Multiplying on the left by $J(A)$ gives $J(A) = J(A)^2$. Since $J(A)$ is nilpotent, this implies we must have $J(A) = 0$. % or by nakamyama?
\end{proof}

\subsection{Artin--Wedderburn theorem}
We are going to state the Artin--Wedderburn theorem for right things, because this makes the notation easier for us.
\begin{thm}[Artin--Wedderburn theorem]\index{Artin--Wedderburn theorem}
  Let $A$ be a semisimple right Artinian algebra. Then
  \[
    A = \bigoplus_{i = 1}^r M_{n_i}(D_i),
  \]
  for some division algebra $D_i$, and these factors are uniquely determined.

  $A$ has exactly $r$ isomorphism classes of simple (right) modules $S_i$, and
  \[
    \End_A (S_i) = \{\text{$A$-module homomorphisms $S_i \to S_i$}\} \cong D_i,
  \]
  and
  \[
    \dim_{D_i}(S_i) = n_i.
  \]
  If $A$ is simple, then $r = 1$.
\end{thm}
If we had the left version instead, then we need to insert $\op$'s somewhere.

To prove this, we use that $\End_A(A_A) \cong A$, where the endomorphism given by $a \in A$ is left-multiplication by $A$.

Artin--Wedderburn is an easy consequence of two trivial lemma. The first is Shur's lemma:
\begin{lemma}[Schur's lemma]\index{Schur's lemma}
  Let $M_1, M_2$ be simple right $A$-modules. Then either $M_1 \cong M_2$, or $\Hom_A(M_1, M_2) = 0$. If $M$ is a simple $A$-module, then $\End_A(M)$ is a division algebra.
\end{lemma}

\begin{proof}
  A non-zero $A$-module homomorphism $M_1 \to M_2$ must be injective, as the kernel is submodule. Similarly, the image has to be the whole thing since the image is a submodule. So this must be an isomorphism, and in particular has an inverse. So the last part follows as well.
\end{proof}

The proof of Artin--Wedderburn relies on the isomorphism
\[
  \End_A(A_A) \cong A_A.
\]
To see this isomorphism, note that any endomorphism of $A_A$ is uniquely determined by the image of $1$, and this gives a map $\End_A(A_A) \mapsto A_A$. Conversely, any element $x \in A_A$ determines an endomorphism of $A_A$ given by left-multiplication by $x$. This is indeed a homomorphism since $A_A$ is a \emph{right} module.

This is really all we need, but we have the following slightly more general result:
\begin{lemma}\leavevmode
  \begin{enumerate}
    \item If $M$ is a right $A$-module and $e$ is an idempotent in $A$, ie. $e^2 = e$, then $Me \cong \Hom_A(eA, M)$.
    \item We have
      \[
        eAe \cong \End_A(eA).
      \]
      In particular, we can take $e = 1$, and recover $\End_A(A_A) \cong A$.
  \end{enumerate}
\end{lemma}

\begin{proof}\leavevmode
  \begin{enumerate}
    \item We define maps
      \[
        \begin{tikzcd}[cdmap]
          me \ar[r, maps to] & (ex \mapsto mex)\\
          Me \ar[r, "f_1", yshift=2] & \Hom(eA, M) \ar[l, "f_2", yshift=-2]\\
          \alpha(e) & \alpha \ar[l, maps to]
        \end{tikzcd}
      \]
      We note that $\alpha(e) = \alpha(e^2) = \alpha(e) e \in Me$. So this is well-defined. By inspection, these maps are inverse to each other. So we are done.

      Note that we might worry that we have to pick representatives $me$ and $ex$ for the map $f_1$, but in fact we can also write it as $f(a)(y) = ay$, since $e$ is idempotent. So we are safe.
    \item Immediate from above by putting $M = eA$.
  \end{enumerate}
\end{proof}

\begin{lemma}
  Let $M$ be a completely reducible right $A$-module. We write
  \[
    M = \bigoplus S_i^{n_i},
  \]
  where $\{S_i\}$ are distinct simple $A$-modules. Write $D_i = \End_A(S_i)$, which we already know is a division algebra. Then
  \[
    \End_A (S_i^{n_i}) \cong M_{n_i} (D_i),
  \]
  and
  \[
    \End_A(M) = \bigoplus M_{n_i} (D_i)
  \]
\end{lemma}

\begin{proof}
  The result for $\End_A(S_i^{n_i})$ is just the familiar fact that a homomorphism $S^n \to S^m$ is given by an $m \times n$ matrix of maps $S \to S$ (in the case of vector spaces over a field $k$, we have $\End(k) \cong k$, so they are matrices with entries in $k$). Then by Schur's lemma, we have
  \[
    \End_A(M) = \bigoplus_i \End_A(M_i) \cong M_{n_i}(D_i).
  \]
\end{proof}

We now prove Artin--Wedderburn.
\begin{proof}[Proof of Artin--Wedderburn]
  If $A$ is semi-simple, then it is completely reducible as a right $A$-module. So we have
  \[
    A \cong \End(A_A) \cong \bigoplus M_{n_i}(D_i).
  \]
  We now decompose each $M_{n_i}(D_i)$ into a sum of simple modules. We know each $M_{n_i}(D_i)$ is a non-trivial $M_{n_i}(D_i)$ module in the usual way, and the action of the other summands is trivial. We can simply decompose each $M_{n_i}(D_i)$ as the sum of submodules of the form
  \[
    \left\{
      \begin{pmatrix}
        0 & 0 & \cdots & 0 & 0\\
        \vdots & \vdots & \ddots & \vdots & \vdots\\
        0 & 0 & \cdots & 0 & 0\\
        a_1 & a_2 & \cdots & a_{n_i - 1} & a_{n_i}\\
        \vdots & \vdots & \ddots & \vdots & \vdots\\
        0 & 0 & \cdots & 0 & 0 \\
      \end{pmatrix}
    \right\}
  \]
  and there are $n_i$ components. We immediately see that if we write $S_i$ for this submodule, then we have
  \[
    \dim_{D_i}(S_i) = n_i.
  \]
  Finally, we have to show that every simple module $S$ of $A$ is one of the $S_i$. We simply have to note that if $S$ is a simple $A$-module, then there is a non-trivial map $f: A \to S$ (say by picking $x \in S$ and defining $f(a) = xa$). Then in the decomposition of $A$ into a direct sum of simple submouldes, there must be one factor $S_i$ such that $f|_{S_i}$ is non-trivial. Then by Shur's lemma, this is in fact an isomorphism $S_i \cong S$.
\end{proof}

This was for semi-simple algebras. For a general right Artinian algebra, we know that $A/J(A)$ is semi-simple and inherits the Artinian property. Then Artin--Wedderburn applies to $A/J(A)$.

Note that if $A$ is a finite-dimensional $k$-algebra, then it is certainly Artinian. So we must have that the division algebras $D_i$ appearing in Artin--Wedderburn are also finite-dimensional $k$-vector spaces.

Now pick an arbitrary $x \in D_i$. Then the sub-algebra of $D_i$ generated by $x$ must be commutative. So it is in fact a subfield, and finite dimensionality means it is algebraic over $k$. If $k$ is algebraically closed, then $x$ must be in $k$, and so we've shown that these $D_i$ must be $k$. Thus we get

\begin{cor}
  If $k$ is algebraically closed and $A$ is a finite-dimensional semi-simple $k$-algebra, then
  \[
    A \cong \bigoplus M_{n_i}(k).
  \]
\end{cor}
This is true, for example, when $k = \C$.

We shall end by applying our results to group algebras. Recall the following definition:
\begin{defi}[Group algebra]\index{group algebra}\index{$kG$}
  Let $G$ be a group and $k$ a field. The \emph{group algebra} of $G$ over $k$ is
  \[
    kG = \left\{\sum \lambda_g g: g \in G, \lambda_g \in k\right\}.
  \]
  This has a bilinear multiplication given by the obvious formula
  \[
    (\lambda_g g) (\mu_h h) = \lambda_g \mu_h (gh).
  \]
\end{defi}

The first thing to note is that group algebras are almost always semi-simple.
\begin{thm}[Maschke's theorem]\index{Maschke's theorem}
  Let $G$ be a finite group and $p \nmid |G|$, where $p = \Char k$, so that $|G|$ is invertible in $k$, then $kG$ is semi-simple.
\end{thm}

\begin{proof}
  We show that any submodule $V$ of a $kG$-module $U$ has a complement. Let $\pi: U \to V$ be any $k$-vector space projection, and define a new map
  \[
    \pi' = \frac{1}{|G|} \sum_{g \in G} g\pi g^{-1}: U \to V.
  \]
  It is easy to see that this is a $kG$-module homomorphism $U \to V$, and is a projection. So we have
  \[
    U = V \oplus \ker \pi',
  \]
  and this gives a $kG$-module complement.
\end{proof}

There is a converse to Maschke's theorem:
\begin{thm}
  Let $G$ be finite and $kG$ semi-simple. Then $\Char k \nmid |G|$.
\end{thm}

\begin{proof}
  We note that there is a simple $kG$-module $S$, given by the trivial module. This is a one-dimensional $k$ vector space. We have
  \[
    D = \End_{kG}(S) = k.
  \]
  Now suppose $kG$ is semi-simple. Then by Artin--Wedderburn, there must be only one summand of $S$ in $kG$.

  Consider the following two ideals of $kG$: we let
  \[
    I_1 = \left\{\sum \lambda_g g \in kG: \sum \lambda_g = 0\right\}.
  \]
  This is in fact a two-sided ideal of $kG$. We also have the center of the algebra, given by
  \[
    I_2 = \left\{\lambda \sum g \in kG: \lambda \in k\right\}.
  \]
  Now if $\Char k \mid |G|$, then we find that $I_2 \subseteq I_2$. Then we can write
  \[
    kG = \frac{kG}{I_1} \oplus I_1 = \frac{kG}{I_1} \oplus I_2 \oplus \cdots,
  \]
  and we know $\frac{kG}{I_1}$ has dimension $1$. So $kG$ has two summands. This gives a contradiction. So we must have $\Char k \nmid |G|$.
\end{proof}

We can do a bit more of representation theory. Recall that when $k$ is algebraically closed and has characteristic zero, then the number of simple $kG$-modules is the number of conjugacy classes of $G$. There is a more general result for a general characteristic $p$ field:
\begin{thm}
  Let $k$ be algebraically closed, and $G$ be finite. Then the number of simple $kG$ modules (up to isomorphism) is equal to the number of conjugacy classes of elements of order not divisible by $p$. These are known as the \term{$p$-regular elements}.
\end{thm}

We immediately deduce that
\begin{cor}
  If $|G| = p^r$ for some $r$ and $p$ is prime, then the trivial module is the only simple $kG$ module, when $\Char k = p$.
\end{cor}
Note that we can prove this directly rather than using the theorem, by showing that $I = \ker (kG \to k)$ is a nilpotent ideal, and annihilates all simple modules. % exercise

\begin{proof}[Proof sketch of theorem]
  The number of simple $kG$ is just the number of simple $kG/J(kG)$ module. % why?
  There is a useful trick to figure out the number of simple $A$-modules for a given semi-simple $A$. Suppose we have a decomposition
  \[
    A \cong \bigoplus_{i = 1}^r M_{n_i}(k).
  \]
  Then we know $r$ is the number of simple $A$-modules. We now consider $[A, A]$, the $k$-subspace generated by elements of the form $xy - yx$. Then we see that
  \[
    \frac{A}{[A, A]} \cong \bigoplus_{i = 1}^r \frac{M_{n_i}(k)}{[M_{n_i}(k), M_{n_i}(k)]}.
  \]
  Now by linear algebra, we know $[M_{n_i}(k), M_{n_i}(k)]$ is trace zero matrices, and so we know
  \[
    \dim_k \frac{M_{n_i}(k)}{[M_{n_i}(k), M_{n_i}(k)]} = 1.
  \]
  Hence we know
  \[
    \dim \frac{A}{[A, A]} = r.
  \]
  Thus we need to compute
  \[
    \dim_k \frac{kG/J(kG)}{[kG/J(kG), kG/J(kG)]}
  \]
  We then note the following facts:
  \begin{enumerate}
    \item For a general algebra $A$, we have
      \[
        \frac{A/J(A)}{[A/J(A), A/J(A)]} \cong \frac{A}{[A, A] + J(A)}.
      \]
    \item Let $g_1, \cdots, g_m$ be conjugacy class representatives of $G$. Then
      \[
        \{g_i + [kG, kG]\}
      \]
      forms a $k$-vector space basis of $kG/[kG, kG]$. % exercise!
    \item If $g_1, \cdots, g_r$ is a set of representatives of $p$-regular conjugacy classes, then
      \[
        \left\{g_i + \Big([kG, kG] + J(kG)\Big)\right\}
      \]
      form a basis of $kG/([kG, kG] + J(kG))$. % not exercise!
  \end{enumerate}
  Hence the result follows.
\end{proof}
One may find it useful to note that $[kG, kG] + J(kG)$ consists of the elements in $kG$ such that $x^{p^s} \in [kG, kG]$ for some $s$.

Note that $A/[A, A]$ is the $0$th Hochschild homology group of the algebra $A$. In the usual proof of this result for the the characteristic $0$ case, instead of lookinng at $A/[A, A]$, we look at the center $Z(A)$. This center is the $0$th Hochschild cohomology group of $A$.

\subsection{Cross products}
Number theorists are often interested in representations of Galois groups and $kG$-modules where $k$ is an algebraic number field, eg. $\Q$. In this case, the $D_i$'s appearing in Artin--Wedderburn may be non-commutative.

We have already met one case of a non-commutative division ring, namely the quaternions $\H$. This is in fact an example of a general construction.

\begin{defi}[Crossed product]\index{crossed product}
  The \emph{crossed product} of a $k$-algebra $B$ and a group $G$ is specified by the following data:
  \begin{itemize}
    \item A group homomorphism $\phi: G \to \Aut_k(B)$, written
      \[
        \phi_g(\lambda) = \lambda^g;
      \]
    \item A function
      \[
        \Psi(g, h): G \times G \to B.
      \]
  \end{itemize}
  The cross product algebra has underlying set
  \[
    \sum \lambda_g g: \lambda_g \in B.
  \]
  with operation defined by
  \[
    \lambda g \cdot \mu h = \lambda \mu^g \Psi(g, h) (gh).
  \]
  The function $\Psi$ is required to be such that the resulting product is associative.
\end{defi}
We should think of the $\mu^g$ as specifying what happens when we conjugate $g$ pass $\mu$, and then $\Psi(g, h) (gh)$ is the product of $g$ and $h$ in the crossed product.

Usually, we take $B = K$, a Galois extension of $k$, and $G = \Gal(K/k)$. Then the action $\phi_g$ is the natural action of $G$ on the elements of $K$, and we restrict to maps $\Psi: G \times G \to K^\times$ only.

\begin{eg}
  Consider $B = K = \C$, and $k = \R$. Then $G = \Gal(\C/\R) \cong \Z/2\Z = \{e, g\}$, where $g$ is complex conjugation. The elements of $\H$ are of the form
  \[
    \lambda_e e + \lambda_g g,
  \]
  where $\lambda_e, \lambda_g \in \C$, and we will write
  \[
    1 \cdot g = g,\quad i \cdot g = k,\quad 1 \cdot e = 1,\quad i \cdot e = i.
  \]
  Now we want to impose
  \[
    -1 = j^2 = 1g \cdot 1g = \psi(g, g) e.
  \]
  So we set $\Psi(g, g) = -1$. We can similarly work out what we want the other values of $\Psi$ to be. % complete?
\end{eg}
Note that in general, crossed products need not be division algebras.

Note that the crossed product is naturally a \term{$G$-graded algebra}, in the sense that we can write it as a direct sum
\[
  BG = \bigoplus_{g \in G} Bg,
\]
and we have $Bg_1 \cdot Bg_2 \subseteq B g_1 g_2$.

Focusing on the case where $K/k$ is a Galois extension, we use the notation $(K, G, \Psi)$, where $\Psi: G \times G \to K^\times$. Associativity of these crossed products is equivalent to a \term{$2$-cocycle condition} $\Psi$, which you will be asked to make precise on the first example sheet.

Two crossed products $(K, G, \Psi_1)$ and $(K, G, \Psi_2)$ are isomorphic iff the map
\[
  \begin{tikzcd}[cdmap]
    G \times G \ar[r] & K^\times\\
    (g, h) \ar[r, maps to] & \Psi_1(g, h) (\Psi_2(g, h))^{-1}
  \end{tikzcd}
\]
satisfies a $2$-coboundary condition, which is again left for the first example sheet. Therefore the second (group) cohomology
\[
  \frac{\{\text{$2$-cocycles}: G \times G \to K^\times\}}{\{\text{$2$-coboundaries}: G \times G \to K^\times\}}
\]
determines the isomorphism classes of (associative) crossed products $(K, G, \Psi)$.

\begin{defi}[Central simple algebra]\index{central simple algebra}\index{algebra!central simple}
  A \emph{central simple $k$-algebra} is a finite-dimensional $k$-algebra which is a simple algebra, and with a center $Z(A) = k$.
\end{defi}
Note that any simple algebra is a division algebra, say by Schur's lemma. So the center must be a field. Hence any simple $k$-algebra can be made into a central simple algebra simply by enlarging the base field.

\begin{eg}
  $M_n(k)$ is a central simple algebra.
\end{eg}

\begin{fact}
  Any central simple $k$-algebra is of the form $M_n(D)$ for some division algebra $D$ which is also a central simple $k$-algebra, and is a crossed product $(K, G, \Psi)$.
\end{fact}
Note that when $K = \C$ and $k = \R$, then the second cohomology group has $2$ elements, and we get that the only central simple $\R$-algebras are $M_n(\R)$ or $M_n(\H)$.

\begin{fact}[Wedderburn]
  Every finite division algebra is a field.
\end{fact}

\subsection{Projectives and blocks}
In general, if $A$ is not semi-simple, then it is not possible to decompose $A$ as a direct sum of simple modules. However, what we can do is to decompose it as a direct sum of indecomposable projectives.

We begin with the definition of a projective module.

\begin{defi}[Projective module]\index{projective module}\index{module!projective}
  An $A$-module is \emph{projective} $P$ if given modules $M$ and $M'$ and maps
  \[
    \begin{tikzcd}
      & P \ar[d, "\alpha"]\\
      M' \ar[r, two heads, "\theta"] & M \ar[r] & 0
    \end{tikzcd},
  \]
  then there exists a map $\beta: P \to M$ such that the following diagram commutes:
  \[
    \begin{tikzcd}
      & P \ar[d, "\alpha"] \ar[ld, dashed, "\beta"']\\
      M' \ar[r, two heads, "\theta"] & M \ar[r] & 0
    \end{tikzcd}.
  \]
  Equivalently, if we have a short exact sequence
  \[
    \begin{tikzcd}
      0 \ar[r] & N \ar[r, hook] & M' \ar[r, two heads] & M \ar[r] & 0,
    \end{tikzcd}
  \]
  then the sequence
  \[
    \begin{tikzcd}
      0 \ar[r] & \Hom(P, N) \ar[r, hook] & \Hom(P, M') \ar[r, two heads] & \Hom(P, M) \ar[r] & 0
    \end{tikzcd}
  \]
  is exact.
\end{defi}
Note that we get exactness at $\Hom(P, N)$ and $\Hom(P, M')$ for any $P$ at all. Projective means it is also exact at $\Hom(P, M)$.

\begin{eg}
  Free modules are always projective.
\end{eg}
In general, projective modules are ``like'' free modules. We all know that free modules are nice, and most of the time, when we want to prove things about free modules, we are just using the property that they are projective. It is also possible to understand projective modules in an algebro-geometric way --- they are ``locally free'' modules.

It is convenient to characterize projective modules as follows:
\begin{lemma}
  The following are equivalent:
  \begin{enumerate}
    \item $P$ is projective.
    \item Every surjective map $\phi: M \to P$ splits, ie.
      \[
        M \cong \ker \phi \oplus N
      \]
      where $N \cong P$.
    \item $P$ is a direct summand of a free module.
  \end{enumerate}
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
    \item (i) $\Rightarrow$ (ii): Consider the following lifting problem:
      \[
        \begin{tikzcd}
          & P \ar[d, equals] \ar[dl, dashed]\\
          M \ar[r, two heads, "\phi"] & P \ar[r] & 0
        \end{tikzcd},
      \]
      The lifting gives an embedding of $P$ into $M$ that complements $\ker \phi$ (by the splitting lemma, or by checking it directly).
    \item (ii) $\Rightarrow$ (iii): Every module admits a surjection from a free module (eg. the free module generated by the elements of $P$)
    \item (iii) $\Rightarrow$ (i): It suffices to show that direct summands of projectives are projective. Suppose $P$ is is projective, and
      \[
        P \cong A \oplus B.
      \]
      Then any diagram
      \[
        \begin{tikzcd}
          & A \ar[d, "\alpha"]\\
          M' \ar[r, two heads, "\theta"] & M \ar[r] & 0
        \end{tikzcd},
      \]
      can be extended to a diagram
      \[
        \begin{tikzcd}
          & A \oplus B \ar[d, "\tilde{\alpha}"]\\
          M' \ar[r, two heads, "\theta"] & M \ar[r] & 0
        \end{tikzcd},
      \]
      by sending $B$ to $0$. Then since $A \oplus B \cong P$ is projective, we obtain a lifting $A \oplus B \to M'$, and restricting to $A$ gives the desired lifting.
  \end{itemize}
\end{proof}

Our objective is to understand the direct summands of a general Artinian $k$-algebra $A$, not necessarily semi-simple. Since $A$ is itself a free $A$-module, we know these direct summands are always projective.

Since $A$ is not necessarily semi-simple, it is in general impossible to decompose it as a direct sum of simples. What we can do, though, is to decompose it as a direct sum of \emph{indecomposable modules}.

\begin{defi}[Indecomposable]\index{indecomposable}
  A non-zero module $M$ is indecomposable if $M$ cannot be expressed as the direct sum of two non-zero submodules.
\end{defi}
Note that since $A$ is (left) Artinian, it can always be decomposed as a finite sum of indecomposable (left) submodules. Sometimes, we are also interested in decomposing $A$ as a sum of (two-sided) ideals. These are called blocks.

\begin{defi}[Block]\index{block}
  The \emph{blocks} are the direct summands of $A$ that are indecomposable as ideals.
\end{defi}

\begin{eg}
  If $A$ is semi-simple Artinian, then Artin-Wedderburn tells us
  \[
    A = \bigoplus M_{n_i}(D_i),
  \]
  and the $M_{n_i}(D_i)$ are the blocks.
\end{eg}

We already know that every Artinian module can be decomposed as a direct sum of indecomposables. The first question to ask is whether this is unique. We note the following definitions:

\begin{defi}[Local algebra]\index{local algebra}\index{algebra!local}
  An algebra is \emph{local} if it has a unique maximal left ideal, which is $J(A)$, which is the unique maximal right ideal.
\end{defi}
If so, then $A/J(A)$ is a division algebra.

\begin{defi}[Unique decomposition property]\index{unique decomposition property}
  A module $M$ has the \emph{unique decomposition property} if $M$ is a finite direct sum of indecomposable modules, and if
  \[
    M = \bigoplus_{i = 1}^m M_i = \bigoplus_{i = 1}^n M_i',
  \]
  then $n = m$, and, after reordering, $M_i = M_i'$.
\end{defi}

We want to prove that $A$ as an $A$-module always has the unique decomposition property. The first step is the following criterion for determining unique decomposition property.

\begin{thm}[Krull--Schmidt theorem]\index{Krull--Schmidt theorem}
  Suppose $M$ is a finite sum of indecomposable $A$-modules $M_i$, with each $\End(M_i)$ local. Then $M$ has the unique decomposition property.
\end{thm}

\begin{proof} % return to this later
  Let
  \[
    M = \bigoplus_{i = 1}^m M_i = \bigoplus_{i = 1}^n M_i'.
  \]
  We prove by induction on $m$. If $m = 1$, then $M$ is indecomposable. Then we must have $n = 1$ as well, and the result follows.

  For $m > 1$, we consider the maps
  \[
    \begin{tikzcd}
      \alpha_i: M_i' \ar[r, hook] & M \ar[r, two heads] & M_1\\
      \beta_i: M_1 \ar[r, hook] & M \ar[r, two heads] & M_i'
    \end{tikzcd}
  \]
  We observe that
  \[
    \id_{M_1} = \sum_{i = 1}^n \alpha_i \circ \beta_i: M_1 \to M_1.
  \]
  Since $\End_A(M_1)$ is local, we know some $\alpha_i \circ \beta_i$ must be invertible, ie. a unit, as they cannot all lie in the Jacobson radical. We may wlog $\alpha_i \circ \beta_1$ is a unit. If this is the case, then both $\alpha_1$ and $\beta_1$ have to be invertible. So $M_1 \cong M_1'$. % why

  Consider the map $\id - \theta = \phi$, where
  \[
    \begin{tikzcd}
      \theta: M \ar[r, two heads] & M_1 \ar[r, "\alpha_1^{-1}"] & M_1' \ar[r, hook] & M \ar[r, two heads] & \bigoplus_{i = 2}^m M_i \ar[r, hook] & M.
    \end{tikzcd}
  \]
  Then $\phi(M_1') = M_1$. So $\phi|_{M_1'}$ looks like $\alpha_1$. Also
  \[
    \phi\left(\bigoplus_{i = 2}^m M_i\right) \bigoplus_{i = 2}^m M_i,
  \]
  So $\phi|_{\bigoplus_{i = 2}^m M_i}$ looks like the identity map. So in particular, we see that $\phi$ is surjective. However, if $\phi(x) = 0$, this says $x = \theta(x)$, So
  \[
    x \in \bigoplus_{i = 2}^m M_i.
  \]
  But then $\theta(x) = 0$. Thus $x = 0$. Thus $\phi$ is an automorphism of $m$ with $\phi(M_1') = \phi(M_1)$. So this gives an isomorphism between
  \[
    \bigoplus_{i = 2}^m M_i = \frac{M}{M_1} \cong \frac{M}{M_1} \cong \bigoplus_{i = 2}^n M_i',
  \]
  and so we are done by induction.
\end{proof}

Now it remains to prove that the endomorphism rings are local. Recall the following result from linear algebra.
\begin{lemma}[Fitting]
  Suppose $M$ is a module with both the ACC and DCC on submodules, and let $f \in \End_A(M)$. Then for large enough $n$, we have
  \[
    M = \im f^n \oplus \ker f^n.
  \]
\end{lemma}

\begin{proof}
  By ACC and DCC, we may choose $n$ large enough so that
  \[
    f^n: f^n(M) \to f^{2n}(M)
  \]
  is an isomorphism, as if we keep iterating $f$, the image is a descending chain and the kernel is an ascending chain, and these have to terminate.

  If $m \in M$, then we can write
  \[
    f^n(m) = f^{2n}(m_1)
  \]
  for some $m_1$. Then
  \[
    m = f^n(m_1) + (m - f^n(m_1)) \in \im f^n + \ker f^n,
  \]
  and also
  \[
    \im f^n \cap \ker f^n = \ker (f^n: f^n(M) \to f^{2n}(M)) = 0.
  \]
  So done.
\end{proof}

\begin{lemma}
  Suppose $M$ is an indecomposable module satisfying ACC and DCC on submodules. Then $B = \End_A(M)$ is local.
\end{lemma}

\begin{proof}
  Choose a maximal left ideal of $B$, say $I$. It's enough to show that if $x \not \in I$, then $x$ is invertible. By maximality of $I$, we know $B = Bx + I$. We write
  \[
    1 = \lambda x + y,
  \]
  for some $\lambda \in B$ and $y \in I$. Since $y \in I$, it has no left inverse. So it is not an isomorphism. By Fitting's lemma and the indecomposability of $M$, we see that $y^m = 0$ for some $m$. Thus
  \[
    (1 + y + y^2 + \cdots + y^{m - 1}) \lambda x = (1 + y + \cdots + y^{m - 1})(1 - y) = 1,.
  \]
  So $x$ is invertible.
\end{proof}

\begin{cor}
  Let $A$ be a left Artinian algebra. Then $A$ has the unique decomposition property.
\end{cor}

\begin{proof}
  We know $A$ satisfies the ACC and DCC condition. So $_A A$ is a finite direct sum of indecomposables.
\end{proof}

So if $A$ is an Artinian algebra, we know $A$ can be uniquely decomposed as a direct sum of indecomposable projectives,
\[
  A = \bigoplus P_j.
\]
For convenience, we will work with right Artinian algebras and right modules instead of left ones. It turns out that instead of studying projectives in $A$, we can alternatively study idempotent elements instead.

Recall that $\End(A_A) \cong A$. The projection onto $P_j$ is achieved by left multiplication by an idempotent $e_j$,
\[
  P_j = e_j A.
\]
The fact that the $A$ decomposes as a \emph{direct sum} of the $P_j$ translates to the condition
\[
  \sum e_j = 1,\quad e_i e_j = 0
\]
for $i \not= j$.
\begin{defi}[Orthogonal idempotent]\index{orthogonal idempotents}\index{idempotent!orthogonal}
  A collection of idempotents $\{e_i\}$ is \emph{orthogonal} if $e_i e_j = 0$ for $i \not= j$.
\end{defi}

The indecomposability of $P_j$ is equivalent to $e_j$ being primitive:
\begin{defi}[Primitive idempotent]\index{idempotent!primitive}\index{primitive idempotent}
  An idempotent is \emph{primitive} if it cannot be expressed as a sum
  \[
    e = e' + e'',
  \]
  where $e', e''$ are orthogonal idempotents, both non-zero.
\end{defi}
We see that giving a direct sum decomposition of $A$ is equivalent to finding an orthogonal collection of primitive idempotents that sum to $1$. This is rather useful, because idempotents are easier to move around that projectives.

Our current plan is as follows --- given an Artinian algebra $A$, we can quotient out by $J(A)$, and obtain a semi-simple algebra $A/J(A)$. By Artin--Wedderburn, we know how we can decompose $A/J(A)$, and we hope to be able to deduce something useful about the decomposition of $A$ from this.
\begin{prop}
  Let $N$ be a nilpotent ideal in $A$, and let $f$ be an idempotent of $A/N \equiv \bar{A}$. Then there is an idempotent $e \in A$ with $f = \bar{e}$.
\end{prop}
In particular, we know $J(A)$ is nilpotent, and this proposition applies. The proof involves a bit of magic.
\begin{proof}
  We consider the quotients $A/N^i$ for $i \geq 1$. We will lift the idempotents successively as we increase $i$, and since $N$ is nilpotent, repeating this process will eventually land us in $A$.

  Suppose we have found an idempotent $f_{i - 1} \in A/N^{i - 1}$ with $\bar{f}_{i - 1} = f$. We want to find $f_i \in A/N^i$ such that $\bar{f}_i = f$.

  For $i > 1$, we let $x$ be an element of $A/N^i$ with image $f_{i - 1}$ in $A/N^{i - 1}$. Then
  \[
    x^2 - x \in N^{i - 1}/N^i,
  \]
  and so $(x^2 - x)^2 = 0$ in $A/N^i$.

  Let
  \[
    f_i = 3x^2 - 2x^3.
  \]
  Then by a direct computation, we find $f_i^2 = f_i$, and $f_i$ has image $3f_{i - 1} - 2 f_{i - 1} = f_{i - 1}$ in $A/N^{i - 1}$ (alternatively, in characteristic $p$, we can use $f_i = x^p$). Since $N^k = 0$ for some $k$, this process gives us what we want.
\end{proof}

Just being able to lift idempotents is not good enough. We want to lift decompositions as projective indecomposables. So we need to do better.
\begin{cor}
  Let $N$ be a nilpotent ideal of $A$. Let
  \[
    \bar{1} = f_1 + \cdots + f_r
  \]
  with $\{f_i\}$ orthogonal primitive idempotents in $A/N$. Then we can write
  \[
    1 = e_1 + \cdots + e_r,
  \]
  with $\{e_i\}$ orthogonal primitive idempotents in $A$, and $\bar{e}_i = f_i$.
\end{cor}

\begin{proof}
  We define $e_i' \in A$ inductively. We set
  \[
    e_1' = 1.
  \]
  Then for each $i > 1$, we pick $e_i'$ a lift of $f_i + \cdots + f_t \in e_{i - 1}' A e_{i - 1}'$, since by inductive hypothesis we know that $f_i + \cdots + f_t \in e_{i - 1}' A e_{i - 1}'/N$. Then
  \[
    e_i' e_{i + 1}' = e_{i + 1}' = e_{i + 1}' e_i'.
  \]
  We let
  \[
    e_i = e_i'- e_{i + 1}'.
  \]
  Then
  \[
    \bar{e}_i = f_i.
  \]
  Also, if $j > i$, then
  \[
    e_j = e_{i + 1}' e_j e_{i + 1}',
  \]
  and so
  \[
    e_i e_j = (e_i' - e_{i + 1}') e_{i + 1}' e_j e_{i + 1}' = 0.
  \]
  Similarly $e_j e_i = 0$.
\end{proof}

We now apply this lifting of idempotents to $N = J(A)$, which we know is nilpotent. We know $A/N$ is the direct sum of simple modules, and thus the decomposition corresponds to
\[
  \bar{1} = f_1 + \cdots + f_t \in A/J(A),
\]
and these $f_i$ are orthogonal primitive idempotents. Idempotent lifting then gives
\[
  1 = e_1 + \cdots + e_t \in A,
\]
and these are orthogonal primitive idempotents. So we can write
\[
  A = \bigoplus e_j A = \bigoplus P_i,
\]
where $P_i = e_i A$ are indecomposable projectives, and $P_i/P_i J(A) = S_i$ is simple. By Krull--Schmidt, any indecomposable projective isomorphic to one of these $P_j$.

The final piece of the picture is when two indecomposable projectives lie in the same block. Recall that if $M$ is a right $A$-module and $e$ is idempotent, then
\[
  Me \cong \Hom_A(eA, M).
\]
In particular, if $M = fA$ for some idempotent $f$, then
\[
  \Hom(eA, fA) \cong fAe.
\]
However, if $e$ and $f$ are in different blocks, say $B_1$ and $B_2$, then
\[
  fAe \in B_1 \cap B_2 = 0,
\]
since $B_1$ and $B_2$ are (two-sided!) ideals. So we know
\[
  \Hom(eA, fA) = 0.
\]
So if $\Hom(eA, fA) \not= 0$, then they are in the same block. The existence of a homomorphism can alternatively be expressed in terms of composition factors.

We have seen that each indecomposable projective $P$ has a simple ``top''
\[
  P/PJ(A) \cong S.
\]
\begin{defi}[Composition factor]\index{composition factor}
  A simple module $S$ is a composition factor of a module $M$ if there are submodules $M_1 \leq M_2$ with
  \[
    M_2/M_1 \cong S.
  \]
\end{defi}
Suppose $S$ is a composition factor of a module $M$. Then we have a diagram
\[
  \begin{tikzcd}
    & P \ar[d] \ar[dl, dashed]\\
     M_2 \ar[r, two heads ] & S \ar[r] & 0
  \end{tikzcd}
\]
So by definition of projectivity, we obtain a non-zero diagonal map $P \to M_2 \leq M$ as shown.

\begin{lemma}
  Let $P$ be an indecomposable projective, and $M$ an $A$-module. Then $\Hom(P, M) \not= 0$ iff $P/P J(A)$ is a composition factor of $M$.
\end{lemma}

\begin{proof}
  We have proven $\Rightarrow$. Conversely, suppose there is a non-zero map $f: P \to M$. Then it factors as
  \[
    S = \frac{P}{PJ(A)} \to \frac{\im f}{(\im f)J(A)}.
  \]
  Now we cannot have $\im f = (\im f)J(A)$, or else we have $\im f = (\im f)J(A)^n = 0$ for sufficiently large $n$ since $J(A)$ is nilpotent. So this map must be injective, hence an isomorphism. So this exhibits $S$ as a composition factor of $M$.
\end{proof}

We define a (directed) graph whose vertices are labelled by indecomposable projectives, and there is an edge $P_1 \to P_2$ if the top $S_1$ of $P_1$ is a composition factor of $P_2$.
\begin{thm}
  Indecomposable projectives $P_1$ and $P_2$ are in the same block if and only if they lie in the same connected component of the graph.
\end{thm}

\begin{proof}
  It is clear that $P_1$ and $P_2$ are in the same connected component, then they are in the same block.

  Conversely, consider a connected component $X$, and consider
  \[
    I = \bigoplus_{P \in X} P.
  \]
  We show that this is in fact a left ideal, hence an ideal. Consider any $x \in A$. Then for each $P \in X$, left-multiplication gives a map $P \to A$, and if we decompose
  \[
    A = \bigoplus P_i,
  \]
  then this can be expressed as a sum of maps $f_i: P \to P_i$. Now such a map can be non-zero only if $P$ is a composition factor of $P_i$. So if $f_i \not= 0$, then $P_i \in X$. So left-multiplication by $x$ maps $I$ to itself, and it follows that $I$ is an ideal.
\end{proof}

\subsection{\texorpdfstring{$K_0$}{K_0}}
We now briefly talk about the notion of $K_0$.
\begin{defi}[$K_0$]\index{$K_0$}
  For any associative $k$-algebra $A$, consider the free abelian group with basis labelled by the isomorphism classes $[P]$ of finitely-generated projective $A$-modules. Then introduce relations
  \[
    [P_1] + [P_2] = [P_1 \oplus P_2],
  \]
  This yields an abelian group which is the quotient of the free abelian group by the subgroup generated by
  \[
    [P_1] + [P_2] - [P_1 \oplus P_2].
  \]
  The abelian group is $K_0(A)$.
\end{defi}

\begin{eg}
  If $A$ is an Artinian algebra, then we know that any finitely-generated projective is a direct sum of indecomposable projectives, and this decomposition is unique by Krull-Schmidt. So
  \[
    K_0(A) = \left\{\parbox{7cm}{\centering abelian group generated by the isomorphism classes of indecomposable projectives}\right\}.
  \]
  So $K_0(A) \cong \Z^r$, where $r$ is the number of isomorphism classes of indecomposable projectives, which is the number of isomorphism classes of simple modules.

  Here we're using the fact that two indecomposable projectives are isomorphic iff their simple tops are isomorphic.
\end{eg}

It turns out there is a canonical map $K_0(A) \to A/[A, A]$. Recall we have met $A/[A, A]$ when we were talking about the number of simple modules. We remarked that it was the $0$th Hochschild homology group, and when $A = kG$, we have that there is a $k$-basis of $A/[A, A]$ given by $g_i + [A, A]$, where $g_i$ are conjugacy class representatives.

First of all, consider the trace map
\[
  M_n(A) \to A/[A, A].
\]
This is a $k$-linear map, invariant under conjugation. We also note that the canonical inclusion
\begin{align*}
  M_n(A) &\hookrightarrow M_{n + 1}(A)\\
  X & \mapsto
  \begin{pmatrix}
    X & 0\\
    0 & 0
  \end{pmatrix}
\end{align*}
is compatible with the trace map. We observe that the trace induces an isomorphism
\[
  \frac{M_n(A)}{[M_n(A), M_n(A)]} \to \frac{A}{[A, A]},
\]
by linear algebra.

Now if $P$ is finitely generated projective. It is a direct summand of some $A^n$. Thus we can write
\[
  A^n = P \oplus Q,
\]
for $P, Q$ projective. Moreover, projection onto $P$ corresponds to an idempotent $e$ in $M_n(A) = \End_A(A^n)$, and that
\[
  P = e(A^n).
\]
and we have
\[
  \End_A(P) = e M_n(A) e.
\]
Any other choice of idempotent yields an idempotent $e_1$ conjugate to $e$ in $M_{2n}(A)$. % exercise

Therefore the trace of an endomorphism of $P$ is well-defined in $A/[A, A]$, independent of the choice of $e$. Thus we have a trace map
\[
  \End_A(P) \to A/[A, A].
\]
In particular, the trace of the identity map on $P$ is the trace of $e$. We call this the \emph{trace of $P$}\index{trace!of projective}.

Note that if we have finitely generated projectives $P_1$ and $P_2$, then we have
\begin{align*}
  P_1 \oplus Q_1 &= A^n\\
  P_2 \oplus Q_2 &= A^m
\end{align*}
Then we have
\[
  (P_1 \oplus P_2) \oplus (Q_1 \oplus Q_2) = A^{m + n}.\
\]
So we deduce that
\[
  \tr (P_1 \oplus P_2) = \tr P_1 + \tr P_2.
\]
\begin{defi}[Hattori-Stallings trace map]
  The map $K_0(A) \to A/[A, A]$ induced by the trace is the \term{Hattori-Stallings trace map}.
\end{defi}

\begin{eg}
  Let $A = kG$, and $G$ be finite. Then $A/[A, A]$ is a $k$-vector space with basis labelled by a set of conjugacy class representatives $\{g_i\}$. Then we know, for a finitely generated projective $P$, we can write
  \[
    \tr P = \sum r_P(g_i) g_i,
  \]
  where the $r_p(g_i)$ may be regarded as class functions. However, $P$ may be regarded as a $k$-vector space.

  So the there is a trace map
  \[
    \End_K(P) \to k,
  \]
  and also the ``character'' $\chi_p: G \to k$, where $\chi_P(g) = \tr g$. Hattori proved that if $C_G(g)$ is the centralizer of $g \in G$, then
  \[
    \chi_p(g) = |C_G(G)| r_p(g^{-1}).\tag{$*$}
  \]
  If $\Char k = 0$ and $k$ is is algebraically closed, then we know $kG$ is semi-simple. So every finitely generated projective is a direct sum of simples, and
  \[
    K_0(kG) \cong \Z^r
  \]
  with $r$ the number of simples, and $(*)$ implies that the trace map
  \[
    \Z^r \cong K_0(kG) \to \frac{kG}{[kG, kG]} \cong k^r
  \]
  is the natural inclusion.
\end{eg}
This is the start of the theory of algebraic $K$-theory, which is a homology theory telling us about the endomorphisms of free $A$-modules. We'll (probably) meet $K_1(A)$, which is the abelianization of
\[
  \GL(A) = \lim_{n \to \infty} \GL_n(A).
\]
$K_2(A)$ tells us something about the relations required if you express $\GL(A)$ in terms of generators and relations. We're being deliberately vague. These groups are very hard to compute.

Just as we saw in the $i = 0$ case, there are canonical maps
\[
  K_i(A) \to HH_i(A),
\]
where $HH_*$ is the Hochschild homology. The $i = 1$ case is called the \term{Dennis trace map}. These are analogous to the \emph{Chern maps} in topology.

\section{Noetherian algebras}
\subsection{Noetherian algebras}
In the introduction, we met the definition of Noetherian algebras.
\begin{defi}[Noetherian algebra]\index{Noetherian algebra}\index{algebra!Noetherian}
  An algebra is \emph{left Noetherian} if it satisfies the \term{ascending chain condition} (\term{ACC}) on left ideals, ie. if
  \[
    I_1 \leq I_2 \leq I_3 \leq \cdots
  \]
  is an ascending chain of left ideals, then there is some $N$ such that $I_{N + m} = I_N$ for all $m \geq 0$.

  Similarly, we say an algebra is \emph{Noetherian} if it is both left and right Noetherian.
\end{defi}
We've also met a few examples. Here we are going to meet lots more.

One source of Noetherian algebras is via constructing polynomial and power series rings. Recall that in IB Groups, Rings and Modules, we proved the Hilbert basis theorem:
\begin{thm}[Hilbert basis theorem]\index{Hilbert basis theorem}
  If $A$ is Noetherian, then $A[X]$ is Noetherian.
\end{thm}
Note that our proof did not depend on $A$ being commutative. The same proof works for non-commutative rings. In particular, this tells us $k[X_1, \cdots, X_n]$ is Noetherian.

By a very similar proof, power series rings of Noetherian algebras are also Noetherian. The proof is very similar, but for completeness, we will spell it out completely.
\begin{thm}
  Let $A$ be left Noetherian. Then $A[[X]]$ is Noetherian.
\end{thm}

\begin{proof}
  Let $I$ be a left ideal of $A[[X]]$. We'll show that if $A$ is left Noetherian, then $I$ is finitely generated. Let
  \[
    J_r = \{a: \text{there exists an element of $I$ of the form }aX^r + \text{higher degree terms}\}.
  \]
  We note that $J_r$ is a left ideal of $A$, and also note that
  \[
    J_0 \leq J_1 \leq J_2 \leq J_3 \leq \cdots,
  \]
  as we can always multiply by $X$. Since $A$ is left Noetherian, this chain terminates at $J_N$ for some $N$. Also, $J_0, J_1, J_2, \cdots, J_N$ are all finitely generated left ideals. We suppose $a_{i1}, \cdots, a_{is_i}$ generates $J_i$ for $i = 1, \cdots, N$. These correspond to elements
  \[
    f_{ij}(X) = a_{ij} X^j + \text{higher odder terms} \in I.
  \]
  We show that this finite collection generates $I$ as a left ideal. Take $f(X) \in I$, and suppose it looks like
  \[
    b_n X^n + \text{higher terms},
  \]
  with $b_n \not = 0$.

  Suppose $n < N$. Then $b_n \in J^n$, and so we can write
  \[
    b_n = \sum c_{nj} a_{nj}.
  \]
  So
  \[
    f(X) - \sum c_{nj} f_{nj}(X) \in I
  \]
  has zero coefficient for $X^n$, and all other terms are of higher degree.

  Repeating the process, we may thus wlog $n \geq N$. We get $f(X)$ of the form $d_N X^N + $ higher degree terms. The same process gives
  \[
    f(X) - \sum c_{Nj} f_{Nj}(X)
  \]
  with terms of degree $N + 1$ or higher. We can repeat this yet again, using the fact $J_N = J_{N + 1}$, so we obtain
  \[
    f(X) - \sum c_{Nj} f_{Nj}(x) - \sum d_{N+1, j} X f_{Nj}(X) + \cdots.
  \]
  So we find
  \[
    f(X) = \sum e_j(X) f_{Nj}(X)
  \]
  for some $e_j(X)$. So $f$ is in the left ideal generated by our list, and hence so is $f$.
\end{proof}

\begin{eg}
  It is straightforward to see that quotients of Noetherian algebras are Noetherian. Thus, algebra images of the algebras $A[x]$ and $A[[x]]$ would also be Noetherian.
  
  For example, finitely-generated commutative $k$-algebras are always Noetherian. Indeed, if we have a generating set $x_i$ of $A$ as a $k$-algebra, then there is an algebra homomorphism
  \[
    \begin{tikzcd}[cdmap]
      k[X_1, \cdots, X_n] \ar[r] & A\\
      X_i \ar[r, maps to] & x_i
    \end{tikzcd}
  \]
\end{eg}

We also saw previously that
\begin{eg}
  Any Artinian algebra is Noetherian.
\end{eg}

The next two examples we are going to see are less obviously Noetherian, and proving that they are Noetherian takes some work. 

\begin{defi}[$n$th Weyl algebra]\index{Weyl algebra}\index{$A_n(k)$}\index{$A_n$}
  The \emph{$n$th Weyl algebra} $A_n(k)$ is the algebra generated by $X_1, \cdots, X_n, Y_1, \cdots, Y_n$ with relations
  \[
    Y_i X_i - X_i Y_i = 1,
  \]
  for all $i$, and everything else commutes.
\end{defi}

This algebra acts on the polynomial algebra $k[X_1, \cdots, X_n]$ with $X_i$ acting by left multiplication and $Y_i = \frac{\partial}{\partial X_i}$. Thus $k[X_1, \cdots, X_n]$ is a left $A_n(k)$ module. This is the prototype for thinking about differential algebras, and $D$-modules in general (which we will not talk about).

The other example we have is the universal enveloping algebra of a Lie algebra.
\begin{defi}[Universal enveloping algebra]\index{universal enveloping algebra}\index{Lie algebra!universal enveloping algebra}
  Let $\mathfrak{g}$ be a Lie algebra over $k$, and take a $k$-vector space basis $x_1, \cdots, x_n$. We form an associative algebra with generators $x_1, \cdots, x_n$ with relations
  \[
    x_i x_j - x_j x_i = [x_i, x_j],
  \]
  and this is the \emph{universal enveloping algebra} $\mathcal{U}(\mathfrak{g})$.
\end{defi}

\begin{eg}
  If $\mathfrak{g}$ is abelian, ie. $[x_i, x_j] = 0$ in $\mathfrak{g}$, then the enveloping algebra is the polynomial algebra in $x_1, \cdots, x_n$.
\end{eg}

\begin{eg}
  If $\mathfrak{g} = \sl_2(k)$, then we have a basis
  \[
    \begin{pmatrix}
      0 & 1\\
      0 & 0
    \end{pmatrix},\quad
    \begin{pmatrix}
      0 & 0\\
      1 & 0
    \end{pmatrix},\quad
    \begin{pmatrix}
      1 & 0\\
      0 & -1
    \end{pmatrix}.
  \]
  They satisfy
  \[
    [e, f] = h,\quad [h, e] = 2e,\quad [h, f] = -2f,
  \]
\end{eg}

To prove that $A_n(k)$ and $\mathcal{U}(\mathfrak{g})$ are Noetherian, we need some machinery, that involves some ``deformation theory''.

We note that often, our polynomials have a natural filtration.
\begin{defi}[Filtered algebra]\index{filtered algebra}\index{algebra!filtered}\index{$\Z$-filtered algebra}
  A \emph{($\Z$-)filtered algebra $A$} is is a collection of $k$-vector spaces
  \[
    \cdots \leq A_{-1} \leq A_0 \leq A_1 \leq A_2 \leq \cdots
  \]
  such that $A_i \cdot A_j \subseteq A_{i + j}$ for all $i, j \in \Z$, and $1 \in A_0$.
\end{defi}
For example a polynomial ring is naturally filtered by the degree of the polynomial.

The definition above was rather general, and often, we prefer to talk about more well-behaved filtrations.
\begin{defi}[Exhaustive filtration]\index{filtration!exhaustive}\index{exhaustive filtration}
  A filtration is \emph{exhaustive} if $\bigcup A_i = A$.
\end{defi}

\begin{defi}[Separated filtration]\index{separated filtration}\index{filtration!separated}
  A filtration is \emph{separated} if $\bigcap A_i = \{0\}$.
\end{defi}
Unless otherwise specified, our filtrations are exhaustive and separated.

For the moment, we will mostly be interested in positive filtrations.

\begin{defi}[Positive filtration]\index{positive filtration}\index{filtration!positive}
  A filtration is \emph{positive} if $A_i = 0$ for $i < 0$.
\end{defi}

Our canonical source of filtrations is the following construction:
\begin{eg}
  If $A$ is an algebra generated by $x_1, \cdots, x_n$, say, we can set
  \begin{itemize}
    \item $A_0$ is the $k$-span of $1$
    \item $A_1$ is the $k$-span of $1, x_1, \cdots, x_n$
    \item $A_1$ is the $k$-span of $1, x_1, \cdots, x_n, x_i x_j$ for $i, j \in \{1, \cdots, n\}$.
  \end{itemize}
  In general, $A_r$ is elements that are of the form of a (non-commutative) polynomial expression of degree $\leq r$.
\end{eg}
Of course, the filtration depends on the choice of the generating set.

Often, to understand a filtered algebra, we consider a nicer object, known as the \emph{associated graded algebra}.
\begin{defi}[Associated graded algebra]\index{associated graded algebra}
  Given a filtration of $A$, the \emph{associated graded algebra} is the vector space direct sum
  \[
    \gr A = \bigoplus \frac{A_i}{A_{i - 1}}.
  \]
  This is given the structure of an algebra by defining multiplication by
  \[
    (a + A_{i - 1}) (b + A_{j - 1}) = ab + A_{i + j - 1} \in \frac{A_{i + j}}{A_{i + j - 1}}.
  \]
\end{defi}
In our example of a finitely-generated algebra, the graded algebra is generated by $x_1 + A_0, \cdots, x_n + A_0 \in A_1/A_0$.

The associated graded algebra has the natural structure of a graded algebra:
\begin{defi}[Graded algebra]\index{graded algebra}\index{algebra!graded}\index{$\Z$-graded algebra}
  A ($\Z$-)\emph{graded algebra} is an algebra $B$ that is of the form
  \[
    B = \bigoplus_{i \in \Z} B_i,
  \]
  where $B_i$ are $k$-subspaces, and $B_i B_j \subseteq B_{i + j}$. The $B_i$'s are called the \term{homogeneous components}\index{graded algebra!homogeneous components}\index{algebra!homogeneous components}.

  A \term{graded ideal}\index{ideal!graded} is an ideal of the form
  \[
    \bigoplus J_i,
  \]
  where $J_i$ is a subspace of $B_i$, and similarly for left and right ideals.
\end{defi}

There is an intermediate object between a filtered algebra and its associated graded algebra, known as the \emph{Rees algebra}.
\begin{defi}[Rees algebra]\index{Rees algebra}\index{filtered algebra!Rees algebra}\index{algebra!Rees algebra}
  Let $A$ be a filtered algebra with filtration $\{A_i\}$. Then the \emph{Rees algebra} $\Rees(A)$ is the subalgebra $\bigoplus A_i T^i$ of the Laurent polynomial algebra $A[T, T^{-1}]$ (where $T$ commutes with $A$).
\end{defi}

Since $1 \in A_0 \subseteq A_1$, we know $T \in \Rees(A)$. The key observation is that
\begin{itemize}
  \item $\Rees(A)/(T) \cong \gr A$.
  \item $\Rees(A)/(1 - T) \cong A$.
\end{itemize}

Since $A_n(k)$ and $\mathcal{U}(\mathfrak{g})$ are finitely-generated algebras, they come with a natural filtering induced by the generating set. It turns out, in both cases, the associated graded algebras are pretty simple.
\begin{eg}
  Let $A = A_n(k)$, with generating set $X_1, \cdots, X_n$ and $Y_1, \cdots, Y_n$. We take the filtration as for a finitely-generated algebra. Now observe that if $a_i \in A_i$, and $a_j \in A_j$, then
  \[
    a_i a_j - a_j a_i \in A_{i + j - 2}.
  \]
  So we see that $\gr A$ is commutative, and in fact
  \[
    \gr A_n(k) \cong k[\bar{X}_1, \cdots, \bar{X}_n, \bar{Y}_1, \cdots, \bar{Y}_n],
  \]
  where $\bar{X}_i$, $\bar{Y}_i$ are the images of $X_i$ and $Y_i$ in $A_1/A_0$ respectively. This is not hard to prove, but is rather messy. It requires a careful induction.
\end{eg}

\begin{eg}
  Let $\mathfrak{g}$ be a Lie algebra, and consider $A = \mathcal{U}(\mathfrak{g})$. This has generating set $x_1, \cdots, x_n$, which is a vector space basis for $\mathfrak{g}$. Again using the filtration for finitely-generated algebras, we get that if $a_i \in A_i$ and $a_j \in A_j$, then
  \[
    a_i a_j - a_j a_i \in A_{i + j - 1}.
  \]
  So again $\gr A$ is commutative. In fact, we have
  \[
    \gr A \cong k[\bar{x}_1, \cdots, \bar{x}_n].
  \]
  The fact that this is a polynomial algebra amounts to the same as the \term{Poincar\'e-Birkhoff-Witt theorem}. This gives a $k$-vector space basis for $\mathcal{U}(\mathfrak{g})$.
\end{eg}

In both cases, we find that $\gr A$ are finitely-generated and commutative, and therefore Noetherian. We want to use this fact to deduce something about $A$ itself.

\begin{lemma}
  Let $A$ be a positively filtered algebra. If $\gr A$ is Noetherian, then $A$ is left Noetherian.
\end{lemma}
By duality, we know that $A$ is also right Noetherian.

\begin{proof}
  Given a left ideal $I$ of $A$, we can form
  \[
    \gr I = \bigoplus \frac{I \cap A_i}{I \cap A_{i - 1}},
  \]
  where $I$ is filtered by $\{I \cap A_i\}$. By the isomorphism theorem, we know
  \[
    \frac{I \cap A_i}{I \cap A_{i - 1}} \cong \frac{I \cap A_i + A_{i - 1}}{A_{i - 1}} \subseteq \frac{A_i}{A_{i - 1}}.
  \]
  Then $\gr I$ is a left graded ideal of $\gr A$.

  Now suppose we have a strictly ascending chain
  \[
    I_1 < I_2 < \cdots
  \]
  of left ideals. Since we have a positive filtration, for some $A_i$, we have $I_1 \cap A_i \subsetneq I_2 \cap A_i$ and $I_1 \cap A_{i - 1} = I_2 \cap A_{i - 1}$. Thus
  \[
    \gr I_1 \subsetneq \gr I_2 \subsetneq \gr I_3 \subsetneq \cdots.
  \]
  This is a contradiction since $\gr A$ is Noetherian. So $A$ must be Noetherian.
\end{proof}
Where we need positivity is the existence of that transition from equality to non-equality. If we have a $\Z$-filtered algebra instead, then we need to impose some completeness assumption, but we will not go into that.

\begin{cor}
  $A_n(k)$ and $\mathcal{U}(\mathfrak{g})$ are left/right Noetherian.
\end{cor}

\begin{proof}
  $\gr A_n(k)$ and $\gr \mathcal{U}(\mathfrak{g})$ are commutative and finitely generated algebras.
\end{proof}

Note that there is an alternative filtration for $A_n(k)$ yielding a commutative associated graded algebra, by setting $A_0 = k[X_1, \cdots, X_n]$ and
\[
  A_1 = k[X_1, \cdots, X_n] + \sum_{j = 1}^n k[X_1, \cdots, X_n] Y_j,
\]
ie. linear terms in the $Y$, and then keep on going. Essentially, we are filtering on the degrees of the $Y_i$ only. This also gives a polynomial algebra as an associative graded algebra. The main difference is that when we take the commutator, we don't go down by two degrees, but one only. Later, we will see this is advantageous when we want to get a Poisson bracket on the associated graded algebra.

We can look at further examples of Noetherian algebras.
\begin{eg}
  The \term{quantum plane} \term{$k_q[X, Y]$} has generators $X$ and $Y$, with relation
  \[
    XY = q YX
  \]
  for some $q \in k^\times$. This thing behaves differently depending on whether $q$ is a root of unity or not.
\end{eg}
This quantum plane first appeared in mathematical physics.
\begin{eg}
  The \term{quantum torus} \term{$k_q[X, X^{-1}, Y, Y^{-1}]$} has generators $X$, $X^{-1}$, $Y$, $Y^{-1}$ with relations
  \[
    XX^{-1} = YY^{-1} = 1,\quad XY = q YX.
  \]
\end{eg}
The word ``quantum'' in this context is usually thrown around a lot, and doesn't really mean much apart from non-commutativity, and there is very little connection with actual physics.

These algebras are both left and right Noetherian. We cannot prove these by filtering, as we just did. We will need a version of Hilbert's basis theorem which allows twisting of the coefficients. This is left as an exercise on the second example sheet.

\subsection{More on \texorpdfstring{$A_n(k)$}{An(k)} and \texorpdfstring{$\mathcal{U}(\mathfrak{g})$}{U(g)}}
We had filtrations yielding commutative polynomial algebras as their associated graded algebras. These filtrations have corresponding Rees algebras, and we saw that $\Rees A / (T) \cong \gr A$. If we know that $\gr A$ is commutative, then we know that
\[
  [\Rees A, \Rees A] \subseteq (T).
\]
This induces a map $\Rees (A) \times \Rees (A) \to (T)/(T^2)$, sending $(r, s) \mapsto T^2 + [r, s]$.

We also have a map
\[
  \begin{tikzcd}[column sep=large]
    \gr A \cong \displaystyle\frac{\Rees(A)}{(T)} \ar[r, "\text{mult. by $T$}"] & \displaystyle\frac{(T)}{(T^2)}
  \end{tikzcd},
\]
which is an isomorphism of $\gr A \cong \Rees A/(T)$-modules. We then have a bracket
\[
  \begin{tikzcd}[cdmap]
    \{\ph, \ph\}: \gr A \times \gr A \ar[r] & \gr A\\
    (\bar{r}, \bar{s}) \ar[r, maps to] & \{r, s\}
  \end{tikzcd}.
\]
Note that in our original filtration of the Weil algebra $A_n(k)$, since the commutator brings us down by two degrees, this bracket vanishes identically, but using the alternative filtration does not give a non-zero $\{\ph, \ph\}$.

This $\{\ph, \ph\}$ is an example of a \emph{Poisson bracket}.

\begin{defi}[Poisson algebra]\index{Poisson algebra}
  An associative algebra $B$ is a \emph{Poisson algebra} if there is a $k$-bilinear bracket $\{\ph, \ph\}: B \times B \to B$ such that
  \begin{itemize}
    \item $B$ is a Lie algebra under $\{\ph, \ph\}$, ie.
      \[
        \{r, s\} = - \{s, r\}
      \]
      and
      \[
        \{\{r, s\}, t\} + \{\{s, t\}, r\} + \{\{t, r\}, s\} = 0.
      \]
    \item We have the \term{Leibnitz rule}
      \[
        \{r, st\} = s\{r, t\} + \{r, s\} t.
      \]
  \end{itemize}
\end{defi}
The second condition says $\{r, \ph\}: B \to B$ is a derivation. Here we get $B = \gr A$, which is a polynomial algebra, in our two cases.

Our goal now is to study modules of $A_n(k)$.
\begin{lemma}
  Suppose $\Char k = 0$. Then $A_n(k)$ has no non-zero modules that are finite-dimensional $k$-vector spaces.
\end{lemma}

\begin{proof}
  Suppose $M$ is a finite-dimensional module. Then we've got an algebra homomorphism $\theta: A_n(k) \to \End_k(M) \cong M_m(k)$, where $m = \dim_k M$.

  In $A_n(k)$, we have
  \[
    Y_1 X_1 - X_1 Y_1 = 1.
  \]
  Applying the trace map, we know
  \[
    \tr(\theta(Y_1) \theta(X_1) - \theta(X_1) \theta(Y_1)) = \tr I = m.
  \]
  But since the trace is cyclic, the left hand side vanishes. So $m = 0$. So $M$ is trivial.
\end{proof}
A similar argument works for the quantum torus, but using determinants instead.

We're going to make use of our associated graded algebras from last time, which are isomorphic to polynomial algebras. Given a filtration $\{A_i\}$ of $A$, we may filter a module with generating set $\mathcal{S}$ by setting
\[
  M_i = A_i \mathcal{S}.
\]
Note that
\[
  A_j M_i \subseteq M_{i + j},
\]
which allows us to form an \term{associated graded module}\index{$\gr M$}
\[
  \gr M = \bigoplus \frac{M_i}{M_{i + 1}}.
\]
This is a graded $\gr A$-module, which is finitely-generated if $M$ is. So we've got a finitely-generated graded module over a graded commutative algebra.

To understand this further, we prove some results about graded modules over commutative algebras, which is going to apply to our $\gr A$ and $\gr M$.
\begin{defi}[Poincar\'e series]\index{Poincar\'e series}
  Let $V$ be a graded module over a graded algebra $S$, say
  \[
    V = \bigoplus_{i = 0}^\infty V_i.
  \]
  Then the \emph{Poincar\'e series} is
  \[
    P(V, t) = \sum_{i = 0}^\infty (\dim V_i) t^i.
  \]
\end{defi}

\begin{thm}[Hilbert-Serre theorem]\index{Hilbert-Serre theorem}
  The Poincar\'e series $P(V, t)$ of a finitely-generated graded module
  \[
    V = \bigoplus_{i = 0}^\infty V_i
  \]
   over a finitely-generated generated commutative algebra
  \[
    S = \bigoplus_{i = 0}^\infty S_i
  \]
  with homogeneous generating set $x_1, \cdots, x_m$ is a rational function of the form
  \[
    \frac{f(t)}{\prod(1 - t^{k_i})},
  \]
  where $f(t) \in \Z[t]$ and $k_i$ is the degree of the generator $x_i$.
\end{thm}

\begin{proof}
  We induct on the number $m$ of generators. If $m = 0$, then $S = S_0 = k$, and $V$ is therefore a finite-dimensional $k$-vector space. So $P(V, t)$ is a polynomial.

  Now suppose $m > 0$. We assume the theorem is true for $<m$ generators. Consider multiplication by $x_m$. This gives a map
  \[
    \begin{tikzcd}
      V_i \ar[r, "x_m"] & V_{i + k_m}
    \end{tikzcd},
  \]
  and we have an exact sequence
  \[
    \begin{tikzcd}
      0 \ar[r] & K_i \ar[r] & V_i \ar[r, "x_m"] & V_{i + k_m}\ar [r] & L_{i + k_m} \to 0,
    \end{tikzcd}\tag{$*$}
  \]
  where
  \[
    K = \bigoplus K_i = \ker (x_m: V \to V)
  \]
  and
  \[
    L = \bigoplus L_{i + k_m} = \coker (x_m: V \to V).
  \]
  Then $K$ is a graded submodule of $V$ and hence is a finitely-generated $S$-module, using the fact that $S$ is Noetherian. Also, $L = V/x_m V$ is a quotient of $V$, and it is thus also finitely-generated.

  Now both $K$ and $L$ are annihilated by $x_m$. So they may be regarded as $S_0[x_1, \cdots, x_{m - 1}]$-modules. Applying $\dim_k$ to $(*)$, we know
  \[
    \dim_k (K_i) - \dim_k (V_i) + \dim (V_{i + k_m}) - \dim (L_{i + k_m}) = 0.
  \]
  We multiply by $t^{i + k_m}$, and sum over $i$ to get
  \[
    t^{k_m} P(K, t) - t^{k_m} P(V, t) + P(V, t) - P(L, t) = g(t),
  \]
  where $g(t)$ is a polynomial with integral polynomial coefficients arising from consideration of the first few terms.

  We now apply the induction hypothesis to $K$ and $L$, and we are done.
\end{proof}

\begin{cor}
  If each $k_1, \cdots, k_m = 1$, ie. $S$ is generated by $S_0 = k$ and homogeneous elements $x_1, \cdots, x_m$ of degree $1$, then for large enough $i$, then $\dim V_i = \phi(i)$ for some polynomial $\phi(t) \in \Q[t]$ of $d - 1$, where $d$ is the order of the pole of $P(V, t)$ at $t = 1$. Moreover,
  \[
    \sum_{j = 0}^i \dim V_j = \chi(i),
  \]
  where $\chi(t) \in \Q[t]$ of degree $d$.
\end{cor}

\begin{proof}
  From the theorem, we know that
  \[
    P(V, t) = \frac{f(t)}{(1 - t)^d},
  \]
  for some $d$ with $f(1) \not= 0$, $f \in \Z[t]$. But
  \[
    (1 - t)^{-1} = 1 + t + t^2 + \cdots
  \]
  By differentiating, we get an expression
  \[
    (1 - t)^{-d} = \sum \binom{d + i - 1}{d - 1} t^i.
  \]
  If
  \[
    f(t) = a_0 + a_1 t + \cdots + a_s t^s,
  \]
  then we get
  \[
    \dim V_i = a_0 \binom{d + i - 1}{d - 1} + a_1 \binom{d + i - 2}{d - 1} + \cdots + a_s \binom{d + i - s- 1}{d - 1},
  \]
  where we set $\binom{r}{d - 1} = 0$ if $r < d - 1$, and this expression can be rearranged to give $\phi(i)$ for a polynomial $\phi(t) \in \Q[t]$, valid for $i - s > 0$. In fact, we have
  \[
    \phi(t) = \frac{f(1)}{(d - 1)!} t^{d - 1} + \text{lower degree term}.
  \]
  Since $f(1) \not= 0$, this has degree $d - 1$.

  This implies that
  \[
    \sum_{j = 0}^i \dim V_j
  \]
  is a polynomial in $\Q[t]$ of degree $d$.
\end{proof}
This $\phi(t)$ is the \term{Hilbert polynomial}, and $\chi(t)$ the \term{Samuel polynomial}. Some people call $\chi(t)$ the Hilbert polynomial instead, though.

We now want to apply this to our cases of $\gr A$, where $A = A_n(k)$ or $\mathcal{U}(\mathfrak{g})$, filtered as before. Then we deduce that
\[
  \sum_0^i \dim \frac{M_j}{M_{j - 1}} = \chi(i),
\]
for a polynomial $\chi(t) \in \Q[t]$. But we also know
\[
  \sum_{j = 0}^i \dim \frac{M_j}{M_{j - 1}} = \dim M_i.
\]
We are now in a position to make a definition.
\begin{defi}[Gelfand-Kirillov dimension]\index{Gelfand-Kirillov dimension}\index{$\GKdim(M)$}\index{$d(M)$}
  Let $A = A_n(k)$ or $\mathcal{U}(\mathfrak{g})$ and $M$ a finitely-generated $A$-module, filtered as before. Then the \emph{Gelfand-Kirillov dimension} of $M$ is the degree of the Samuel polynomial of $\gr M$ as a $\gr A$-module.
\end{defi}
This makes sense because $\gr A$ is a commutative algebra in this case. \emph{A priori}, it seems like this depends on our choice of filtering on $M$, but actually, it doesn't. For a more general algebra, we can define the dimension as below:

\begin{defi}[Gelfand-Kirillov dimension]\index{Gelfand-Kirillov dimension}\index{$\GKdim(M)$}\index{$d(M)$}
  Let $A$ be a finitely-generated $k$-algebra, which is filtered as before, and a finitely-generated $A$-module $M$, filtered as before. Then the GK-dimension of $M$ is
  \[
    \limsup_{n \to \infty} \frac{\log (\dim M_n)}{\log n}.
  \]
\end{defi}
In the case of $A = A_n(k)$ or $\mathcal{U}(\mathfrak{g})$, then this matches the previous definition. Again, this does not actually depend on the choice of generating sets.

Recall we showed that no non-zero $A_n(k)$-module $M$ can have finite dimension as a $k$-vector space. So we know $d(M) > 0$. Also, we know that $d(M)$ is an integer for cases $A = A_n$ or $\mathcal{U}(\mathfrak{g})$, since it is the degree of a polynomial. However, for general $M = A$, we can get non-integral values. In fact, the values we can get are $0, 1, 2$, and then any real number $\geq 2$. We can also have $\infty$ if the $\limsup$ doesn't exist.

\begin{eg}
  If $A = kG$, then we have $\GKdim(kG) < \infty$ iff $G$ has a subgroup $H$ of finite index with $H$ embedding into the strictly upper triangular integral matrices, ie. matrices of the form
  \[
    \begin{pmatrix}
      1 & * & \cdots & *\\
      0 & 1 & \cdots & *\\
      \vdots & \vdots & \ddots & \vdots\\
      0 & 0 & \cdots & 1
    \end{pmatrix}.
  \]
  This is a theorem of Gromov, and is quite hard to prove.
\end{eg}

\begin{eg}
  We have $\GKdim(A)= 0$ iff $A$ is finite-dimensional as a $k$-vector space.

  We have
  \[
    \GKdim(k[x]) = 1,
  \]
  and in general
  \[
    \GKdim(k[X_1, \cdots, X_n]) = n.
  \]
  Indeed, we have
  \[
    \dim_k(\text{$m$th homogeneous component}) = \binom{m + n}{n}.
  \]
  So we have
  \[
    \chi(t) =
    \begin{pmatrix}
      t + n\\n
    \end{pmatrix}
  \]
  This is of degree $n$, with leading coefficient $\frac{1}{n!}$.
\end{eg}

We can make the following definition, which we will not use again:
\begin{defi}[Multiplicity]\index{multiplicity}
  Let $A$ be a commutative algebra, and $M$ an $A$-module. The \emph{multiplicity} of $M$ with $d(M) = d$ is
  \[
    d! \times \text{leading coefficient of $\chi(t)$}.
  \]
\end{defi}
On the second example sheet, we will see that the multiplicity is integral.

We continue looking at more dimensions.
\begin{eg}
  We have $d(A_n(k)) = 2n$, and $d(\mathcal{U}(\mathfrak{g})) = \dim_k \mathfrak{g}$. Here we are using the fact that the associated graded algebras are polynomial algebras.
\end{eg}

\begin{eg}
  We met $k[X_1, \cdots, X_n]$ as the ``canonical'' $A_n(k)$-module. The filtration of the module matches the one we used when thinking about the polynomial algebra as a module over itself. So we get
  \[
    d(k[X_1, \cdots, X_n]) = n.
  \]
\end{eg}

\begin{lemma}
  Let $M$ be a finitely-generated $A_n$-module. Then $d(M) \leq 2n$.
\end{lemma}

\begin{proof}
  Take generators $m_1, \cdots, m_s$ of $M$. Then there is a surjective filtered module homomorphism
  \[
    \begin{tikzcd}[cdmap]
      A_n \oplus \cdots \oplus A_n \ar[r] & M\\
      (a_1, \cdots, a_s) \ar[r, maps to] & \sum a_i m_i
    \end{tikzcd}
  \]
  It is easy to see that quotients can only reduce dimension, so
  \[
    \GKdim(M) \leq d(A_n \oplus \cdots \oplus A_n).
  \]
  But
  \[
    \chi_{A_n \oplus \cdots \oplus A_n} = s \chi_{A_n}
  \]
  has degree $2n$.
\end{proof}

More interestingly, we have the following result:
\begin{thm}[Bernstein's inequality]\index{Bernstein's inequality}
  Let $M$ be a non-zero finitely-generated $A_n(k)$-module, and $\Char k = 0$. Then
  \[
    d(M) \geq n.
  \]
\end{thm}

%\begin{cor}
%  If $M$ is an $A_n(k)$-module, and $\Char k = 0$, then 
%  \[
%    d(M) \geq n.
%  \]
%\end{cor}
%
\begin{defi}[Holonomic module]\index{holonomic module}
  An $A_n(k)$ module $M$ is \emph{holonomic} iff $d(M) = n$.
\end{defi}
If we have a holonomic module, then we can quotient by a maximal submodule, and get a simple holonomic module. For a long time, people thought all simple modules are holonomic, until someone discovered a simple module that is not holonomic. In fact, most simple modules are not holonomic, but we something managed to believe otherwise.

\begin{proof}
  Take a generating set and form the canonical filtrations $\{A_i\}$ of $A_n(k)$ and $\{M_i\}$ of $M$. We let $\chi(t)$ be the Samuel polynomial. Then for large enough $i$, we have
  \[
    \chi(i) = \dim M_i.
  \]
  We claim that
  \[
    \dim A_i \leq \dim \Hom_k(M_i, M_{2i}) = \dim M_i \times \dim M_{2i}.
  \]
  Assuming this, for large enough $i$, we have
  \[
    \dim A_i \leq \chi(i) \chi(2i).
  \]
  But we know
  \[
    \dim A_i = \binom{i + 2}{2n},
  \]
  which is a polynomial of degree $2n$. But $\chi(t) \chi(2t)$ is a polynomial of degree $2 d(M)$. So we get that
  \[
    n \leq d(M).
  \]
  So it remains to prove the claim. It suffices to prove that the natural map
  \[
    A_i \to \Hom_k (M_i, M_{2i}),
  \]
  given by multiplication is injective.

  So we want to show that if $a \in A_i \not= 0$, then $a M_i \not= 0$. We prove this by induction on $i$. When $i = 0$, then $A_0 = k$, and $M_0$ is a finite-dimensional $k$-vector space. Then the result is obvious.

  If $i > 0$, we suppose the result is true for smaller $i$. We let $a \in A_i$ is non-zero. If $a M_i = 0$, then certainly $a \not\in k$. We express
  \[
    a = \sum c_{\boldsymbol\alpha\boldsymbol\beta} X_1^{\alpha_1} X_2^{\alpha_2} \cdots X_n^{\alpha_n} Y_1^{\beta_1} \cdots Y_n^{\beta_n},
  \]
  where $\boldsymbol\alpha = (\alpha_1, \cdots, \alpha_n)$, $\boldsymbol\beta = (\beta_1, \cdots, \beta_n)$, and $c_{\boldsymbol\alpha, \boldsymbol\beta} \in k$.

  If possible, pick a $j$ such that $c_{\boldsymbol\alpha, \boldsymbol\alpha} \not= 0$ for some $\boldsymbol\alpha$ with $\alpha_j \not= 0$ (this happens when there is an $X$ involved). Then
  \[
    [Y_j, a] = \sum \alpha_j c_{\boldsymbol\alpha, \boldsymbol\beta} X_1^{\alpha_1} \cdots X_j^{\alpha_j - 1} \cdots X_n^{\alpha_n} Y_1^{\beta_1} \cdots Y_n^{\beta_n},
  \]
  and this is non-zero, and lives in $A_{i - 1}$.

  If $a M_i = 0$, then certainly $a M_{i - 1} = 0$. Hence
  \[
    [Y_j, a] M_{i - 1} = (Y_j a - a Y_j) M_{i - 1} = 0,
  \]
  using the fact that $Y_j M_{i - 1} \subseteq M_i$. This is a contradiction.

  If $a$ only has $Y$'s involved, then we do something similar using $[X_j, a]$.
\end{proof}
There is also a geometric way of doing this.

We take $k = \C$. We know $\gr A_n$ is a polynomial algebra
\[
  \gr A_n = k[\bar{X}_1, \cdots, \bar{X}_n, \bar{Y}_1, \cdots, \bar{Y}_n],
\]
which may be viewed as the coordinate algebra of the cotangent bundle on affine $n$-space $\C^n$. The points of this correspond to the maximal ideals of $\gr A_n$. If $I$ is a left ideal of $A_n(\C)$, then we can form $\gr I$ and we can consider the set of maximal ideals containing it. This gives us the \term{characteristic variety} $\mathrm{Ch}(A_n/I)$.

We saw that there was a Poisson bracket on $\gr A_n$, and this may be used to define a skew-symmetric form on the tangent space at any point of the cotangent bundle. In this case, this is non-degenerate skew-symmetric form.

We can consider the tangent space $U$ of $\mathrm{Ch} (A_n/I)$ at a non-singular point, and there's a theorem of Gabber (1981) which says that $U \supseteq U^\perp$, where $\perp$ is with respect to the skew-symmetric form. By non-degeneracy, we must have $\dim U \geq n$, and we also know that
\[
  \dim \mathrm{Ch}(A_n/I) = d(A_n/I).
\]
So we find that $d(A_n/I) \geq n$.

In the case of $A = U(\mathfrak{g})$, we can think of $\gr A$ as the coordinate algebra on $\mathfrak{g}^*$, the vector space dual of $\mathfrak{g}$. The Poisson bracket leads to a skew-symmetric form on tangent spaces at points of $\mathfrak{g}^*$. In this case, we don't necessarily get non-degeneracy. However, on $\mathfrak{g}$, we have the adjoint action of the corresponding Lie group $G$, and this induces a co-adjoint action on $\mathfrak{g}^*$. Thus $\mathfrak{g}^*$ is a disjoint union of orbits. If we consider the induced skew-symmetric form on tangent spaces of orbits (at non-singular points), then it is non-degenerate.

\subsection{Injective modules}
We are now going to start talking about Goldie's theorem. This is going to involve talking about injective modules, which are dual to the notion of projective modules.
\begin{defi}[Injective module]\index{injective module}\index{module!injective}
  An $A$-module $E$ is \emph{injective} if for every diagram of $A$-module maps
  \[
    \begin{tikzcd}
      0 \ar[r] & M \ar[r,  "\theta", tail] \ar[d, "\phi"] & N \ar[ld, "\psi", dashed]\\
      & E
    \end{tikzcd},
  \]
  such that $\theta$ is injective, there exists a map $\psi$ that makes the diagram commute. Equivalently, $\Hom(\ph, E)$ is an exact functor.
\end{defi}

\begin{eg}
  Take $A = k$.  Then all $k$-vector spaces are injective $k$-modules.
\end{eg}

\begin{eg}
  Take $A = k[X]$. Then $k(X)$ is an injective $k[X]$-module.
\end{eg}

\begin{lemma}
  Every direct summand of an injective module is injective.
\end{lemma}

\begin{proof}
  Exercise. % fill in
\end{proof}

\begin{lemma}
  Every $A$-module may be embedded in an injective module.
\end{lemma}
We say the category of $A$-modules has \term{enough injectives}. The dual result for projectives was immediate, as free modules are projective.

\begin{proof}
  Let $M$ be a right $A$-module. Then $\Hom_k(A, M)$ is a right $A$-module via
  \[
    (fa)(x) = f(ax).
  \]
  We claim that $\Hom_k(A, M)$ is an injective module. Suppose we have
  \[
    \begin{tikzcd}
      0 \ar[r] & M_1 \ar[r,  "\theta"] \ar[d, "\phi"] & N_1\\
      & \Hom_k(A, M)
    \end{tikzcd}
  \]
  We consider the $k$-module diagram
  \[
    \begin{tikzcd}
      0 \ar[r] & M_1 \ar[r,  "\theta"] \ar[d, "\alpha"] & N_1 \ar[ld, dashed, "\beta"]\\
      & M
    \end{tikzcd}
  \]
  where $\alpha(m_1) = \phi(m_1)(1)$. Since $M$ is injective as a $k$-module, we can find the $\beta$ such that $\alpha = \beta \theta$. We define $\psi: N_1 \to \Hom_k(A, M)$ by
  \[
    \psi(n_1)(x) = \beta(n_1 x).
  \]
  It is straightforward to check that this does the trick. Also, we have an embedding $M \hookrightarrow \Hom_k(A, M)$ by $m \mapsto (\phi_n: x \mapsto mx)$.
\end{proof}
The category theorist will write the proof in a line as
\[
  \Hom_A(\ph, \Hom_k(A, M)) \cong \Hom_k(\ph \otimes_A A , M) \cong \Hom_k(\ph, M),
\]
which is exact since $M$ is injective as a $k$-module.

Note that neither the construction of $\Hom_k(A, M)$, nor the proof that it is injective requires the right $A$-modules structure of $M$. All we need is that $M$ is an injective $k$-module.

\begin{lemma}
  An $A$-module is injective iff it is a direct summand of every extension of itself.
\end{lemma}

\begin{proof}
  Suppose $E$ is injective and $E'$ is an extension of $E$. Then we can form the diagram
  \[
    \begin{tikzcd}
      0 \ar[r] & E \ar[r] \ar[d, "\mathrm{id}"] & E' \ar[rl, "\psi"]\\
      & E
    \end{tikzcd},
  \]
  and then by injectivity, we can find $\psi$, and then
  \[
    E' = E \oplus \ker \psi.
  \]
  Conversely, suppose $E$ is a direct summand of every extension. But by the previous lemma, we can embed $E$ in an injective $E'$. This implies that $E$ is a direct summand of $E'$, and hence injective.
\end{proof}

There is some sort of ``smallest'' injective a module embeds into, and this is called the \emph{injective hull}, or \emph{injective envelope}. This is why our injectives are called $E$.

\begin{defi}[Essential submodule]\index{essential submodule}\index{submodule!essential}
  An \emph{essential submodule} $M$ of an $A$-module $N$ is one where $M \cap V \not= \{0\}$ for every non-zero submodule $V$ of $N$. We say $N$ is an \term{essential extension}\index{extension!essential} of $M$.
\end{defi}

\begin{lemma}
  A maximal essential extension is an injective module.
\end{lemma}
Such maximal things exist by Zorn's lemma.

\begin{proof}
  Let $E$ be a maximal essential extension of $M$, and consider any embedding $E \hookrightarrow F$. We shall show that $E$ is a direct summand of $F$, and by our previous lemma, we know $E$ is injective. Let $S$ be the set of all non-zero submodules $V$ of $F$ with $V \cap E = \{0\}$. We apply Zorn's lemma to get a maximal such module, say $V_1$.

  We can check that if $E + V_1 \not= F$, then we can find a larger essential extension of $M$. So we must have
  \[
    F = E \oplus V_1.
  \]
\end{proof}
We can now make the following definition:
\begin{defi}[Injective hull]\index{injective hull}\index{injective envelope}
  A maximal essential extension of $M$ is the \emph{injective hull} (or \emph{injective envelope}) of $M$, written $E(M)$.
\end{defi}

\begin{prop}
  Any two injective hulls of $M$ are isomorphic.
\end{prop}

\begin{proof}
  Exercise % exercise
\end{proof}

It is also not hard to see that
\begin{prop}
  \[
    E(M_1 \oplus M_2) = E(M_1) \oplus E(M_2).
  \]
\end{prop}

\begin{eg}
  Take $A = k[X]$, and $M = k[X]$. Then $E(M) = k(X)$.
\end{eg}
\printindex
\end{document}
