\documentclass[a4paper]{article}

\def\npart {II}
\def\nterm {Lent}
\def\nyear {2017}
\def\nlecturer {H. S. Reall}
\def\ncourse {Statistical Physics}

\input{header}

\begin{document}
\maketitle
{\small
  \noindent\emph{Part IB Quantum Mechanics and ``Multiparticle Systems'' from Part II Principles of Quantum Mechanics are essential}

  \vspace{10pt}
  \noindent\textbf{Fundamentals of statistical mechanics}\\
  Microcanonical ensemble. Entropy, temperature and pressure. Laws of thermodynamics. Example of paramagnetism. Boltzmann distribution and canonical ensemble. Partition function. Free energy. Specific heats. Chemical Potential. Grand Canonical Ensemble.\hspace*{\fill} [5]

  \vspace{10pt}
  \noindent\textbf{Classical gases}\\
  Density of states and the classical limit. Ideal gas. Maxwell distribution. Equipartition of energy. Diatomic gas. Interacting gases. Virial expansion. Van der Waal's equation of state. Basic kinetic theory.\hspace*{\fill} [3]

  \vspace{10pt}
  \noindent\textbf{Quantum gases}\\
  Density of states. Planck distribution and black body radiation. Debye model of phonons in solids. Bose-Einstein distribution. Ideal Bose gas and Bose-Einstein condensation. Fermi-Dirac distribution. Ideal Fermi gas. Pauli paramagnetism.\hspace*{\fill}[8]

  \vspace{10pt}
  \noindent\textbf{Thermodynamics}\\
  Thermodynamic temperature scale. Heat and work. Carnot cycle. Applications of laws of thermodynamics. Thermodynamic potentials. Maxwell relations.\hspace*{\fill} [4]

  \vspace{10pt}
  \noindent\textbf{Phase transitions}\\
  Liquid-gas transitions. Critical point and critical exponents. Ising model. Mean field theory. First and second order phase transitions. Symmetries and order parameters.\hspace*{\fill} [4]%
}

\setcounter{section}{-1}
\section{Introduction}
In previous physics courses, we mostly focused on ``microscopic'' physics. We looked at the interaction of atoms, electrons etc. We now want to look at more macroscopic things, when we have lots of atoms, and lots of molecules. The aim of the course is translate microscopic laws of physics into macroscopic laws of physics.

By ``macroscopic'', we mean a system that is made of many particles. For example, $\SI{12}{\gram}$ of Carbon-12 contains $6 \times 10^{23}$ atoms, known as a mole. This number is very big. This is the key ingredient we are going to talk about in this course. How do we go about doing that? We might want to write down the Schr\"odinger equation for all water particles in a bottle, and then see how it evolves in time, and then predict how water evolves. But this is, obviously, hopeless. Even if we have two atoms, it is very hard to solve the Schr\"odinger equation, let alone $10^{23}$. But we don't have to. We don't have to have a complete description of the molecules if we want to know how the water behaves. It doesn't matter if we swap around two water molecules. We just want to know about macroscopic properties of the substance.

For example, temperature is something we can ask about a system, and this is a property of a system. It doesn't make sense to ask the temperature of a particle.

Historically, one reason to study statistical physics is that we can use it to test our microscopic laws of matter. For example, this is how quantum mechanics was discovered. Nowadays, techniques in statistical physics have many applications. In addition to studying, say, condensed matter physics, they can be used to do many other things like studying black holes, or biology!

\section{Fundamentals of statistical mechanics}
\subsection{Microcanonical ensemble}
We are going to consider an isolated system containing $N$ particles, where $N$ is a Large Number\textsuperscript{TM}. For example, a gas in a box can be considered to be isolated. We are going to use the term \term{microstate} to describe the individual (quantum) state of the system. This gives a complete description of the system. As we would expect, this is very complicated. In statistical physics, we observe that many microstates are indistinguishable macroscopically. So we don't attempt to describe the system using the microstate. Instead, we use a probability distribution on microstates.

More precisely, we let $\{\bket{n}\}$ be a basis of normalized eigenstates, say
\[
  \hat{H}\bket{n} = E_n \bket{n}.
\]
We let $p(n)$ be the probability that the microstate is $\bket{n}$. We can then define the expectation value of an operator $\mathcal{O}$ by
\[
  \bra \mathcal{O}\ket = \sum_n p(n) \brak{n}\mathcal{O}\bket{n}.
\]
In other words, we assume the system is described by a mixed state with density operator
\[
  \rho = \sum_n p(n) \bket{n}\brak{n}.
\]
There is an equivalent way of looking at this. We can consider an \term{ensemble} consisting of $W \gg 1$ independent copies of our system such that $Wp(m)$ many copies are in the microstate $\bket{n}$. Then the expectation is just the average over the ensemble.

We will further assume our system is in \term{equilibrium}, ie. the probability distribution $p(n)$ does not change in time. So in particular $\bra O\ket$ is independent of time. Of course, this does not mean the particles stop moving. The particles are still whizzing around. It's just that the statistical distribution does not change. In this course, we will mostly be talking about equilibrium systems. When we get out of equilibrium, things become very complicated.

We may have some partial knowledge about the system. For example, we might know its total energy. The microstates that are compatible with this partial knowledge are called \emph{accessible}\index{accessible state}. We can now state our fundamental assumption of statistical mechanics:
\begin{center}
  \emph{An isolated system in equilibrium is equally likely to be in any of the accessible microstates.}
\end{center}
This is an assumption we make, and we will see that it works very well.

Thus, different probability distributions, or different ensembles, are distinguished by the partial knowledge we know.
\begin{defi}[Microcanonical ensemble]\index{microcanonical ensemble}
  In a \emph{microcanonical ensemble}, we know the energy is between $E$ and $E + \delta E$, where $\delta E$ Is the accuracy of our measuring device. The accessible microstates are those with energy $E \leq E_n \leq E + \delta E$. We let $\Omega(E)$ be the number of such states.
\end{defi}
In practice, $\delta E$ is much much larger than the spacing of energy levels, and so $\Omega(E) \gg 1$. This notation doesn't really do justice to how big $\Omega(E)$ is. We should really write something like $\Omega(E) \gg{}\gg{}\gg 1$.

Note that we are working with a quantum system here, so the possible systems is discrete, and so it makes sense to count the number of systems. We need to do more work if we want to do this classically.

\begin{eg}
  Suppose we have $N = 10^{23}$ particles, and each particle can occupy two states $\bket{\uparrow}$ and $\bket{\downarrow}$, which have the same energy $\varepsilon$. Then we always have $N\varepsilon$ total energy, and we have
  \[
    \Omega(N\varepsilon) = 2^{10^{23}}.
  \]
  This is a fantastically huge, mind-boggling number. This is the kind of number we are talking about.
\end{eg}

By the fundamental assumption, we have
\[
  p(n) =
  \begin{cases}
    \frac{1}{\Omega(E)} & \text{if } E \leq E_n \leq E + \delta E\\
    0 & \text{otherwise}
  \end{cases}
\]
This probability distribution defines what is known as as the microcanonical ensemble.

Since this $\Omega(E)$ is huge, we want something smaller, one that is linear in $N$. So we take the log

\begin{defi}[Boltzmann entropy]\index{Boltzmann entropy}\index{entropy}
  The \emph{(Boltzmann) entropy} is defined as
  \[
    S(E) = k \log \Omega(E),
  \]
  where $k = \SI{1.381e-23}{\joule\per\kelvin}$ is \term{Boltzmann's constant}\index{$k$}.
\end{defi}
This annoying constant $k$ is necessary because when people started doing thermodynamics, they didn't know about statistical physics, and picked weird conventions.

As we have seen previously, we expect $\Omega(E) \sim e^N$. So we would expect $S(E) \sim kN$, and this is why the entropy is more convenient.

The second nice property of the entropy is that it is additive --- if we have two \emph{non-interacting} systems with energies $E_{(1)}, E_{(2)}$. Then the total number of states of the combined system is
\[
  \Omega(E_{(1)}, E_{(2)}) = \Omega_{1}(E_{(1)})\Omega_{2} (E_{(2)}).
\]
So when we take the logarithm, we find
\[
  S(E_{(1)}, E_{(2)}) = S(E_{(1)}) + S(E_{(2)}).
\]
This is for systems that do not interact with each other.

Suppose we bring the two systems together, and let them exchange energy. Then the energy of the individual systems is no longer fixed, and only the total energy
\[
  E_{\mathrm{total}} = E_{(1)} + E_{(2)}
\]
is fixed. Then we find that
\[
  \Omega(E_{\mathrm{total}}) = \sum_{E_i} \Omega_1(E_i) \Omega_2(E_{\mathrm{total}} - E_i),
\]
where we sum over all possible energy levels of the first system. In terms of the entropy of the system, we have
\[
  \Omega(E_{\mathrm{total}}) = \sum_{E_i} \exp\left(\frac{S_1(E_i)}{k} + \frac{S_2(E_{\mathrm{total}} - E_i)}{k}\right)
\]
We know that $S_{1, 2}/k \sim N_{1, 2} \sim 10^{23}$, which is a ridiculously large number. So the sum is overwhelmingly dominated by the term with the largest exponent. Suppose this is maximized when $E_i = E_*$.

%We now want to figure out which $E_i$ maximizes this exponent. We suppose this is given by $E_i = E_*$. For the sake of this analysis, we suppose we have a continuum of energies, so that we can differentiate the expression. Then we need
%\[
% \frac{\d}{\d E_i} \left(S_1(E_i) + S_2(E_{\mathrm{total}} - E_i)\right) = 0.
%\]
%In other words, we need
%\[
% \left.\frac{\d S_1}{\d E}\right|_{E_{(1)} = E_*} - \left.\frac{\d S_i}{\d E} \right|_{E_{(2)} = E_{\mathrm{total}} - E_*} = 0
%\]
Then we have
\[
  S(E_{\mathrm{total}}) = k \log \Omega(E_{\mathrm{total}}) \approx S_1(E_*) + S_2(E_{\mathrm{total}} - E_*) \geq S_1(E_{(1)}) + S_2(E_{(2)}),
\]
where the last inequality comes from the fact that we defined $E_*$ to maximize the expression.

Note that $E_{(1)}$ is not fixed, but the probability that the first system has energy $E_{(1)}$ is
\[
  \frac{\Omega_1(E_{(1)})\Omega_2(E_{\mathrm{total}} - E_{(1)})}{\Omega(E_{\mathrm{total}})} = \exp\left(\frac{1}{k}\left(S_1(E_{(1)}) + S_2(E_{(2)}) - S(E_{\mathrm{total}})\right)\right).
\]
Since the $S_i$ are incredibly large numbers, this is overwhelmingly maximized for $E_{(1)} = E_*$, so that equality almost holds, and is negligible otherwise.


\tableofcontents
\end{document}
