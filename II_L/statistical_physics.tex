\documentclass[a4paper]{article}

\def\npart {II}
\def\nterm {Lent}
\def\nyear {2017}
\def\nlecturer {H. S. Reall}
\def\ncourse {Statistical Physics}

\input{header}

\begin{document}
\maketitle
{\small
  \noindent\emph{Part IB Quantum Mechanics and ``Multiparticle Systems'' from Part II Principles of Quantum Mechanics are essential}

  \vspace{10pt}
  \noindent\textbf{Fundamentals of statistical mechanics}\\
  Microcanonical ensemble. Entropy, temperature and pressure. Laws of thermodynamics. Example of paramagnetism. Boltzmann distribution and canonical ensemble. Partition function. Free energy. Specific heats. Chemical Potential. Grand Canonical Ensemble.\hspace*{\fill} [5]

  \vspace{10pt}
  \noindent\textbf{Classical gases}\\
  Density of states and the classical limit. Ideal gas. Maxwell distribution. Equipartition of energy. Diatomic gas. Interacting gases. Virial expansion. Van der Waal's equation of state. Basic kinetic theory.\hspace*{\fill} [3]

  \vspace{10pt}
  \noindent\textbf{Quantum gases}\\
  Density of states. Planck distribution and black body radiation. Debye model of phonons in solids. Bose-Einstein distribution. Ideal Bose gas and Bose-Einstein condensation. Fermi-Dirac distribution. Ideal Fermi gas. Pauli paramagnetism.\hspace*{\fill}[8]

  \vspace{10pt}
  \noindent\textbf{Thermodynamics}\\
  Thermodynamic temperature scale. Heat and work. Carnot cycle. Applications of laws of thermodynamics. Thermodynamic potentials. Maxwell relations.\hspace*{\fill} [4]

  \vspace{10pt}
  \noindent\textbf{Phase transitions}\\
  Liquid-gas transitions. Critical point and critical exponents. Ising model. Mean field theory. First and second order phase transitions. Symmetries and order parameters.\hspace*{\fill} [4]%
}

\tableofcontents
\setcounter{section}{-1}
\section{Introduction}
In previous physics courses, we mostly focused on ``microscopic'' physics. We looked at the interaction of atoms, electrons etc. We now want to look at more macroscopic things, when we have lots of atoms, and lots of molecules. The aim of the course is translate microscopic laws of physics into macroscopic laws of physics.

By ``macroscopic'', we mean a system that is made of many particles. For example, $\SI{12}{\gram}$ of Carbon-12 contains $6 \times 10^{23}$ atoms, known as a mole. This number is very big. This is the key ingredient we are going to talk about in this course. How do we go about doing that? We might want to write down the Schr\"odinger equation for all water particles in a bottle, and then see how it evolves in time, and then predict how water evolves. But this is, obviously, hopeless. Even if we have two atoms, it is very hard to solve the Schr\"odinger equation, let alone $10^{23}$. But we don't have to. We don't have to have a complete description of the molecules if we want to know how the water behaves. It doesn't matter if we swap around two water molecules. We just want to know about macroscopic properties of the substance.

For example, temperature is something we can ask about a system, and this is a property of a system. It doesn't make sense to ask the temperature of a particle.

Historically, one reason to study statistical physics is that we can use it to test our microscopic laws of matter. For example, this is how quantum mechanics was discovered. Nowadays, techniques in statistical physics have many applications. In addition to studying, say, condensed matter physics, they can be used to do many other things like studying black holes, or biology!

\section{Fundamentals of statistical mechanics}
\subsection{Microcanonical ensemble}
The general setting in this section is that we have an isolated system containing $N$ particles, where $N$ is a Large Number\textsuperscript{TM}.

\begin{eg}
  A gas in a box can be considered to be an isolated system with many particles.
\end{eg}

\begin{defi}[Microstate]\index{microstate}
  The \emph{microstate} of a system is the actual (quantum) state of the system. This gives a complete description of the system.
\end{defi}

As one would expect, the microstate is very complicated and infeasible to describe, especially when we have \emph{many} particles. In statistical physics, we observe that many microstates are indistinguishable macroscopically. Thus, we only take note of some macroscopically interesting quantities, and use these macroscopic quantities to put a probability distribution on the microstates.

More precisely, we let $\{\bket{n}\}$ be a basis of normalized eigenstates, say
\[
  \hat{H}\bket{n} = E_n \bket{n}.
\]
We let $p(n)$ be the probability that the microstate is $\bket{n}$. Note that this probability is not the quantum probability we usually meet, but just something reflecting our ignorance.

Given such probabilities, we can define the expectation of an operator in the least imaginative way:
\begin{defi}[Expectation value]\index{expectation value}
  Given a probability distribution $p(n)$ on the states, the expectation value of an operator $\mathcal{O}$ is
  \[
    \bra \mathcal{O}\ket = \sum_n p(n) \brak{n}\mathcal{O}\bket{n}.
  \]
\end{defi}
If one knows about density operators, we can describe the system as a mixed state with density operator
\[
  \rho = \sum_n p(n) \bket{n}\brak{n}.
\]
There is an equivalent way of looking at this. We can consider an \term{ensemble} consisting of $W \gg 1$ independent copies of our system such that $Wp(m)$ many copies are in the microstate $\bket{n}$. Then the expectation is just the average over the ensemble.

We will further assume our system is in \term{equilibrium}, ie. the probability distribution $p(n)$ does not change in time. So in particular $\bra O\ket$ is independent of time. Of course, this does not mean the particles stop moving. The particles are still whizzing around. It's just that the statistical distribution does not change. In this course, we will mostly be talking about equilibrium systems. When we get out of equilibrium, things become very complicated.

The idea of statistical physics is that we have some partial knowledge about the system. For example, we might know its total energy. The microstates that are compatible with this partial knowledge are called \emph{accessible}\index{accessible state}. The \term{fundamental assumption of statistical mechanics} is then
\begin{significant}
  An isolated system in equilibrium is equally likely to be in any of the accessible microstates.
\end{significant}
This is an assumption we make, and we will see that it works very well.

Thus, different probability distributions, or different ensembles, are distinguished by the partial knowledge we know.
\begin{defi}[Microcanonical ensemble]\index{microcanonical ensemble}
  In a \emph{microcanonical ensemble}, we know the energy is between $E$ and $E + \delta E$, where $\delta E$ Is the accuracy of our measuring device. The accessible microstates are those with energy $E \leq E_n \leq E + \delta E$. We let $\Omega(E)$ be the number of such states.
\end{defi}
In practice, $\delta E$ is much much larger than the spacing of energy levels, and so $\Omega(E) \gg 1$.

Note that we are working with a quantum system here, so the possible systems is discrete, and so it makes sense to count the number of systems. We need to do more work if we want to do this classically.

\begin{eg}
  Suppose we have $N = 10^{23}$ particles, and each particle can occupy two states $\bket{\uparrow}$ and $\bket{\downarrow}$, which have the same energy $\varepsilon$. Then we always have $N\varepsilon$ total energy, and we have
  \[
    \Omega(N\varepsilon) = 2^{10^{23}}.
  \]
  This is a fantastically huge, mind-boggling number. This is the kind of number we are talking about.
\end{eg}

By the fundamental assumption, we have
\[
  p(n) =
  \begin{cases}
    \frac{1}{\Omega(E)} & \text{if } E \leq E_n \leq E + \delta E\\
    0 & \text{otherwise}
  \end{cases}
\]
This probability distribution defines what is known as as the microcanonical ensemble.

Since this $\Omega(E)$ is huge, we want something smaller, one that is linear in $N$. So we take the log

\begin{defi}[Boltzmann entropy]\index{Boltzmann entropy}\index{entropy}
  The \emph{(Boltzmann) entropy} is defined as
  \[
    S(E) = k \log \Omega(E),
  \]
  where $k = \SI{1.381e-23}{\joule\per\kelvin}$ is \term{Boltzmann's constant}\index{$k$}.
\end{defi}
This annoying constant $k$ is necessary because when people started doing thermodynamics, they didn't know about statistical physics, and picked weird conventions.

As we have seen previously, we expect $\Omega(E) \sim e^N$. So we would expect $S(E) \sim kN$, and this is why the entropy is more convenient.

Notice that we wrote our expressions as $S(E)$, instead of $S(E, \delta E)$. It turns out the value of $\delta E$ doesn't really matter. We know that $\Omega(E)$ will scale approximately linearly with $\delta E$. So if we, say, double $\delta E$, then $S(E)$ will increase by $k \log 2$, which is incredibly tiny compared to $S(E) = k \log \Omega(E)$. So it doesn't matter which value of $\delta E$ we pick.

Even if you are not so convinced that multiplying $10^{10^{23}}$ by a factor of $2$ or adding $\log 2$ to $10^{23}$ do not really matter, you should be reassured that at the end, we will rarely talk about $\Omega(E)$ or $S(E)$ itself. Instead, we will often divide two different $\Omega$'s to get probabilities, or differentiate $S$ to get other interesting quantities, and in these cases, those factors do not matter.


The second nice property of the entropy is that it is additive --- if we have two \emph{non-interacting} systems with energies $E_{(1)}, E_{(2)}$. Then the total number of states of the combined system is
\[
  \Omega(E_{(1)}, E_{(2)}) = \Omega_{1}(E_{(1)})\Omega_{2} (E_{(2)}).
\]
So when we take the logarithm, we find
\[
  S(E_{(1)}, E_{(2)}) = S(E_{(1)}) + S(E_{(2)}).
\]
This is for systems that do not interact with each other.

\subsubsection*{Interacting systems}
Suppose we bring the two systems together, and let them exchange energy. Then the energy of the individual systems is no longer fixed, and only the total energy
\[
  E_{\mathrm{total}} = E_{(1)} + E_{(2)}
\]
is fixed. Then we find that
\[
  \Omega(E_{\mathrm{total}}) = \sum_{E_i} \Omega_1(E_i) \Omega_2(E_{\mathrm{total}} - E_i),
\]
where we sum over all possible energy levels of the first system. In terms of the entropy of the system, we have
\[
  \Omega(E_{\mathrm{total}}) = \sum_{E_i} \exp\left(\frac{S_1(E_i)}{k} + \frac{S_2(E_{\mathrm{total}} - E_i)}{k}\right)
\]
We can be a bit more precise with what the sum means. We are not summing over all eigenstates. Recall that we have previously fixed an accuracy $\delta E$. So we can imagine dividing the whole energy spectrum into chunks of size $\delta E$, and here we are summing over the chunks.

We know that $S_{1, 2}/k \sim N_{1, 2} \sim 10^{23}$, which is a ridiculously large number. So the sum is overwhelmingly dominated by the term with the largest exponent. Suppose this is maximized when $E_i = E_*$. Then we have
\[
  S(E_{\mathrm{total}}) = k \log \Omega(E_{\mathrm{total}}) \approx S_1(E_*) + S_2(E_{\mathrm{total}} - E_*).
\]
Again, we are not claiming that only the factor coming from $E_*$ has significant contribution. Maybe one or two energy levels next to $E_*$ are also very significant, but taking these into account will only multiply $\Omega(E_{\mathrm{total}})$ by a (relatively) small constant, hence contributes a small additive factor to $S(E_{\mathrm{total}})$, which can be neglected.

Now given any $E_{(1)}$, what is the probability that the actual energy of the first system is $E_{(1)}$? For convenience, we write $E_{(2)} = E_{\mathrm{total}} = E_{(1)}$, the energy of the second system. Then the probability desired is
\[
  \frac{\Omega_1(E_{(1)})\Omega_2(E_{(2)})}{\Omega(E_{\mathrm{total}})} = \exp\left(\frac{1}{k}\left(S_1(E_{(1)}) + S_2(E_{(2)}) - S(E_{\mathrm{total}})\right)\right).
\]
Again recall that the numbers at stake are unimaginably huge. So if $S_1(E_{(1)}) + S_2(E_{(2)})$ is even slightly different from $S(E_{\mathrm{total}})$, then the probability is effectively zero. And by above, for the two quantities to be close, we need $E_{(1)} = E_*$. So for all practical purposes, the value of $E_{(1)}$ is fixed into $E_*$.

Now imagine we prepare two systems separately with energies $E_{(1)}$ and $E_{(2)}$ such that $E_{(1)} \not= E_*$, and then bring the system together, then we are no longer in equilibrium. $E_{(1)}$ will change until it takes value $E_*$, and then entropy of the system will increase from $S_1(E_{(1)}) + S_2(E_{(2)})$ to $S_1(E_*) + S_2(E_{\mathrm{total}} - E_*)$. In particular, the entropy increases.
\begin{law}[Second law of thermodynamics]\index{second law of thermodynamics}
  The entropy of an isolated system increases (or remains the same) in any physical process. In equilibrium, the entropy attains its maximum value.
\end{law}
This prediction is verified by virtually all observations of physics.

While our derivation did not show it is \emph{impossible} to violate the second law of thermodynamics, it is very very very very very very very very unlikely to be violated.

\subsubsection*{Temperature}
Having defined entropy, the next interesting thing we can define is the \emph{temperature}. We assume that $S$ is a smooth function in $E$. Then we can define the temperature as follows:
\begin{defi}[Temperature]\index{temperature}
  The \emph{temperature} is defined to be
  \[
    \frac{1}{T} = \frac{\d S}{\d E}.
  \]
\end{defi}
Why do we call this the temperature? Over the course, we will see that this quantity we decide to call ``temperature'' does behave as we would expect temperature to behave. Since there is no other ``standard'' definition of temperature we can compare this to, it would be difficult to give more direct justifications of this definition.

\begin{prop}
  Two interacting systems in equilibrium have the same temperature.
\end{prop}

\begin{proof}
  Recall that the equilibrium energy $E_*$ is found by maximizing
  \[
    S_1(E_i) + S_2(E_{\mathrm{total}} - E_i)
  \]
  over all possible $E_i$. Thus, at an equilibrium, the derivative of this expression has to vanish, and the derivative is exactly
  \[
    \left.\frac{\d S_1}{\d E}\right|_{E_{(1)} = E_*} - \left.\frac{\d S_i}{\d E} \right|_{E_{(2)} = E_{\mathrm{total}} - E_*} = 0
  \]
  So we need
  \[
    \frac{1}{T_1} = \frac{1}{T_2}.
  \]
  In other words, we need
  \[
    T_1 = T_2.
  \]
\end{proof}

Now suppose initially, our systems have different temperature. We would expect energy to flow from the hotter system to the cooler system. This is indeed the case.

\begin{prop}
  Suppose two systems with initial energies $E_{(1)}, E_{(2)}$ and temperatures $T_1, T_2$ are put into contact. If $T_1 > T_2$, then energy will flow form the first system to the second.
\end{prop}

\begin{proof}
  Since we are not in equilibrium, there must be some energy transfer from one system to the other. Suppose after time $\delta t$, the energy changes by
  \begin{align*}
    E_{(1)} & \mapsto E_{(1)} + \delta E\\
    E_{(2)} & \mapsto E_{(2)} - \delta E,
  \end{align*}
  keeping the total energy constant. Then the change in entropy is given by
  \[
    \delta S = \frac{\d S_1}{\d E} \delta E_{(1)} + \frac{\d S_2}{\d E} \delta E_{(2)} = \left(\frac{1}{T_1} - \frac{1}{T_2}\right) \delta E.
  \]
  By assumption, we know
  \[
    \frac{1}{T_1} - \frac{1}{T_2} < 0,
  \]
  but by the second law of thermodynamics, we know $\delta S$ must increase. So we must have $\delta E < 0$, ie. energy flows from the first system to the second.
\end{proof}
So this notion of temperature agrees with the basic properties of temperature we expect.

Note that, these properties we've derived only depends on the fact that $\frac{1}{T}$ is a monotonically decreasing function of $T$. In principle, we could have picked \emph{any} monotonically decreasing function of $T$, and set it to $\frac{\d S}{\d E}$. We will later see that this definition will agree with the other definitions of temperature we have previously seen, eg. via the ideal gas law, and so this is indeed the ``right'' one.

\subsubsection*{Heat capacity}
As we will keep on doing later, we can take different derivatives to get different interesting quantities. This time, we are going to get heat capacity. Recall that $T$ was a function of energy, $T = T(E)$. We will assume that we can invert this function, at least locally, to get $E$ as a function of $T$.
\begin{defi}[Heat capacity]\index{heat capacity}
  The \emph{heat capacity} of a system is
  \[
    C = \frac{\d E}{\d T}.
  \]
  The \term{specific heat capacity} is
  \[
    \frac{C}{\text{mass of system}}.
  \]
\end{defi}
The specific heat capacity is a property of the substance that makes up the system, and not how much stuff there is, as both $C$ and the mass scale linearly with the size of the system.

This is some quantity we can actually physically measure. We can measure the temperature with a thermometer, and it is usually not too difficult to see how much energy we pump into a system. Then by measuring the temperature change, we can figure out the heat capacity.

In doing so, we can indirectly measure the entropy, or at least the changes in entropy. Note that we have
\[
  \frac{\d S}{\d T} = \frac{\d S}{\d E} \frac{\d E}{\d T} = \frac{C}{T}.
\]
Integrating up, if the temperature changes from $T_1$ to $T_2$, we know
\[
  \Delta S = \int_{T_1}^{T_2} \frac{C(T)}{T}\;\d T.
\]
As promised, by measuring heat capacity experimentally, we can measure the change in entropy.

The heat capacity is useful in other ways. Recall that to find the equilibrium energy $E_*$, a necessary condition was that it satisfies
\[
  \frac{\d S_1}{\d E} - \frac{\d S_2}{\d E} = 0.
\]
However, we only know that the solution is an extrema, and not necessarily maximum. To figure out if it is the maximum, we take the second derivative. Note that for a single system, we have
\[
  \frac{\d^2 S}{\d E^2} = \frac{\d}{\d E} \left(\frac{1}{T}\right) = -\frac{1}{T^2 C}.
\]
Applying this to two systems, one can check that entropy is maximized at $E_{(1)} = E_*$ if $C_1, C_2 > 0$. The actual computations is left as an exercise on the first example sheet.

Let's look at some actual systems and try to calculate these quantities.
\begin{eg}
  Consider a $2$-state system, where we have $N$ non-interacting particles with fixed positions. Each particle is either in $\bket{\uparrow}$ or $\bket{\downarrow}$. We can think of these as spins, for example. These two states have different energies
  \[
    E_{\uparrow} = \varepsilon,\quad E_{\downarrow} = 0.
  \]
  We let $N_{\uparrow}$ and $N_{\downarrow}$ be the number of particles in $\bket{\uparrow}$ and $\bket{\downarrow}$ respectively. Then the total energy of the system is
  \[
    E = \varepsilon N_{\uparrow}.
  \]
  We want to calculate this quantity $\Omega(E)$. Here in this very contrived example, it is convenient to pick $\delta E < \varepsilon$, so that $\Omega(E)$ is just the number of ways of choosing $N_{\uparrow}$ particles from $N$. By basic combinatorics, we have
  \[
    \Omega(E) = \frac{N!}{N_{\uparrow}! (N - N_\uparrow)!},
  \]
  and
  \[
    S(E) = k \log \left(\frac{N!}{N_{\uparrow}! (N - N_\uparrow)!}\right).
  \]
  This is not an incredibly useful formula. Since we assumed that $N$ and $N_\uparrow$ are huge, we can use \term{Stirling's approximation}
  \[
    N! = \sqrt{2\pi N} N^N e^{-N} \left(1 + O\left(\frac{1}{N}\right)\right).
  \]
  Then we have
  \[
    \log N! = N \log N - N + \frac{1}{2}\log (2\pi N) + O\left(\frac{1}{N}\right).
  \]
  We just use the approximation three times to get
  \begin{align*}
    S(E) &= k\left(N \log N - N - N_\uparrow \log N_\uparrow + N_\uparrow - (N - N_\uparrow) \log(N - N_\uparrow) + N - N_\uparrow\right)\\
    &= -k \left((N - N_\uparrow) \log \left(\frac{N - N_\uparrow}{N}\right) + N_\uparrow \log\left(\frac{N_{\uparrow}}{N}\right)\right)\\
    &= -kN\left(\left(1 - \frac{E}{N\varepsilon}\right) \log\left(1 - \frac{E}{N\varepsilon}\right) + \frac{E}{N\varepsilon} \log\left(\frac{E}{N\varepsilon}\right)\right).
  \end{align*}
  This is better, but we can get much more out of it if we plot it:
  \begin{center}
    \begin{tikzpicture}
      \draw [->] (0, 0) -- (6, 0) node [right] {$E$};
      \draw [->] (0, 0) -- (0, 4) node [above] {$S(E)$};

      \draw [mblue, thick, domain=0.001:4.999] plot [smooth] (\x, {(-4.5) * ((1 - \x/5) * ln (1 - \x/5) + (\x / 5) * ln (\x / 5))});

      \node [left] at (0, 0) {$0$};
      \node [below] at (5, 0) {$N\varepsilon$};
      \node [below] at (2.5, 0) {$N\varepsilon/2$};
      \draw [dashed] (2.5, 0) -- (2.5, 3);

      \draw [dashed] (2.5, 3.119) -- (0, 3.119) node [left] {$Nk \log 2$};
    \end{tikzpicture}
  \end{center}
  The temperature is
  \[
    \frac{1}{T} = \frac{\d S}{\d T} = \frac{k}{\varepsilon} \log \left(\frac{N \varepsilon}{E} - 1\right),
  \]
  and we can invert to get
  \[
    \frac{N_{\uparrow}}{N} = \frac{E}{N\varepsilon} = \frac{1}{e^{\varepsilon /kT} + 1}.
  \]
  Suppose we get to control the temperature of the system, eg. if we put it with a heat reservoir. What happens as we vary our temperature?
  \begin{itemize}
    \item As $T \to 0$, we have $N_\uparrow \to 0$. So the states all try to go to the ground state.
    \item As $T \to \infty$, we find $N_\uparrow/N \to \frac{1}{2}$, and $E \to N\varepsilon/2$.
  \end{itemize}
  The second result is a bit weird. As $T \to \infty$, we might expect all things to go the maximum energy level, and not just half of them. What is happening?

  We can plot another graph, for $\frac{1}{T}$ vs $E$. The graph looks like
  \begin{center}
    \begin{tikzpicture}
      \draw [->] (0, 0) -- (6, 0) node [right] {$E$};
      \draw [->] (0, -2.5) -- (0, 2.5) node [above] {$\frac{1}{T}$};

      \draw [mblue, thick, domain=0.1:4.9] plot [smooth] (\x, {(-0.6) * (ln (\x) - ln (5 - \x))});

      \node [left] at (0, 0) {$0$};
      \node [below] at (5, 0) {$N\varepsilon$};
      \node [below] at (2.5, 0) {$N\varepsilon/2$};

    \end{tikzpicture}
  \end{center}
  We see that having energy $> N\varepsilon/2$ corresponds to negative temperature, and to go from positive temperature to negative temperature, we need to pass through infinite temperature. So in some sense, negative temperature is ``hotter'' than infinite temperature.

  What is going on? For negative $T$, we know $\Omega(E)$ is a decreasing function of energy. This is a very unusual situation. What we have in this system is that these particles are fixed, and have no kinetic energy. If we do have kinetic energy, then kinetic energy can be arbitrarily large. So there is no upper bound on $E$, and $\Omega(E)$ is usually an increasing function of $E$.

  Negative $T$ has indeed been observed experimentally. This requires setups where the kinetic energy is not so important in the range of energies we are talking about. One particular scenario where this is observed is in nuclear spins of crystals in magnetic fields. If we have a magnetic field, then naturally, most of the spins will align with the field. We now suddenly flip the field, and then most of the spins are anti-aligned, and this can give us a negative temperature state.

  Now we can't measure negative temperature by sticking a thermometer into the material and getting a negative answer. Something that \emph{can} be interestingly measured is the heat capacity
  \[
    C = \frac{\d E}{\d T} = \frac{N\varepsilon^2}{ kT^2 } \frac{e^{\varepsilon/kT}}{(e^{\varepsilon/kT} + 1)^2}.
  \]
  This again exhibits some peculiar properties. We begin by looking at a plot:
  \begin{center}
    \begin{tikzpicture}
      \draw [->] (0, 0) -- (6, 0) node [right] {$T$};
      \draw [->] (0, 0) -- (0, 4) node [above] {$C$};

      \draw [mblue, thick ] plot coordinates {(0.01,0.00000) (0.04,0.00000) (0.07,0.00102) (0.10,0.03632) (0.13,0.21581) (0.16,0.60094) (0.19,1.13589) (0.22,1.71794) (0.25,2.26083) (0.28,2.71418) (0.31,3.05901) (0.34,3.29686) (0.37,3.44009) (0.40,3.50519) (0.43,3.50895) (0.46,3.46653) (0.49,3.39067) (0.52,3.29165) (0.55,3.17751) (0.58,3.05435) (0.61,2.92675) (0.64,2.79804) (0.67,2.67060) (0.70,2.54609) (0.73,2.42564) (0.76,2.30996) (0.79,2.19945) (0.82,2.09433) (0.85,1.99462) (0.88,1.90026) (0.91,1.81111) (0.94,1.72699) (0.97,1.64766) (1.00,1.57290) (1.03,1.50244) (1.06,1.43606) (1.09,1.37350) (1.12,1.31453) (1.15,1.25893) (1.18,1.20647) (1.21,1.15697) (1.24,1.11022) (1.27,1.06605) (1.30,1.02429) (1.33,0.98478) (1.36,0.94739) (1.39,0.91196) (1.42,0.87839) (1.45,0.84654) (1.48,0.81631) (1.51,0.78760) (1.54,0.76031) (1.57,0.73436) (1.60,0.70966) (1.63,0.68614) (1.66,0.66373) (1.69,0.64237) (1.72,0.62198) (1.75,0.60252) (1.78,0.58393) (1.81,0.56617) (1.84,0.54918) (1.87,0.53292) (1.90,0.51735) (1.93,0.50244) (1.96,0.48815) (1.99,0.47445) (2.02,0.46130) (2.05,0.44868) (2.08,0.43656) (2.11,0.42492) (2.14,0.41372) (2.17,0.40295) (2.20,0.39259) (2.23,0.38262) (2.26,0.37302) (2.29,0.36376) (2.32,0.35484) (2.35,0.34624) (2.38,0.33795) (2.41,0.32994) (2.44,0.32221) (2.47,0.31475) (2.50,0.30753) (2.53,0.30056) (2.56,0.29382) (2.59,0.28731) (2.62,0.28100) (2.65,0.27490) (2.68,0.26899) (2.71,0.26326) (2.74,0.25772) (2.77,0.25235) (2.80,0.24714) (2.83,0.24209) (2.86,0.23719) (2.89,0.23243) (2.92,0.22782) (2.95,0.22334) (2.98,0.21899) (3.01,0.21477) (3.04,0.21066) (3.07,0.20667) (3.10,0.20280) (3.13,0.19902) (3.16,0.19536) (3.19,0.19179) (3.22,0.18832) (3.25,0.18494) (3.28,0.18165) (3.31,0.17844) (3.34,0.17532) (3.37,0.17228) (3.40,0.16932) (3.43,0.16644) (3.46,0.16362) (3.49,0.16088) (3.52,0.15820) (3.55,0.15559) (3.58,0.15305) (3.61,0.15056) (3.64,0.14814) (3.67,0.14577) (3.70,0.14346) (3.73,0.14120) (3.76,0.13899) (3.79,0.13684) (3.82,0.13474) (3.85,0.13268) (3.88,0.13067) (3.91,0.12870) (3.94,0.12678) (3.97,0.12490) (4.00,0.12307) (4.03,0.12127) (4.06,0.11951) (4.09,0.11779) (4.12,0.11611) (4.15,0.11446) (4.18,0.11284) (4.21,0.11126) (4.24,0.10972) (4.27,0.10820) (4.30,0.10672) (4.33,0.10526) (4.36,0.10384) (4.39,0.10244) (4.42,0.10107) (4.45,0.09973) (4.48,0.09842) (4.51,0.09713) (4.54,0.09587) (4.57,0.09463) (4.60,0.09341) (4.63,0.09222) (4.66,0.09105) (4.69,0.08990) (4.72,0.08877) (4.75,0.08767) (4.78,0.08658) (4.81,0.08552) (4.84,0.08447) (4.87,0.08345) (4.90,0.08244) (4.93,0.08145) (4.96,0.08047) (4.99,0.07952)};
      % map (\x -> (showFFloat (Just 2) x "", showFFloat (Just 5) (8 / (x*x) * exp (1/x) / (exp(1/x) + 1)^2) "")) [0.01,0.04..5]
      \node [left] at (0, 0) {$0$};

      \draw [dashed] (0.43, 0) node [below] {$T \sim \varepsilon/k$} -- (0.43, 3.50895);

    \end{tikzpicture}
  \end{center}
  Now the maximum $T$ is related to the microscopic $\varepsilon$. So we can use the macroscopic observation of $C$ to deduce something about the microscopic $\varepsilon$.

  Note that $C$ is proportional to $N$. As $T \to 0$, we have
  \[
    C \propto T^{-2} e^{-\varepsilon/kT},
  \]
  and this is a function that decreases very rapidly as $T \to 0$, and in fact this is one of the favorite examples in Analysis where all derivatives of the function at $0$ vanish. We will later see that this is due to the energy gap between the ground state and the first excited state.

  More interestingly, we see that the heat capacity vanishes at high temperature, but this is due to the peculiar property of the system at high temperature. In a general system, we expect the heat capacity to increase with temperature.

  How much of this is actually physical? The answer is ``not much'', as one might expect because we didn't really do much physics. For most solids, the contribution to $C$ from spins is swamped by other effects such as contributions of phonons (quantized vibrations in the solid) or electrons. In this case, $C(T)$ is monotonic in $T$.

  However, there are some very peculiar materials for which we obtain a small local maximum in $C(T)$ for very small $T$, before increasing monotonically, which is due to the contributions of spin:
  \begin{center}
    \begin{tikzpicture}
      \draw [->] (0, 0) -- (6, 0) node [right] {$T$};
      \draw [->] (0, 0) -- (0, 4) node [above] {$C$};

      \draw [mblue, thick ] plot coordinates {(0.01,0.00501) (0.04,0.02066) (0.07,0.07551) (0.10,0.21670) (0.13,0.36877) (0.16,0.47499) (0.19,0.53051) (0.22,0.54983) (0.25,0.54810) (0.28,0.53607) (0.31,0.52028) (0.34,0.50437) (0.37,0.49016) (0.40,0.47848) (0.43,0.46957) (0.46,0.46340) (0.49,0.45979) (0.52,0.45850) (0.55,0.45931) (0.58,0.46196) (0.61,0.46624) (0.64,0.47196) (0.67,0.47896) (0.70,0.48707) (0.73,0.49618) (0.76,0.50617) (0.79,0.51695) (0.82,0.52844) (0.85,0.54056) (0.88,0.55325) (0.91,0.56646) (0.94,0.58014) (0.97,0.59425) (1.00,0.60875) (1.03,0.62362) (1.06,0.63882) (1.09,0.65434) (1.12,0.67014) (1.15,0.68622) (1.18,0.70255) (1.21,0.71912) (1.24,0.73592) (1.27,0.75293) (1.30,0.77015) (1.33,0.78756) (1.36,0.80515) (1.39,0.82293) (1.42,0.84087) (1.45,0.85899) (1.48,0.87725) (1.51,0.89568) (1.54,0.91425) (1.57,0.93297) (1.60,0.95183) (1.63,0.97082) (1.66,0.98995) (1.69,1.00922) (1.72,1.02861) (1.75,1.04812) (1.78,1.06776) (1.81,1.08752) (1.84,1.10740) (1.87,1.12740) (1.90,1.14752) (1.93,1.16775) (1.96,1.18809) (1.99,1.20854) (2.02,1.22910) (2.05,1.24978) (2.08,1.27056) (2.11,1.29145) (2.14,1.31244) (2.17,1.33354) (2.20,1.35475) (2.23,1.37606) (2.26,1.39747) (2.29,1.41898) (2.32,1.44060) (2.35,1.46232) (2.38,1.48413) (2.41,1.50605) (2.44,1.52807) (2.47,1.55019) (2.50,1.57240) (2.53,1.59471) (2.56,1.61713) (2.59,1.63964) (2.62,1.66224) (2.65,1.68495) (2.68,1.70775) (2.71,1.73064) (2.74,1.75364) (2.77,1.77672) (2.80,1.79991) (2.83,1.82319) (2.86,1.84656) (2.89,1.87003) (2.92,1.89360) (2.95,1.91726) (2.98,1.94101) (3.01,1.96486) (3.04,1.98880) (3.07,2.01283) (3.10,2.03696) (3.13,2.06118) (3.16,2.08550) (3.19,2.10991) (3.22,2.13441) (3.25,2.15901) (3.28,2.18370) (3.31,2.20848) (3.34,2.23335) (3.37,2.25832) (3.40,2.28338) (3.43,2.30853) (3.46,2.33377) (3.49,2.35911) (3.52,2.38454) (3.55,2.41006) (3.58,2.43567) (3.61,2.46138) (3.64,2.48717) (3.67,2.51306) (3.70,2.53904) (3.73,2.56512) (3.76,2.59128) (3.79,2.61754) (3.82,2.64388) (3.85,2.67032) (3.88,2.69685) (3.91,2.72348) (3.94,2.75019) (3.97,2.77699) (4.00,2.80389) (4.03,2.83088) (4.06,2.85796) (4.09,2.88513) (4.12,2.91239) (4.15,2.93974) (4.18,2.96718) (4.21,2.99472) (4.24,3.02234) (4.27,3.05006) (4.30,3.07787) (4.33,3.10577) (4.36,3.13376) (4.39,3.16184) (4.42,3.19001) (4.45,3.21827) (4.48,3.24662) (4.51,3.27507) (4.54,3.30360) (4.57,3.33223) (4.60,3.36094) (4.63,3.38975) (4.66,3.41865) (4.69,3.44764) (4.72,3.47672) (4.75,3.50589) (4.78,3.53515) (4.81,3.56450) (4.84,3.59394) (4.87,3.62347) (4.90,3.65310) (4.93,3.68281) (4.96,3.71261) (4.99,3.74251)};
      % map (\x -> (showFFloat (Just 2) x "", showFFloat (Just 5) (1 / (4*x*x) * exp (1/(2*x)) / (exp(1/(2*x)) + 1)^2 + x/2 + x^2/20) "")) [0.01,0.04..5]

      \node [left] at (0, 0) {$0$};
    \end{tikzpicture}
  \end{center}

\end{eg}

\subsection{Pressure, volume and the first law of thermodynamics}
Many systems have other external parameters which can be varied. Consider a system whose total volume $V$ can vary, such as gas in a box with movable walls. As we change the volume, the allowed energies eigenstates will change. So no w$\Omega$, and hence $S$ are functions of energy \emph{and} volume:
\[
  S(E, V) = k \log \Omega(E, V).
\]
We now need to modify our definition of temperature to account for this dependence:
\begin{defi}[Temperature]\index{temperature}
  The \emph{temperature} of a system with variable volume is
  \[
    \frac{1}{T} = \left(\frac{\partial S}{\partial E}\right)_V,
  \]
  with $V$ fixed.
\end{defi}

But now we can define a different thermodynamic quantity by taking the derivative with respect to $V$.

\begin{defi}[Pressure]\index{pressure}
  We define the \emph{pressure} of a system with variable volume to be
  \[
    p = T \left(\frac{\partial S}{\partial V}\right)_E.
  \]
\end{defi}
Is this thing we call the ``pressure'' any thing like what we used to think of as pressure, namely force per unit area? We will soon see that this is indeed the case.


We begin by deducing some familiar properties of pressure.
\begin{prop}
  Consider as before two interacting systems where the total volume $V = V_1 + V_2$ is fixed by the individual volumes can vary. Then the entropy of the combined system is maximized when $T_1 = T_2$ and $p_1 = p_2$.
\end{prop}

\begin{proof}
  We have previously seen that we need $T_1 = T_2$. We also want
  \[
    \left(\frac{\d S}{\d V}\right)_E = 0.
  \]
  So we need
  \[
    \left(\frac{\d S_1}{\d V}\right)_E = \left(\frac{\d S_2}{\d V}\right)_E.
  \]
  Since the temperatures are equal, we know that we also need $p_1 = p_2$.
\end{proof}

For a single system, we can use the chain rule to write
\[
  \d S = \left(\frac{\partial S}{\partial E}\right)_V \d E + \left(\frac{\partial S}{\partial V}\right)_E \d V.
\]
Then we can use the definitions of temperature and pressure to write
\begin{prop}[First law of thermodynamics]\index{first law of thermodynamics}
  \[
    \d E = T \d S - p \d V.
  \]
\end{prop}
This law relates two infinitesimally close equilibrium states. This is sometimes called the \term{fundamental thermodynamics relation}.

\begin{eg}
  Consider a box with one side a movable piston of area $A$. We apply a force $F$ to keep the piston in place.

% can insert picture
  What happens if we move the piston for a little bit? If we move through a distance $\d x$, then the volume of the gas has increased by $A \;\d x$. We assume $S$ is constant. Then the first law tells us
  \[
    \d E = -pA \;\d x.
  \]
  This formula should be very familiar to us. This is just the work done by the force, and this must be $F = pA$. So our definition of pressure in terms of partial derivatives reproduces the mechanics definition of force per unit area.
\end{eg}

There is a word of caution about this. We can equate $-p \;\d V$ with the work done on systems on for a \emph{reversible} change. We will not go into details now, as we will talk about reversibility later. One example with non-reversible changes include when we have friction.

But if we equate $-p \;\d V$ with the work done, then what is $T \;\d S$? There is only one possible thing left, and it is the heat supplied.

Note that the first law holds for \emph{any} change. It's just that the above interpretation does not.
\begin{eg}
  Consider the irreversible change, where we have a ``free expansion'' of gas into vacuum. We have a box
  \begin{center}
    \begin{tikzpicture}
      \draw (0, 0) rectangle (4, 2);
      \draw (2, 0) -- (2, 2);
      \node at (1, 1) {gas};
      \node at (3, 1) {vacuum};
    \end{tikzpicture}
  \end{center}
  We have a valve in the partition, and as soon as we open up the valve, the gas flows to the other side of the box.

  In this system, no energy has been supplied. So $\d E = 0$. However, $\d V > 0$, as volume clearly increased. But there is no work done on or by the gas. So in this case, $-p \;\d V$ is certainly not the work done. Using the first law, we know that
  \[
    T \;\d S = p \;\d V.
  \]
  So as the volume increases, the entropy increases as well.
\end{eg}

We now revisit the concept of heat capacity. We previously defined it as $\d E/\d T$, but now we need to decide what we want to keep fixed. We can keep $V$ fixed, and get
\[
  C_V = \left(\frac{\partial E}{\partial T}\right)_V = T \left(\frac{\partial S}{\partial T}\right)_V.
\]
While this is the obvious generalization of what we previously had, it is not a very useful quantity. We do not usually do experiments with fixed volume. For example, if we do a chemistry experiment in a test tube, say, then the volume is not fixed, as the gas in the test tube is free to go around. Instead, what is fixed is the pressure. We can analogously define
\[
  C_p = T \left(\frac{\partial S}{\partial T}\right)_p.
\]
Note that we cannot write this as some sort of $\frac{\partial E}{\partial T}$.

\subsection{The canonical ensemble}
So far, we have been using the microcanonical ensemble. This is a system with the energy known. This is appropriate for an isolated system, where the energy cannot change.

However, most systems are \emph{not} isolated, eg. a glass of water in the room. What \emph{is} fixed is the temperature. The \emph{canonical ensemble} describes exactly that --- we know the temperature, but not energy.

Imagine we have a system $S$, and it is interacting with a much larger system $R$. We call this $R$ a \emph{heat reservoir}. Since $R$ is assumed to be large, the energy of $S$ is negligible to $R$, and we will assume $R$ always has a fixed temperature $T$. Then in this set up, the system can exchange energy without changing $T$.

As before, we let $\bket{n}$ be a basis of microstates with energy $E_n$. We suppose we fix a total energy $E_{\mathrm{total}}$, and we want to find the total number of microstates of the combined system with this total energy. To do so, we fix some state $\bket{n}$ of $S$, and ask how many states of $S + R$ there are for which $S$ is in $\bket{n}$. We then later sum over all $\bket{n}$.

By definition, we can write this as
\[
  \Omega_R(E_{\mathrm{total}} - E_n) = \exp\left(k^{-1} S_R(E_{\mathrm{total}} - E_n)\right).
\]
Since $R$ is assumed to be much larger than $E$, we only get sensible contributions

By assumption, we know $R$ is a much larger system than $S$. So we only get significant contributions when $E_n \ll E_{\mathrm{total}}$. In these cases, we can Taylor expand to write
\[
  \Omega_R(E_{\mathrm{total}} - E_n) = \exp\left(k^{-1} S_R(E_{\mathrm{total}}) - \left(\frac{\partial S}{\partial E}\right)_V E_n\right).
\]
But we know what $\frac{\partial S_R}{\partial E}$ is --- it is just $T^{-1}$. So we finally get
\[
  \Omega_R(E_{\mathrm{total}} - E_n) = e^{k^{-1}S_R(E_{\mathrm{total}})} e^{-\beta E_n},
\]
where we define
\begin{defi}[$\beta$]\index{$\beta$}
  \[
    \beta = \frac{1}{kT}.
  \]
\end{defi}
Note that while we derived this this formula under the assumption that $E_n$ is small, it is effectively still valid when $E_n$ is large, because both sides are very tiny, and even if they are very tiny in different ways, it doesn't matter when we add over all states.

Now we can write the \emph{total} number of microstates of $S + R$ as
\[
  \Omega(E_{\mathrm{total}}) = \sum_n \Omega_R(E_{\mathrm{total}} - E_n) = e^{k^{-1} S_R(E_{\mathrm{total}})} \sum_n e^{-\beta E_n}.
\]
Note that we are summing over all states, not energy.

We now use the fundamental assumption of statistical mechanics that all states of $S + R$ are equally likely. Then we know the probability that $S$ is in state $\bket{n}$ is
\[
  p(n) =\frac{\Omega_R(E_{\mathrm{total}} - E_n)}{\Omega(E_{\mathrm{total}})} = \frac{e^{-\beta E_n}}{\sum_k e^{-\beta E_k}}.
\]
This is called the \term{Boltzmann distribution} for the canonical ensemble. Note that at the end, all the details have dropped out apart form the temperature. This describes the energy distribution of a system with fixed temperature.

Note that if $E_n \gg kT = \frac{1}{\beta}$, then the exponential is small. So only states with $E_n \sim kT$ have significant probability. In particular, as $T \to 0$, we have $\beta \to \infty$, and so only the ground state can be occupied.

We now define an important quantity.
\begin{defi}[Partition function]\index{partition function}
  The \emph{partition function} is
  \[
    Z = \sum_n e^{-\beta E_n}.
  \]
\end{defi}
So we have
\[
  p(n) = \frac{e^{-\beta E_n}}{Z}.
\]
\begin{prop}
  For two non-interacting systems, we have $Z(\beta) = Z_1(\beta) Z_2(\beta)$.
\end{prop}

\begin{proof}
  Since the systems are not interacting, we have
  \[
    Z = \sum_{n, m} e^{-\beta (E_n^{(1)} + E_n^{(2)})} = \left(\sum_n e^{-\beta E_n^{(1)}}\right)\left(\sum_n e^{-\beta E_n^{(2)}}\right) = Z_1 Z_2.
  \]
\end{proof}

Note that in general, energy is \emph{not} fixed, but we can compute the average value:
\[
  \bra E\ket = \sum_n p(n) E_n = \sum \frac{E_n e^{-\beta E_n}}{Z} = -\frac{\partial}{\partial \beta} \log Z.
\]
This partial derivative is taken with all $E_i$ fixed. Of course, in the real world, we don't get to directly change the energy eigenstates and see what happens. However, they do depend on some ``external'' parameters, such as the volume $V$, the magnetic field $B$ etc. So when we take this derivative, we have to keep all those parameters fixed.

We look at the simple case where $V$ is the only parameter we can vary. Then $Z = Z(\beta, V)$. We can rewrite the previous formula as
\[
  \bra E \ket = - \left(\frac{\partial}{\partial \beta} \log Z\right)_V.
\]
This gives us the average, but we also want to know the variance of $E$. We have
\[
  \Delta E^2 = \bra (E - \bra E\ket)^2 \ket = \bra E^2\ket - \bra E\ket^2.
\]
On the first example sheet, we calculate that this is in fact
\[
  \Delta E^2 = \left(\frac{\partial^2}{\partial \beta^2} \log Z\right)_V = - \left(\frac{\partial \bra E\ket}{\partial \beta}\right)_V.
\]
We can now convert $\beta$-derivatives to $T$-derivatives using the chain rule. Then we get
\[
  \Delta E^2 = kT^2 \left(\frac{\partial \bra E\ket}{\partial T}\right)_V = kT^2 C_V.
\]
So the fluctuation around the expectation is actually proportional to the heat capacity.

From this, we can learn something important. We would expect $\bra E\ket \sim N$, the number of particles of the system. But we also know $C_V \sim N$. So
\[
  \frac{\Delta E}{\bra E\ket} \sim \frac{1}{\sqrt{N}}.
\]
Therefore, the fluctuations are negligible if $N$ is large enough. This is called the \term{thermodynamic limit} $N \to \infty$. In this limit, we can ignore the fluctuations in energy. So we expect the microcanonical ensemble and the canonical ensemble to give the same result. And for all practical purposes, $N \sim 10^{23}$ is a large number.

Because of that, we are often going to just write $E$ instead of $\bra E\ket$.

\begin{eg}
  Suppose we had particles with
  \[
    E_{\uparrow} = \varepsilon,\quad E_{\downarrow} = 0.
  \]
  So for one particle, we have
  \[
    Z_1 = \sum_n e^{-\beta E_n} = 1 + e^{-\beta \varepsilon} = 2 e^{-\beta \varepsilon/2} \cosh \frac{\beta \varepsilon}{2}.
  \]
  If we have $N$ non-interacting systems, then since the partition function is multiplicative, we have
  \[
    Z = Z_1^N = 2^n e^{-\beta \varepsilon N/2} \cosh^N \frac{\beta \varepsilon}{2}.
  \]
  From the partition function, we can compute
  \[
    \bra E \ket = - \frac{\d \log Z}{\d \beta} = \frac{N\varepsilon}{2}\left(1 - \tanh \frac{\beta \varepsilon}{2}\right).
  \]
  We can check that this agrees with the value we computed with the microcanonical ensemble (when we wrote the result using exponentials directly), but the calculation is much easier.
\end{eg}

\subsubsection*{Entropy}
We now want to talk about the entropy of the canonical ensemble, but we do not really have a working definition for it, as the Boltzmann definition only applies to the microcanonical ensemble. So we need a new definition.

We can motivate our new definition as follows --- recall that we can think of the ensemble as having $W \gg 1$ copies of the world with $Wp(n)$ copies in the state $\bket{n}$. We now try to apply the Boltzmann definition of entropy
\[
  S = k \log \Omega
\]
to the whole ensemble, where now $\Omega$ is the number of ways of choosing which copy is in which state. This is just
\[
  \Omega = \frac{W!}{\prod_n (W p(n))!}.
\]
We can use Stirling's approximation, and find that
\[
  S_{\mathrm{ensemble}} = -kW \sum_n p(n) \log p(n).
\]
This suggests that we should define the entropy of a single copy as follows:
\begin{defi}[Gibbs entropy]\index{Gibbs entropy}
  The \emph{Gibbs entropy} of a probability distribution $p(n)$ is
  \[
    S = -k \sum_n p(n) \log p(n).
  \]
\end{defi}
If the density operator is given by
\[
  \hat{\rho} = \sum_n p(n) \bket{n}\brak{n},
\]
then we have
\[
  S = - \Tr(\hat{\rho} \log \hat{\rho}).
\]
We now check that this definition makes sense, in that when we have a microcanonical ensemble, we do get what we expect.
\begin{eg}
  In the microcanonical ensemble, we have
  \[
    p(n) =
    \begin{cases}
      \frac{1}{\Omega(E)} & E \leq E_n \leq E + \delta E\\
      0 & \text{otherwise}
    \end{cases}
  \]
  Then we have
  \[
    S = -k \sum_{n: E \leq E_n \leq E + \delta E} \frac{1}{\Omega(E)} \log \frac{1}{\Omega(E)} = -k \Omega(E) \frac{1}{\Omega(E)} \log \frac{1}{\Omega(E)} = k \log \Omega(E).
  \]
  So the Gibbs entropy reduces to the Boltzmann entropy.
\end{eg}

How about the canonical ensemble?
\begin{eg}
  In the canonical ensemble, we have
  \[
    p(n) = \frac{e^{-\beta E_n}}{Z}.
  \]
  Plugging this into the definition, we find that
  \begin{align*}
    S &= -k \sum_n p(n) \log \left(\frac{e^{-\beta E_n}}{Z}\right)\\
    &= -k \sum_n p(n) (-\beta E_n - \log Z)\\
    &= k \beta \bra E\ket + k\log Z,
  \end{align*}
  using the fact that $\sum p(n) = 1$.

  Using the formula of the expected energy, we find that this is in fact
  \[
    S = k \frac{\partial}{\partial T} (T \log Z)_V.
  \]
\end{eg}

\subsubsection*{Maximizing entropy}
It turns out we can reach the canonical ensemble in a different way. The second law of thermodynamics suggests we should always seek to maximize entropy. Now if we take the optimization problem of ``maximizing entropy'', what probability distribution will we end up with?

The answer depends on what constraints we put on the optimization problem. We can try to maximize $S_{\mathrm{Gibbs}}$ over all probability distributions such that $p(n) = 0$ unless $E \leq E_n \leq E + \delta E$. Of course, we also have the constraint $\sum p(n) = 1$. Then we can use a Lagrange multiplier $\alpha$ and extremize
\[
  k^{-1}S_{\mathrm{Gibbs}} + \alpha \left(\sum_n p(n) - 1\right),
\]
Differentiating with respect to $p(n)$ and solving, we get
\[
  p(n) = e^{\alpha - 1}.
\]
In particular, this is independent of $n$. So all microstates with energy in this range are equally likely, and this gives the microcanonical ensemble.

What about the canonical ensemble? It turns out this is obtained by maximizing the entropy over all $p(n)$ such that $\bra E\ket$ is fixed. The computation is on the first example sheet.

\subsection{Helmholtz free energy}

In the microcanonical ensemble, we discussed the second law of thermodynamics, namely the entropy increases with time and the maximum is achieved in an equilibrium.

But this is no longer true in the case of the canonical ensemble, because we now want to maximize the total entropy of the system plus the heat bath, instead of just the system itself. Then is there a proper analogous quantity for the canonical ensemble?

The answer is given by the \emph{free energy}. In the canonical ensemble, the probability of energy being between $E$ and $E + \delta E$ (for $\delta E \ll kT$) is given by
\[
  p(E) = \Omega_S(E) \frac{e^{-\beta E}}{Z} = \frac{1}{Z} e^{k^{-1} S - \beta E} = \frac{1}{Z} e^{-\beta (E - TS)}
\]
where $\Omega_S(E)$ is the number of states with energy between $E$ and $\delta E$. Here we assumed that $\delta E$ is small so that the probabilities of the different particles are similar. % fix this

We define
\begin{defi}[Helmholtz free energy]\index{Helmholtz free energy}\index{free energy!Helmholtz}
  The \emph{Helmholtz free energy} is
  \[
    F = E - TS,
  \]
\end{defi}
Then we have
\[
  p(E) = \frac{1}{Z} e^{-\beta F}.
\]
So in the canonical ensemble, the most likely $E$ is the one that minimizes $F$.

In general, in an isolated system, $S$ increases, and $S$ is maximized in equilibrium. In a system with a reservoir, $F$ decreases, and $F$ minimizes in equilibrium. In some sense $F$ captures the competition between entropy and energy. We will discuss this further later.

Now is there anything analogous to the first law
\[
  \d E = T \d S - p \d V?
\]
Since we have $S$ and $V$ on the right, here we are regarding energy as a function of entropy and volume, or entropy as a function of $E$ and $V$. Using this, we can write
\[
  \d F = \d E - \d(TS) = -S \d T - p \d V.
\]
So it is natural to think of $F$ as a function of $T$ and $V$. Mathematically, the relation between $F$ and $E$ is that $F$ is the \term{Legendre transform} of $E$.

From this expression, we can immediately write down
\[
  S = - \left(\frac{\partial F}{\partial T}\right)_V,
\]
and the pressure is
\[
  p = -\left(\frac{\partial F}{\partial V}\right)_T.
\]
The final claim about the free energy is that we have a very useful formula for it.
\begin{prop}
  \[
    F = -kT \log Z.
  \]
  Alternatively,
  \[
    Z = e^{-\beta F}.
  \]
\end{prop}

\begin{proof}
  We use the fact that
  \[
    \frac{\d}{\d \beta} = kT^2 \frac{\d}{\d T}.
  \]
  Then we can start from
  \begin{align*}
    F &= E - TS \\
    &= - \left(\frac{\partial \log Z}{\partial \beta}\right) - TS \\
    &= kT^2 \left(\frac{\partial \log Z}{\partial T}\right)_V - kT \frac{\partial}{\partial T}(T \log Z)_V\\
    &= -k T \log Z,
  \end{align*}
  and we are done. Good.
\end{proof}

\subsection{The chemical potential}
So far we have considered situations where we had fixed energy or fixed volume. However, there are often other things that are fixed. For example, the number of particles, or the electric charge of the system would be fixed. If we measure these things, then this restricts which microstates are accessible.

Let's consider $N$. This quantity was held fixed in the microcanonical and canonical ensembles. But these quantities do depend on $N$, and we can write
\[
  S(E, V, N) = k \log \Omega(E, V, N).
\]
Previously, we took this expression, and asked what happens when we varied $E$, and we got temperature. We then asked what happens when we vary $V$, and we got pressure. Now we ask what happens when we vary $N$.
\begin{defi}[Chemical potential]\index{chemical potential}
  The \emph{chemical potential} of a system is given by
  \[
    \mu = -T \left(\frac{\partial S}{\partial N}\right)_{E, V}.
  \]
\end{defi}

Why is this significant? Recall that when we only varied $E$, then we figured that two systems in equilibrium must have equal temperature. Then we varied $V$ as well, and found that two systems in equilibrium must have equal temperature \emph{and} pressure. Then it shouldn't be surprising that if we have two interacting systems that can exchange particles, then we must have equal temperature, pressure \emph{and} chemical potential. Indeed, the same argument works.

If we want to consider what happens to the first law when we vary $N$, we can just straightforwardly write
\[
  \d S = \left(\frac{\partial S}{\partial E}\right)_{V, N} \d E + \left(\frac{\partial S}{\partial V}\right)_{E, N} \d V + \left(\frac{\partial S}{\partial N}\right)_{E, V} \d V.
\]
Then as before, we get
\[
  \d E = T \;\d S - p \;\d V + \mu \;\d N.
\]
From this expressions, we can get some feel for what $\mu$ is. This is the energy cost of adding one particle at $S, V$. We will actually see later that $\mu$ is actually negative, as it is hard to add a particle keeping the entropy fixed.

Note that if we change $N$ to $Q$, then $\mu$ changes into $\Phi$, the electrostatic potential.

From the first law, we can write
\[
  \mu = \left(\frac{\partial E}{\partial N}\right)_{S, V}.
\]
We might ask, can we obtain this result from the original definition directly? Indeed, we can, using the magic identity
\[
  \left(\frac{\partial x}{\partial y}\right)_z \left(\frac{\partial y}{\partial z}\right)_x \left(\frac{\partial z}{\partial x}\right)_y = -1.
\]
In the canonical ensemble, we have fixed $T$, but the free energy will also depend on energy:
\[
  F(T, V, N) = E - TS.
\]
Again, we have
\[
  \d F = \d E - \d (TS) = -S\;\d T - p\;\d V + \mu\;\d N.
\]
So we have
\[
  \mu = \left(\frac{\partial F}{\partial N}\right)_{T, V}.
\]
\subsection{Grand canonical ensemble}
Consider a system $S$ in equilibrium with a ``heat and particle'' reservoir $R$, with which it can exchange both heat and particles. Again, $\mu$ and $T$ are fixed by their values in $R$. We repeat the argument with the canonical ensemble, and we will find that the probability that a system is in state $n$ is
\[
  p(n) = \frac{e^{-\beta (E_n - \mu N_n)}}{\mathcal{Z}},
\]
where $N_n$ is the number of particles in $\bket{n}$, and
\[
  \mathcal{Z} = \sum_n e^{-\beta(E_n- \mu N_n)}
\]
is the \term{grand canonical partition function}.

Of course, we can introduce more and more quantities after $V, N$, and then get more and more terms in the partition function, but they are really just the same. We can quickly work out how we can compute quantities from $Z$. By writing out the expressions, we have
\begin{prop}
  \[
    \bra E \ket - \mu \bra N\ket = -\left(\frac{\partial \mathcal{Z}}{\partial \beta}\right)_{\mu, V}.
  \]
\end{prop}

\begin{prop}
  \[
    \bra N\ket = \sum_n p(n) N_n = \frac{1}{\beta} \left(\frac{\partial \log \mathcal{Z}}{\partial \mu}\right)_{T, V}.
  \]
\end{prop}

As in the canonical ensemble, there is a simple formula for variance:
\begin{prop}
  \[
    \nabla N^2 = \bra N^2 \ket - \bra N\ket^2 = \frac{1}{\beta^2} \left(\frac{\partial^2 \log Z}{\partial \mu^2}\right)_{T, V} = \frac{1}{\beta} \left(\frac{\partial \bra N\ket}{\partial \mu}\right)_{T, V} \sim N.
  \]
  So we have
  \[
    \frac{\Delta N}{ \bra N\ket} \sim  \frac{1}{\sqrt{N}}.
  \]
\end{prop}

So again in the thermodynamic limit, the fluctuations in $\bra N\ket$ are negligible.

We can also calculate the Gibbs entropy:
\begin{prop}
  \[
    S = k \beta (\bra E \ket - \mu \bra N\ket) + k \log \mathcal{Z} - k \beta \left(\frac{\partial \log \mathcal{Z}}{\partial \beta}\right)_{\mu, V} + k \log \mathcal{Z} = k \frac{\partial}{\partial T}(T \log \mathcal{Z})_{\mu, N}.
  \]
\end{prop}

With the canonical ensemble, we had the free energy. There is an analogous thing we can define for the grand canonical ensemble.
\begin{defi}[Grand canonical potential]\index{grand canonical potential}\index{$\Phi$}
  The \emph{Grand canonical potential} is
  \[
    \Phi = F - \mu N = E - TS - \mu N.
  \]
\end{defi}
Then we have
\begin{prop}
  \[
    \d \Phi = - S \;\d T - p \;\d V - N \;\d \mu.
  \]
\end{prop}
We thus see that it is natural to view $\Phi$ as a function of $T$, $V$ and $\mu$. Using the formula for $E - \mu N$, we find
\[
  \Phi = - \left(\frac{\partial \log \mathcal{Z}}{ \partial \beta}\right)_{\mu, V} - kT \frac{\partial}{\partial T} (\log \mathcal{Z})_{\mu, V} = -kT \log \mathcal{Z},
\]
which is exactly the form we had for the free energy in the canonical ensemble. In particular, we have
\[
  \mathcal{Z} = e^{-\beta \Phi}.
\]
\subsection{Extensive and intensive properties}
The quantities we separate into two different types. There are things like $V, N$ etc that scale with the size of the volume. On the other hand, we have things like $\mu$ and $p$ do not scale with the size.

\begin{defi}[Extensive quantity]\index{extensive quantity}
  An \emph{extensive quantity} is one that scales proportionally to the size of the system
\end{defi}

\begin{eg}
  $N, V, E, S$ are all extensive quantities.
\end{eg}

Now note that the entropy is a function of $E, V, N$. So if we scale a system by $\lambda$, we find that
\[
  S(\lambda E, \lambda V, \lambda N) = \lambda S(E, V, N).
\]
\begin{defi}[Intensive quantity]\index{intensive quantity}
  An \emph{intensive quantity} is one that is independent of the size of the system.
\end{defi}

\begin{eg}
  Recall that we defined
  \[
    \frac{1}{T} = \left(\frac{\partial S}{\partial E}\right))_{V, N}.
  \]
  So if we scale the system by $\lambda$, then both $S$ and $E$ scale by $\lambda$, and so $T$ does not change. Similarly,
  \[
    p = T \left(\frac{\partial S}{\partial V}\right)_{T, N},\quad \mu = -T \left(\frac{\partial S}{\partial N}\right)_{T, V}
  \]
  are intensive quantities.
\end{eg}

\begin{eg}
  The \emph{free energy} is defined by
  \[
    F = E - TS.
  \]
  Since $E$ and $S$ are both extensive, and $T$ is intensive, we find that $F$ is extensive. So
  \[
    F (T, \lambda V, \lambda N) = \lambda F(T, V,  N).
  \]
  Similarly, the grand canonical potential is
  \[
    \Phi = F - \mu N.
  \]
  Since $F$ and $N$ are extensive and $\mu$ are intensive, we know $\Phi$ is extensive:
  \[
    \Phi(T, \lambda V, \mu) = \lambda \Phi(T, V, \mu).
  \]
\end{eg}
This tells us something useful. We see that $\Phi$ must be proportional to $V$. Indeed, taking the above equation with respect to $\lambda$< we find
\[
  V \left(\frac{\partial \Phi}{\partial V}\right)_{T, \mu} (T, \lambda V, \mu) = \Phi(T, V, \mu).
\]
Now setting $\lambda = 1$, we find
\[
  \Phi(T, V, \mu) = V \left(\frac{\partial \Phi}{\partial V} \right)_{T, \mu} = -pV.
\]
Here $p$ is an intensive quantity, it cannot depend on $V$. So we have
\[
  \Phi(T, V, \mu) = -p(T, \mu) V.
\]
This is quite a strong result.

\section{Classical gases}
So far, we have been doing statistical physics starting from quantum mechanics. But statistical mechanics was invented \emph{before} quantum mechanics. So we should be able to do statistical mechanics classically, and we will apply it to the case of classical gases. This classical theory agree quite well with experiment, except for a few small things that went wrong, and it turns out we have to solve these problems by going quantum.
\subsection{The classical partition function}
In the canonical ensemble, the quantum partition function is
\[
  Z = \sum_n e^{-\beta E_n}.
\]
What is the classical analogue? Classically, we can specify the state of a system by a point in \term{phase space}, which is the space of all positions and momentum.

For example, if we have a simple particle, then a point in phase space is just $(\mathbf{q}(t), \mathbf{p}(t))$, the position and momentum of the particle. It is conventional to use $\mathbf{q}$ instead of $\mathbf{x}$ when talking about a point in the phase space. So in this case, the phase space is a $6$-dimensional space.

The equation of motion determines the trajectory of the particle through phase space, given the initial position in phase space. This suggests that when we should replace the sum over states by an integral over phase space. Classically, we also know what the energy is. For a single particle, the energy is given by the Hamiltonian
\[
  H = \frac{\mathbf{p}^2}{2m} + V(\mathbf{q}).
\]
So it seems that we know what we need to know to make sense of the partition function classically. We might want to define the partition function as
\[
  Z_1 = \int \d^3 q\; \d^3 p e^{-\beta H(\mathbf{p}, \mathbf{q})}.
\]
This seems to make sense, except that we expect the partition function to be dimensionless. The solution is to introduce a quantity $h$, which has dimensions of length times momentum. Then we have
\begin{defi}[Partition function (single particle)]\index{partition function!single particle}
  We define the \emph{single particle partition function} as 
  \[
    Z_1 = \frac{1}{h^3} \int \d^3 q\; \d^3 p e^{-\beta H(\mathbf{p}, \mathbf{q})}.
  \]
\end{defi}
We notice that whenever we use the partition function, we usually differentiate the log of $Z$. So the factor of $h^3$ doesn't really matter for observable quantities.

However, recall that entropy is just given by $\log Z$, and we might worry that the entropy depends on $Z$. But it doesn't matter, because entropy is not actually observable. Only entropy differences are. So we are fine.

\begin{claim}
  This definition is ``correct'' with $h = 2\pi \hbar$.
\end{claim}

\begin{proof}[``Proof'']
  We can derive this from the quantum partition function in the limit $\hbar \to 0$.
\end{proof}
We will not actually prove or do the computations.

\subsection{Ideal gas}
Consider a system of $N$ particles in a volume $V$.
\begin{defi}[Ideal gas]\index{ideal gas}
  An \emph{ideal gas} is a gas where the particles do not interact with each other.
\end{defi}
Of course, this is never true, but we can hope this is a good approximation when the particles are far apart.

We start by assuming that the gases have no internal structure, and they are just single atoms, ie. we have a \term{monoatomic ideal gas}\index{ideal gas!monoatomic}. We will see what happens later when we have molecules.

In this case, the only energy we have is the kinetic energy, and we get
\[
  H = \frac{\mathbf{p}^2}{2m}.
\]
We just have to plug this into our partition function and evaluate the integral. We have
\begin{align*}
  Z_1(V, T) &= \frac{1}{(2\pi \hbar)63} \int \d^3 p\; \d^3 q e^{-\beta \mathbf{p}^2 /2m}\\
  &= \frac{V}{(2\pi \hbar)^3} \int \d^3 p\; e^{-\beta \mathbf{p}^2/2m}
\end{align*}
Recall that we have
\[
  \int \d x\; e^{-a x^2} = \sqrt{\frac{\pi}{a}}.
\]
Using this three times, we find
\begin{prop}
  For a monoatomic gas, we have
  \[
    Z_1(V, T) = V\left(\frac{mkT}{2\pi \hbar^2}\right)^{3/2} = \frac{V}{\lambda^3},
  \]
\end{prop}
where we define
\begin{defi}[Thermal de Broglie wavelength]\index{thermal de Broglie wavelength}\index{$\lambda$}\index{de Broglie wavelength!thermal}
  The \emph{thermal de Broglie wavelength} is
  \[
    \lambda = \sqrt{\frac{2\pi \hbar^2}{mkT}}.
  \]
\end{defi}
If we think of the wavelength as the ``size'' of the particle, then we see that the partition function counts the number of particles we can fit in the volume $V$. We notice that this involves $\hbar$, which is a bit weird since we are working classically, but we will see that the $\hbar$ doesn't appear in the formulas we derive from this.

What if we have more particles? For a system of $N$ particles, since the partition function is again multiplicative, we have
\[
  Z(N, V, T) = Z_1^N =  V^N \lambda^{-3N}.
\]
There is a small caveat at this point. This is not quite right, as we will see later, since if the particles are indistinguishable, then we are over-counting something here. It turns out this doesn't matter, so we'll just live with this.

Recall that we had
\[
  p = - \left(\frac{\partial F}{\partial V}\right)_T = \frac{\partial}{\partial V} (k T \log Z)_T,
\]
and this formula makes sense in this case. So we obtain
\[
  p = \frac{NkT}{V}.
\]
Rearranging, we obtain
\begin{prop}[Ideal gas law]\index{ideal gas law}
  \[
    pV = NkT.
  \]
\end{prop}
Notice that in this formula, the $\lambda$ has dropped out, and there is no dependence on $\hbar$. This is called an ``equation of state'', which links $p, V, T$.

This works well for gases at low densities, where particle-particle interactions are unimportant. However, as we increase the density in experiments, we start to see deviations from this law.

Let's now look at the energy of the ideal gas. We have
\[
  \bra E\ket = - \left(\frac{\partial \log Z}{\partial \beta}\right)_V = \frac{3}{2} NkT = 3N \left(\frac{1}{2} kT\right).
\]
We have $N$ particles, and each particle has three degrees of freedom. So the whole system has $3N$ degrees of freedom. So we can view this as saying for each degree of freedom, we get $\frac{1}{2}kT$ of energy. This is called the \term{equipartition of energy}. In the second example sheet, we see how this is modified when we have interactions.

Note that the average energy of a single particle is
\[
  \frac{\bra p^2\ket}{2m} = \frac{3}{2} kT.
\]
So we have
\[
  \bra p^2 \ket \sim mkT.
\]
So for a single particle, we have
\[
  |\mathbf{p}| \sim \sqrt{mKt},
\]
and so we have
\[
  \lambda \sim \frac{h}{|\mathbf{p}|},
\]
and this is the formula for the de Broglie wavelength. Finally, we can compute the heat capacity
\[
  C_V = \left(\frac{\partial E}{\partial T}\right)_V = \frac{3}{2}Nk.
\]
\subsubsection*{Boltzmann's constant}
Recall that Boltzmann's constant is
\[
  k = \SI{1.381e-23}{\joule\per\kelvin}
\]
This number shows that we were terrible at choosing units. If we were to invent physics again, we would pick energy to have the same unit as temperature, so that $k = 1$. This number $k$ is just a conversion factor between temperature and energy.

Now we may ask --- why does $k$ have such a small value, in these units? We can look at the ideal gas law
\[
  \frac{pV}{T} = Nk.
\]
We can try to plug in everyday values for the LHS. We have
\begin{align*}
  p &= \SI{e5}{\newton\per\meter\squared}\\
  V &= \SI{1}{\meter\cubed}\\
  T &= \SI{300}{\kelvin},
\end{align*}
and we find that the LHS is $\sim 300$. We also know that $N \sim 10^{23}$. So we would expect $k$ to be tiny.

So the reason that $k$ is small tells us that everyday lumps of matter contain a lot of particles, and this lets us know that atoms are small.
\subsubsection*{Entropy}
The next thing we want to figure out is the entropy of the ideal gas. We previously had the formula
\[
  Z = Z_1^N,
\]
but this is not quite right. In quantum mechanics, we know that if we swap two indistinguishable particles, then we get back the same state, at least up to a sign. Similarly, if we permute any of our particles, which are indistinguishable, then we get the same system. We are over-counting the particles. What we really should do is to divide by the number of ways to permute the particles, namely $N!$:
\[
  Z = \frac{1}{N!} Z_1^N.
\]
Just as in the constant $h$ in our partition function, this $N!$ doesn't affect any of our observables. In particular, $p$ and $\bra E\ket$ are unchanged. However, this $N!$ does affect the entropy
\[
  S = \frac{\partial}{\partial T} (kT \log Z).
\]
Plugging the partition function in and using Stirling's formula, we get
\[
  S = Nk \left(\log \left(\frac{V}{N\lambda^3}\right) + \frac{5}{2}\right).
\]
This is known as the \term{Sackur-Tetrode equation}.

Recall that the entropy is tan extensive property. If we re-scale the system by a factor of $\alpha$, then
\[
  N \mapsto \alpha N, V \mapsto \alpha V.
\]
Since $\lambda$ depends on $T$ only, it is an intensive quantity, and this indeed scales as $S \mapsto \alpha S$. But for this to work, we really needed the $N$ inside the logarithm, and the reason we have the $N$ inside the logarithm is that we had an $N!$ in the partition function.

When people first studied statistical mechanics of ideal gases, they didn't know about quantum mechanics, and didn't know they should put in the $N!$. Then the resulting value of $S$ is no longer extensive. This leads to \term{Gibbs paradox}. The actual paradox is as follows:

Suppose we have a box of bas with entropy $S$. We introduce a partition between the gases, so that the individual partitions have entropy $S_1$ and $S_2$. Then the fact that the gas is not extensive means
\[
  S \not= S_1 + S_2.
\]
This means by introducing or removing a partition, we have increased or decreased the entropy, which violates the second law of thermodynamics.

This $N!$, which comes from quantum effects, fixes this problem. This is a case where quantum mechanics is needed to understand something that is really should be classical.

Note again that here $S$ depends on $\lambda$, which depends on $\hbar$. However, what actually is measurable is the entropy differences, and the $\hbar$ drops out when when we take energy differences.

\subsubsection*{Grand canonical ensemble}
We now consider the case where we have a grand canonical ensemble, so that we can exchange heat \emph{and} particles. In the case of gas, we can easily visualize this as a small open box of gas where gas is allowed to freely flow around. The grand ensemble has partition function
\begin{align*}
  \mathcal{Z}_{\mathrm{ideal}}(\mu, V, T) &= \sum_{N = 0}^\infty e^{\beta \mu N} Z_{\mathrm{ideal}}(N, V, T) \\
  &= \sum_{N = 0}^\infty \frac{1}{N!} \left(\frac{e^{\beta \mu}}{\lambda^3}\right)^N\\
  &= \exp\left(\frac{e^{\beta \mu}V}{\lambda^3}\right)
\end{align*}
Armed with this, we can now calculate the average number of particles in our system. Doing the same computations as before, we have
\[
  N = \frac{1}{\beta} \left(\frac{\partial \log Z}{\partial \mu}\right)_{V, T} = \frac{e^{\beta \mu}V}{\lambda^3}.
\]
So we can work out the value of $\mu$:
\[
  \mu = kT \log \left(\frac{\lambda^3 N}{V}\right).
\]
Now we can use this to get some idea of what the chemical potential actually means. For a classical gas, we need the wavelength to be significantly less than the average distance between particles, ie.
\[
  \lambda \ll \left(\frac{V}{N}\right)^3,
\]
so that the particles are sufficiently separated. If this is not true, then quantum  effects are important, and we will look at them later. If we plug this into the logarithm, we find that $\mu < 0$.

Remember that $\mu$ is defined by
\[
  \mu = \left(\frac{\partial E}{\partial N}\right)_{S, V}.
\]
It might seem odd that we get energy out when we add a particle. But note that this derivative is taken with $S$ fixed. Normally, we would expect adding a particle to increase the entropy. So to keep the entropy fixed, we must simultaneously take out energy of the system, and so $\mu$ is negative.

Continuing our exploration of the grand canonical ensemble, we can look at the fluctuations in $N$, and find
\[
  \Delta N^2 = \frac{1}{\beta^2}  \log \mathcal{Z}_{\mathrm{ideal}} = N.
\]
So we find that
\[
  \frac{\Delta N}{N} = \frac{1}{\sqrt{N}} \to 0
\]
as $N \to \infty$. So in the thermodynamic limit, the fluctuations are negligible.

We can now get the equation of state. Recall the grand canonical potential is
\[
  \Phi = -kT \log \mathcal{Z},
\]
and that
\[
  pV = -\Phi.
\]
Since we know $\log \mathcal{Z}$, we can work out what $pV$ is, we find that
\[
  pV = kT \frac{e^{\beta \mu}V}{\lambda^3} = NkT.
\]
So the ideal gas law\index{ideal gas law} is also true in the grand canonical ensemble. Also, from cancelling the $V$ from both sides, we see that this determines $p$ as a function of $T$ and $\mu$.

\subsection{Maxwell distribution}
We calculated somewhere the average energy of our gas, so we can calculate the average energy of an atom in the gas. But that is just the average energy. We might want to figure out the distribution of energy in the atoms of our gas. Alternatively, what is the distribution of particle speed in a gas?

We can get that fairly straightforwardly from what we've got so far.

We ask what's the probability of a given particle being in a region of phase space of volume $\d^3 q \;\d^3 p$ centered at $(\mathbf{q}, \mathbf{p})$. We know what this is. It is just
\[
  C e^{-\beta \mathbf{p}^2/2m}\;\d^3 q\;\d^3 p
\]
for some normalization constant $C$, since the kinetic energy of a particle is $\mathbf{p}^2/2m$. Now suppose we don't care about the position, and just want to know about the momentum. So we integrate over $q$, and the probability that the momentum is within $\d^3 p$ of $\mathbf{p}$ is
\[
  CV \d^3 \mathbf{p} e^{-\beta \mathbf{p}^2/2m}.
\]
Let's say we are interested in velocity instead of momentum, so this is equal to
\[
  CV m^2 \d^3 \mathbf{v} e^{-\beta m\mathbf{v}^2/2}.
\]
Moreover, we are only interested in the speed, not the velocity itself. So we introduce spherical polar coordinates $(v, \theta, \phi)$ for $\mathbf{v}$. Then we get
\[
  CV m^3 \sin \theta \d \theta\;\d \varphi\;v^2 \d v\;e^{-mv^2/(2kT)}.
\]
Now we don't care about the direction, so we again integrate over all possible values of $\theta$ and $\phi$. Thus, the probability that the speed is within $\d v$ of $v$ is
\[
  f(v)\;\d v = \mathcal{N} v^2 e^{-mv^2/(2kT)} \;\d v,
\]
where we absorbed all our numbers into the constant $\mathcal{N}$. Then $f$ is the probability density function of $v$. We can fix this constant $\mathcal{N}$ by normalizing:
\[
  \int_0^\infty f(v) \;\d v = 1.
\]
So this fixes
\[
  \mathcal{N} = 4\pi \left(\frac{m}{2\pi kT}\right)^{1/2}.
\]
This $f(v)$ is called the \term{Maxwell distribution}.

We can try to see what $f$ looks like. We see that for large $v$, it is exponentially decreasing, and for small $v$, it is quadratic in $v$. % insert plot

This is expected, because we know that the energy of the particle is always $\frac{1}{2}kT$, and so for lighter particles, we need to have higher energy.

We can sanity-check that the expected value of $v$ is correct. We have
\[
  \bra v\ket = \int_0^\infty v^2 f(v) \;\d v = \frac{3kT}{m}.
\]
So we find that
\[
  \bra E\ket = \frac{1}{2} m\bra v^2\ket = \frac{3}{2} kT.
\]
This agrees with the equipartition of energy, as it must. But now we have an actual distribution, we can compute other quantities like $\bra v^4\ket$.

In these derivations, we assumed we were dealing with a monoatomic ideal gas, it happens that in fact this holds for a much more general family of gases.

\subsection{Diatomic gases}
We now move on to consider more complicated gases. If we have molecules, then they can contain other forms of energy such as rotational energy.

Everything we are doing is classical, so we need to come up with a model of a diatomic molecule. We can model them as two point masses joined together by a massless spring.
\begin{center}
  \begin{tikzpicture}
    \fill circle [radius=0.1];
    \fill (2, 0) circle [radius=0.1];
    \node [above] at (0, 0.1) {$m_1$};
    \node [above] at (2, 0.1) {$m_2$};
    \draw [decorate, decoration={snake, amplitude=1, segment length=5}] (0, 0) -- (2, 0);
  \end{tikzpicture}
\end{center}
As we know from classical dynamics, we can decompose the motion into different components:
\begin{itemize}
  \item Translation of the center of mass. We can view this as a single mass with energy $M = m_1 + m_2$.
  \item Rotations about center of mass. There are two axes of rotations orthogonal to the spring, and these have a moment of inertia $I$. We will ignore the rotation along the axes parallel to the spring because we assume the masses are point masses.
  \item Vibrations along the axis of symmetry. The important quantity here is the reduced mass
    \[
      m = \frac{m_1 m_1}{m_1 + m_2}.
    \]
\end{itemize}
We will assume all these motions are independent. In reality, the translation is indeed independent from the others, but the rotations and vibrations can couple in complicated ways. But with this assumption, we have
\[
  Z_1 = Z_{\mathrm{trans}} Z_{\mathrm{rot}} Z_{\mathrm{vib}}.
\]
We can obtain $Z_{\mathrm{trans}}$ just as the partition energy we obtained previously for a single mass, with mass $M$. The remaining two terms depend only on the relative position of the masses. So they do not depend on the molecule as a whole, and are going to be independent of $V$.

Now when we differentiate $\log Z_1$ with respect to $V$, then the latter two terms drop out, and the ideal gas law still holds.

We now try to figure out how rotations and vibrations contribute to the partition function. For rotations, we can parametrize this using spherical polars $(\theta, \varphi)$. The Lagrangian for this rotational motion is given by
\[
  \mathcal{L}_{\mathrm{rot}} = \frac{1}{2} I (\dot{\theta}^2 + \sin^2 \theta \dot{\varphi}^2).
\]
The conjugate momenta are given by
\begin{align*}
  p_\theta &= \frac{\partial \mathcal{L}}{\partial \dot{\theta}} = I \dot{\theta}\\
  p_\varphi &= \frac{\partial \mathcal{L}}{\partial \dot{\varphi}} = I \sin^2 \theta \dot\varphi.
\end{align*}
So we have
\[
  H_{\mathrm{rot}} = \dot{\theta} p_\theta + \dot{\varphi} p_\varphi - \mathcal{L}_{\mathrm{rot}} = \frac{p_\theta^2}{2I} + \frac{p_\varphi^2}{2I \sin^2 \theta}.
\]
We can then work out the partition function
\[
  Z_{\mathrm{rot}} = \frac{1}{(2\pi \hbar)^2} \int \d \theta \d \varphi \d p_\theta \d p_\varphi e^{-\beta H_{\mathrm{rot}}}
\]
We note that the $p_\theta$ and $p_\varphi$ integrals are just Gaussians, and so we get
\[
  Z_{\mathrm{rot}} = \frac{1}{(2\pi \hbar)^2} \frac{2\pi I}{\beta} \int_0^\pi \d \theta \sin \theta \int_0^{2\pi} \d \varphi = \frac{2IkT}{\hbar^2}.
\]
Then we get
\[
  E_{\mathrm{rot}} = - \frac{\partial}{\partial \beta} \log Z_{\mathrm{rot}} = \frac{1}{\beta} = kT.
\]
We have two independent axes of rotation, so by equipartition of energy, we have an energy of
\[
  \frac{1}{2}kT
\]
for each degree of freedom.

\begin{eg}
  In the case of a system where vibrations are not important, eg. if the string is very rigid, then we are done, and we have found
  \[
    Z = Z_{\mathrm{trans}} Z_{\mathrm{rot}} \propto (kT)^{5/2}.
  \]
  Then the partition function for $N$ particles is
  \[
    Z = \frac{1}{N!} Z_1^N.
  \]
  and the total energy is
  \[
    -\frac{\partial}{\partial \beta} \log Z = \frac{5}{2} NkT.
  \]
  This is exactly as we expect. There is $\frac{3}{2}NkT$ from translation, and $NkT$ from rotation. From this, we obtain the heat capacity
  \[
    C_V = \frac{5}{2} Nk.
  \]
\end{eg}
We now put in the vibrations. Since we are modelling it by a spring, we can treat it as a harmonic oscillator, with mass $m$ and frequency $\omega$, which is determined by the bond strength. Then if we let $\zeta$ be the displacement form equilibrium, then the Hamiltonian is
\[
  H_{\mathrm{vib}} = \frac{p_\zeta^2}{2m} + \frac{1}{2}m \omega^2 \zeta^2.
\]
So we have
\[
  Z_{\mathrm{fib}} = \frac{1}{2\pi \hbar} \int \d \zeta \d p_\zeta e^{-\beta H_{\mathrm{vib}}} = \frac{kT}{\hbar \omega}.
\]
From the partition function, we can get the energy of a single molecule, and find it to be
\[
  E_{\mathrm{vib}} = kT.
\]
This is the average energy in the vibrational motion of a molecule. This looks a bit funny. The vibration is only one degree of freedom, but the equipartition of energy seems to think this has two degrees of freedom. It turns out equipartition of energy behaves differently when we have potential energy, and we can think of this as one degree of freedom for kinetic energy and another for potential.

Putting all three types of motion together, we get
\[
  E = \frac{7}{2} NkT,
\]
and the heat capacity is
\[
  C_V- = \frac{7}{2}Nk.
\]
Note that these results are completely independent of the parameters describing the molecule. Of course, there are things that depend on, say, the mass. For example, to keep the same energy, if we increase the mass, then the speed must decrease.

Does this agree with experiments? The short answer is no! If we go and measure the heat capacity of, say, molecular hydrogen, we find
\begin{center}
  \begin{tikzpicture}
    \draw [->] (0, 0) -- (5, 0) node [right] {$T/K$};
    \draw [->] (0, 0) -- (0, 3) node [above] {$C_V/Nk$};
  \end{tikzpicture} % mark 200, 2000; 1.5, 2.5, 3.5
\end{center}
So it seems like our prediction only works when we have high enough temperature. At lower temperatures, the vibration modes freeze out. Then as we further lower the energy, rotation modes also go away.  This is a big puzzle classically! This is explained by quantum effects, which we will discuss later. % check ``freeze''.

\subsection{Interacting gases}
So far, we have been talking about ideal gases. What happens if there \emph{are} interactions?

For a real gas, if they are sufficiently dilute, ie. $N/V$ is small, then we expect the interactions to be negligible. This suggests that we can try to capture the effects of interactions perturbatively in $\frac{N}{V}$. We can write
\[
  \frac{p}{kT} = \frac{N}{V}.
\]
We can think of this as a first term in an expansion, and add higher order terms
\[
  \frac{p}{kT} = \frac{N}{V} + B_2(T) \frac{N^2}{V^2} + B_3 (T) \frac{N^3}{V^3} + \cdots.
\]
Note that the coefficients depend on $T$ only, as they should be intensive quantities. This is called the \term{Virial expansion}, and the coefficients $B_k(T)$ are the \term{Virial coefficients}. Our goal is to figure out what these $B_k(T)$ are.

We suppose the interaction is given by a potential energy $U(r)$ between two neutral atoms (assuming monoatomic) at separation $r$. Here we need to use some physics. It turns out that for large $r$ (relative to atomic size), we have
\[
  U(r) \propto -\frac{1}{r^6}.
\]
This comes from dipole-dipole interactions. Heuristically, we can understand this as follows --- the expectation values of electric dipole of an atom vanishes, but there exists non-trivial probability that the dipole $p_1$ is non-zero. This gives an electric field
\[
  E \sim \frac{p_1}{r^3}.
\]
This induces a dipole $p_2$ in atom $2$. So we have
\[
  p_2 \propto E \sim \frac{p_1}{r^3}.
\]
So the resulting potential energy is
\[
  U \propto -p_2 E \sim -\frac{p_1^2}{r^6}.
\]
This is called the \term{van der Waals interaction}. Note that the negative sign means this is an attractive force.

For small $r$, the electron orbitals of the atoms start to overlap, and then we get repulsion due to the Pauli principle. All together, the Lennard-Jones potential is given by
\[
  U(r) = U_0\left(\left(\frac{r_0}{r}\right)^{12} - \left(\frac{r_0}{r}\right)^6\right).
\]
We are going to use a ``hard core repulsion'' potential instead:
% insert diagram, straight after $r_0$.
This is given by
\[
  U(r) = 
  \begin{cases}
    \infty & r < r_0\\
    -U_0 \left(\frac{r_0}{r}\right)^6 & r > r_0
  \end{cases}
\]
This makes the calculations easy. The Hamiltonian of the gas is
\[
  H = \sum_{i = 1}^N \frac{\mathbf{p}_i^2}{2m} + \sum_{i > j} U(r_{ij}),
\]
with
\[
  r_{ij} = |\mathbf{r}_i - \mathbf{r}_j|.
\]
is the separation between particle $i$ and particle $j$.

Because of this interaction, it is no longer the case that the partition function for $N$ particles is the $N$th power of the partition function for one particle. We have
\begin{align*}
  Z(N, V, T) &= \frac{1}{N} \frac{1}{(2\pi \hbar)^{3n}} \int \prod_{i = 1}^N \d^3 p_i\;  \d^3 r_i e^{-\beta H}\\
  &= \frac{1}{N!} \frac{1}{(2\pi \hbar)^{3n}} \left(\int  \prod_{i} \d^3 p_i e^{-\beta p_i^2/2m}\right)\left(\int \prod_i \d^3 r_i\; e^{-\beta \sum_{j < k} U(r_{ij})} \right)\\
  &= \frac{1}{N! \lambda^{3N}} \int \prod_i \d^3 r_i\;  e^{-\beta \sum_{j < k} U(r_{ij})},
\end{align*}
where again
\[
  \lambda = \sqrt{\frac{2\pi \hbar^2}{mkT}}.
\]
Since we want to get an expansion, we might want to expand this in terms of the potential, but that is not helpful, because the potential is infinite for small $r$. Instead it is useful to consider the \term{Mayer $f$-function}:
\[
  f(r) = e^{-\beta U(r)} - 1.
\]
This function has the property that $f(r) = -1$ for $r < r_0$, and $f(r) \to 0$ as $r \to \infty$. So this is a nicer function as it only varies within this finite range.

We further define
\[
  f_{ij} = f(r_{ij}).
\]
Then we have
\begin{align*}
  Z(N, V, T) &= \frac{1}{N! \lambda^{3N}} \left(\prod_i \d^3 r_i\prod_{j < k} (1 + f_{jk})\right)\\
  &= \frac{1}{N!\lambda^{3N}} \int \prod_i \d^3 r_i \left(1 + \sum_{j < k} f_{jk} + \sum_{j < k} \sum_{\ell < m} f_{jk} f_{\ell m} + \cdots\right).
\end{align*}
The first term is just
\[
  \int \prod_i \d^3 r_i = V^N,
\]
and this gives the ideal gas term. Now each of the second terms is the same, eg. for $j = 1, k = 2$, this is
\[
  \int \prod_i \d^3 r_i f_{12} = V^{N - 2} \int \d^3 r_1 \; \d^3 r_2 f_{12} = V^{N - 1} I,
\]
where
\[
  I = \int\d^3 r f(r).
\]
Since $f(r) \to 0$ as $r \to \infty$, we might as well integrate over all space. Summing over all terms, and approximating $N(N - 1)/2 \sim N^2/2$, we find that the first two terms of the partition function are
\[
  Z(N, V, T) = \frac{V^N}{N! \lambda^{3N}} \left(1 + \frac{N^2}{2V}I + \cdots\right).
\]
Up to first order, we can write this as
\begin{align*}
  Z(N, V, T) &= \frac{V^N}{N! \lambda^{3N}} \left(1 + \frac{N}{2V}I + \cdots\right)^N\\
  &= Z_{\mathrm{ideal}} \left(1 + \frac{N}{2V}I + \cdots \right)^N.
\end{align*} % this makes it look extensive.
So we find that the free energy is
\[
  F = -kT \log Z = F_{\mathrm{ideal}} - NkT \log \left(1 + \frac{N}{2V} I + \cdots\right).
\]

\printindex
\end{document}
