\documentclass[a4paper]{article}

\def\npart {II}
\def\nterm {Michaelmas}
\def\nyear {2016}
\def\nlecturer {J. Miller}
\def\ncourse {Probability and Measure}
\def\nlecture {MWF.9}

\input{header}

\begin{document}
\maketitle
{\small
\noindent\emph{Analysis II is essential}
\vspace{10pt}

\noindent Measure spaces, $\sigma$-algebras, $\pi$-systems and uniqueness of extension, statement *and proof* of Carath\'eodory's extension theorem. Construction of Lebesgue measure on $\R$. The Borel $\sigma$-algebra of $\R$. Existence of non-measurable subsets of $\R$. Lebesgue-Stieltjes measures and probability distribution functions. Independence of events, independence of $\sigma$-algebras. The Borel--Cantelli lemmas. Kolmogorov's zero-one law.\hspace*{\fill}[6]

\vspace{5pt}
\noindent Measurable functions, random variables, independence of random variables. Construction of the integral, expectation. Convergence in measure and convergence almost everywhere. Fatou's lemma, monotone and dominated convergence, differentiation under the integral sign. Discussion of product measure and statement of Fubini's theorem.\hspace*{\fill}[6]

\vspace{5pt}
\noindent Chebyshev's inequality, tail estimates. Jensen's inequality. Completeness of $L^p$ for $1 \leq p \leq \infty$. The H\"older and Minkowski inequalities, uniform integrability.\hspace*{\fill}[4]

\vspace{5pt}
\noindent $L^2$ as a Hilbert space. Orthogonal projection, relation with elementary conditional probability. Variance and covariance. Gaussian random variables, the multivariate normal distribution.\hspace*{\fill}[2]

\vspace{5pt}
\noindent The strong law of large numbers, proof for independent random variables with bounded fourth moments. Measure preserving transformations, Bernoulli shifts. Statements *and proofs* of maximal ergodic theorem and Birkhoff's almost everywhere ergodic theorem, proof of the strong law.\hspace*{\fill}[4]

\vspace{5pt}
\noindent The Fourier transform of a finite measure, characteristic functions, uniqueness and inversion. Weak convergence, statement of L\'vy's convergence theorem for characteristic functions. The central limit theorem.\hspace*{\fill}[2]%
}

\tableofcontents
\setcounter{section}{-1}
\section{Introduction}
Recall that if $f: [0, 1] \to \R$ is continuous, then the Riemann integral of $f$ is defined as follows:
\begin{enumerate}
  \item Take a partition $0 = t_0 < t_1 < \cdots < t_n = 1$ of $[0, 1]$.
  \item Consider the Riemann sum
    \[
      \sum_{j = 1}^n f(t_j) (t_j - t_{j - 1})
    \]
  \item The Riemann integral is
    \[
      \int f = \text{Limit of Riemann sums as the mesh size of the partition }\to 0.
    \]
\end{enumerate}
% insert diagram

In this course, we are going to learn a different way of calculating integrals. The idea is very simple, but it is going to be very powerful mathematically.

The idea of measure theory is to use a different approximation scheme. Instead of partitioning the domain, we partition the range of the function. We fix some numbers $r_0 < r_1 < r_2 < \cdots < r_n$.

% insert analogous diagram

We then approximate the integral of $f$ by
\[
  \sum_{j = 1}^n r_j \cdot (\text{``size of }f^{-1}([r_{j - 1}, r_j])\text{''}).
\]
We then define the integral as the limit of approximations of this type as the mesh size of the partition $\to 0$.

We can make an analogy with bankers --- If a Riemann banker is given a stack of money, they would just add the values of the money in order. A measure-theoretic banker will sort the bank notes according to the type, and then find the total value by multiplying the number of each type by the value, and adding up.

Why would we want to do so? It turns out this leads to a much more general theory of interaction on a much more general spaces. In the context of $\R$, this theory of integration is much much more powerful than the Riemann sum, and can integrate a much wider class of functions. Theorems for interchanging limits and integration also become much stronger.

We will write $f_n \nearrow f$ for ``$f_n$ converges to $f$ monotonically increasingly'', and $f_n \searrow f$ similarly.

\section{Measures}
The starting point of all these is to come up with a function that determines the ``size'' of a given set, known as a \emph{measure}. It turns out we cannot sensibly define a size for \emph{all} subsets of $[0, 1]$. Thus, we need to restrict our attention to a collection of ``nice'' subsets.

\begin{defi}[$\sigma$-algebra]\index{$\sigma$-algebra}\index{sigma-algebra}
  Let $E$ be a set. A \emph{$\sigma$-algebra} $\mathcal{E}$ on $E$ is a collection of subsets of $E$ such that
  \begin{enumerate}
    \item $\emptyset \in \mathcal{E}$.
    \item $A \in \mathcal{E}$ implies that $A^C = X \setminus A \in \mathcal{E}$.
    \item For any sequence $(A_n)$ in $\mathcal{E}$, we have that
      \[
        \bigcup_n A_n \in \mathcal{E}.
      \]
  \end{enumerate}
  The pair $(E, \mathcal{E})$ is called a \emph{measurable space}.\index{measurable space}
\end{defi}
Note that the axioms imply that $\sigma$-algebras are also closed under intersections.

\begin{defi}[Measure]\index{measure}
  A \emph{measure} on a measurable space $(E, \mathcal{E})$ is a function $\mu: \mathcal{E} \to [0, \infty]$ such that
  \begin{enumerate}
    \item $\mu(\emptyset) = 0$
    \item Countable additivity: For any disjoint sequence $(A_n)$ in $\mathcal{E}$, then
      \[
        \mu\left(\bigcup_n A_n\right) = \sum_{n = 1}^\infty \mu(A_n).
      \]
  \end{enumerate}
\end{defi}

\begin{eg}
  Let $E$ be any countable set, and $\mathcal{E} = P(E)$ be the set of all subsets of $E$. A \emph{mass function}\index{mass function} is any function $m: E \to [0, \infty]$. We can then define a measure by setting
  \[
    \mu(A) = \sum_{x \in A} m(x).
  \]
  In particular, if we put $m(x) = 1$ for all $x \in E$, then we obtain the \emph{counting measure}\index{counting measure}.
\end{eg}

Countable spaces are nice, because we can always take $\mathcal{E} = P(E)$, and the measure can be defined on all possible subsets. However, for ``bigger'' spaces, we have to be more careful. The set of all subsets is often ``too large''. We will see a concrete and also important example of this later.

In general, $\sigma$-algebras are often described on large spaces in terms of a smaller set, known as the \emph{generating sets}\index{generating set}.
\begin{defi}[Generator of $\sigma$-algebra]\index{generator of $\sigma$-algebra}
  Let $E$ be a set, and that $\mathcal{A} \subseteq P(E)$ be a collection of subsets of $E$. We define
  \[
    \sigma(\mathcal{A}) = \{A \subseteq E: A \in \mathcal{E}\text{ for all $\sigma$-algebras $\mathcal{E}$ that contain $\mathcal{A}$}\}..
  \]
  In other words $\sigma(\mathcal{A})$ is the smallest sigma algebra that contains $\mathcal{A}$. This is known as the sigma algebra \emph{generated by} $\mathcal{A}$.
\end{defi}

\begin{eg}
  Take $E = \Z$, and $\mathcal{A} = \{\{x\}: x \in \Z\}$. Then $\sigma(\mathcal{A})$ is just $P(E)$, since every subset of $E$ can be written as a countable union of singletons.
\end{eg}

\begin{eg}
  Take $E = \Z$, and let $\mathcal{A} = \{ \{x, x + 1, x + 2, x + 3, \cdots\}: x \in E\}$. Then again $\sigma(E)$ is the set of all subsets of $E$.
\end{eg}

The following is the most important $\sigma$-algebra in the course:
\begin{defi}[Borel $\sigma$-algebra]\index{Borel $\sigma$-algebra}
  Let $E = \R$, and $\mathcal{A} = \{U \subseteq \R: U \text{ is open}\}$. Then $\sigma(\mathcal{A})$ is known as the \emph{Borel $\sigma$-algebra}, which is \emph{not} the set of all subsets of $\R$.

  We can equivalently define this by $\tilde{\mathcal{A}} = \{(a, b): a < b, a, b \in \Q\}$. Then $\sigma(\tilde{\mathcal{A}})$ is also the Borel $\sigma$-algebra.
\end{defi}

When proving things about $\sigma$-algebras, we typically check a given property on a generating set. Often, it us useful to focus on special generating sets which are called $\pi$-systems.

\begin{defi}[$\pi$-system]\index{pi-system}\index{$\pi$-system}
  Let $\mathcal{A}$ be a collection of subsets of $E$. Then $\mathcal{A}$ is called a \emph{$\pi$-system} if
  \begin{enumerate}
    \item $\emptyset \in A$
    \item If $A, B \in \mathcal{A}$, then $A \cap B \in A$.
  \end{enumerate}
\end{defi}

\begin{defi}[d-system]\index{d-system}
  Let $\mathcal{A}$ be a collection of subsets of $E$. Then $\mathcal{A}$ is called a \emph{d-system} if
  \begin{enumerate}
    \item $E \in \mathcal{A}$
    \item If $A, B \in \mathcal{A}$ and $A \subseteq B$, then $B \setminus A \in \mathcal{A}$
    \item For all increasing sequences $(A_n)$ in $\mathcal{A}$, we have that $\bigcup_n A_n \in \mathcal{A}$.
  \end{enumerate}
\end{defi}
The point of d-systems and $\pi$-systems is that they separate the axioms of a $\sigma$-algebra into two parts. More precisely, we have
\begin{prop}
  A collection $\mathcal{A}$ is a $\sigma$-algebra if and only if it is both a $\pi$-system and a $d$-system.
\end{prop}
This follows rather straightforwardly from the definitions.

\begin{lemma}[Dynkin's $\pi$-system lemma]\index{Dynkin's $\pi$-system lemma}
  Let $\mathcal{A}$ be a $\pi$-system. Then any d-system which contains $\mathcal{A}$ contains $\sigma(A)$.
\end{lemma}
While this seems like a simple statement with an easy proof, this is very useful and will be used a lot.

\begin{proof}
  Let $\mathcal{D}$ be the intersection of all d-systems containing $\mathcal{A}$, ie. the smallest d-system containing $\mathcal{A}$. We show that $\mathcal{D}$ contains $\sigma(\mathcal{A})$. To do so, we will show that $\mathcal{D}$ is a $\pi$-system, hence a $\sigma$-algebra.

  We let
  \[
    \mathcal{D}' = \{ B \in \mathcal{D}: B \cap A \in \mathcal{D}\text{ for all }A \in \mathcal{A}\}.
  \]
  We note that $\mathcal{D}' \supseteq \mathcal{A}$ because $\mathcal{A}$ is a $\pi$-system, and is hence closed under intersections. We check that $\mathcal{D}'$ is a d-system. It is clear that $E \in \mathcal{D}'$. If we have $B_1, B_2 \in \mathcal{D}'$, where $B_1 \subseteq B_2$, then for any $A \in \mathcal{A}$, we have
  \[
    (B_2 \setminus B_1) \cap A = (B_2 \cap A) \setminus (B_1 \cap A).
  \]
  By definition of $\mathcal{D}'$, we know $B_2 \cap A$ and $B_1 \cap A$ are elements of $\mathcal{D}$. Since $\mathcal{D}$ is a d-system, we know this intersection is in $\mathcal{D}$. So $B_2 \setminus B_1 \in \mathcal{D}'$.

  Finally, suppose that $(B_n)$ is an increasing sequence in $\mathcal{D}'$, with $B = \bigcup B_n$. Then for every $A \in \mathcal{A}$, we have that
  \[
    \left(\bigcup B_n\right) \cap A = \bigcup (B_n \cap A) = B \cap A \in \mathcal{D}.
  \]
  Therefore $B \in \mathcal{D}'$.

  Therefore $\mathcal{D}'$ is a d-system contained in $\mathcal{D}$, which also contains $\mathcal{A}$. By our choice of $\mathcal{D}$, we know $\mathcal{D}' = \mathcal{D}$.

  We now let
  \[
    \mathcal{D}'' = \{B \in \mathcal{D}: B \cap A \in \mathcal{D}\text{ for all }A \in \mathcal{D}\}.
  \]
  Since $\mathcal{D}' = \mathcal{D}$, we again have $\mathcal{A} \subseteq \mathcal{D}''$, and the same argument as above implies that $\mathcal{D}''$ is a d-system which is between $\mathcal{A}$ and $\mathcal{D}$. But the only way that can happen is if $\mathcal{D}'' = \mathcal{D}$, and this implies that $\mathcal{D}$ is a $\pi$-system.
\end{proof}

Now let's starting constructing an actual, interesting measure. To do so, we will prove a useful theorem that says to specify a measure, it will suffice to specify on a special generating subset of the $\sigma$-algebra.

\begin{defi}[Set function]
  Let $\mathcal{A}$ be a collection of subsets of $E$ with $\emptyset \in \mathcal{A}$. A \term{set function} function $\mu: \mathcal{A} \to [0, \infty]$ such that $\mu(\emptyset) = 0$.
\end{defi}

\begin{defi}[Increasing set function]\index{Increasing set function}\index{set function!increasing}
  A set function is \emph{increasing} if it has the proper that for all $A, B \in \mathcal{A}$ with $A \subseteq B$, we have $\mu(A) \leq \mu(B)$.
\end{defi}
\begin{defi}[Additive set function]\index{Additive set function}\index{set function!additive}
  A set function is \emph{additive} if whenever $A, B \in \mathcal{A}$ and $A \cup B \in \mathcal{A}$, $A \cup B = \emptyset$, then $\mu(A \cup B) = \mu(A) + \mu(B)$.
\end{defi}

\begin{defi}[Countably additive set function]\index{countably additive set function}\index{set function!countably additive}
  A set function is \emph{countably additive} if whenever $A_n$ is a seqquence of disjoint sets in $\mathcal{A}$ with $\cup A_n \in \mathcal{A}$, then
  \[
    \mu\left(\bigcup_n A_n \right) = \sum_n \mu(A_n).
  \]
\end{defi}

Under these definitions, a measure is just a countable additive set function defined on a $\sigma$-algebra.

\begin{defi}[Countably subadditive set function]\index{countably subadditive set function}\index{set function!countably additive}
  A set function is \emph{countably subadditive} if whenever $(A_n)$ is a sequence of sets in $\mathcal{A}$ with $\bigcup_n A_n \in \mathcal{A}$, then
  \[
    \mu\left(\bigcup_n A_n\right) \leq \sum_n \mu(A_n).
  \]
\end{defi}

\begin{defi}[Ring]\index{ring}
  A collection of subsets $\mathcal{A}$ is a \emph{ring} on $E$ if $\emptyset \in A$ and for all $A, B \in \mathcal{A}$, we have $B \setminus A \in \mathcal{A}$ and $A \cup B \in \mathcal{A}$.
\end{defi}

\begin{defi}[Algebra]\index{algebra}
  A collection of subsets $\mathcal{A}$ is an \emph{algebra} on $E$ if $\emptyset \in A$, and for all $A, B \in \mathcal{A}$, we have $A^C \in \mathcal{A}$ and $A \cup B \in \mathcal{A}$.
\end{defi}
So an algebra is like a $\sigma$-algebra, but it is just closed under finite unions only, rather than countable unions.

The big theorem that allows us to construct measures is the Caratheodory extension theorem.
\begin{thm}[Caratheodory extension theorem]\index{Caratheodory extension theorem}
  Let $\mathcal{A}$ be a ring on $E$, and $\mu$ a countably additive set function on $\mathcal{A}$. Then $\mu$ extends to a measure on the $\sigma$-algebra generated by $\mathcal{A}$.
\end{thm}

\begin{proof}(non-examinable)
  We start by defining what we want our measure to be. For $B \subseteq E$, we set
  \[
    \mu^*(B) = \inf\left\{\sum_n\mu(A_n): (A_n) \in \mathcal{A}\text{ and } B\subseteq \bigcup A_n\right\}.
  \]
  If it happens that there is no such sequence, we set this to be $\infty$. This measure is known as the \term{outer measure}. It is clear that $\mu^*(\phi) = 0$, and that $\mu^*$ is increasing.

  We say a set $A \subseteq E$ is $\mu^*$-measurable if
  \[
    \mu^*(B) = \mu^*(B \cap A) + \mu^*(B \cap A^C)
  \]
  for all $B \subseteq E$. We let
  \[
    \mathcal{M} = \{\text{$\mu^*$-measurable sets}\}.
  \]
  We will show the following:
  \begin{enumerate}
    \item $\mathcal{M}$ is a $\sigma$-algebra containing $\mathcal{A}$.
    \item $\mu^*$ is a measure on $\mathcal{M}$ with $\mu^*|_{\mathcal{A}} = \mu$.
  \end{enumerate}
  Note that it is not true in general that $\mathcal{M} = \sigma(A)$. However, we will always have $M \supseteq \sigma(\mathcal{A})$.

  We are going to break this up into five nice bitesize chunks.

  \begin{claim}
    $\mu^*$ is countably subadditive.
  \end{claim}
  Suppose $B\subseteq \bigcup_n B_n$. We need to show that $\mu^*(B) \leq \sum_n \mu^*(B_n)$. We can wlog assume that $\mu^*(B_n)$ is finite for all $n$, or else the inequality is trivial. Let $\varepsilon > 0$. Then by definition of the outer measure, for each $n$, we can find a sequence $(B_{n, m})_{m = 1}^\infty$ in $\mathcal{A}$ with the property that
  \[
    B_n \subseteq \bigcup_m B_{n, m}
  \]
  and
  \[
    \mu^*(B_n) + \frac{\varepsilon}{2^n} \geq \sum_m \mu(B_{n, m}).
  \]
  Then we have
  \[
    B \subseteq \bigcup_n B_n \subseteq \bigcup_{n, m}B_{n, m}.
  \]
  Thus, by definition, we have
  \[
    \mu^*(B) \leq \sum_{n, m}\mu^*(B_{n, m}) \leq \sum_n \left(\mu^*(B_n) + \frac{\varepsilon}{2^n}\right) = \varepsilon + \sum_n \mu^*(B_n).
  \]
  Since $\varepsilon$ was arbitrary, we are done.
  \begin{claim}
    $\mu^*$ agrees with $\mu$ on $\mathcal{A}$.
  \end{claim}
  In the first example sheet, we will show that if $\mathcal{A}$ is a ring and $\mu$ is a countably addiive set function on $\mu$, then $\mu$ is in fact countably subadditive and increasing.

  Assuming this, suppose that $A, (A_n)$ are in $\mathcal{A}$ and $A \subseteq \bigcup_n A_n$. Then by subadditivity, we have
  \[
    \mu(A) \leq \sum_n \mu(A \cap A_n) \leq \sum_n \mu(A_n),
  \]
  using that $\mu$ is countably subadditivity and increasing. Note that we have to do this in two steps, rather than just applying countable subadditivity, since we did not assume that $\bigcup_n A_n \in \mathcal{A}$. Taking the infimum over all sequences, we have
  \[
    \mu(A) \leq \mu^*(A).
  \]
  Also, we see by definition that $\mu(A) \geq \mu^*(A)$, since $A$ covers $A$. So we get that $\mu(A) = \mu^*(A)$ for all $A \in \mathcal{A}$.

  \begin{claim}
    $\mathcal{M}$ contains $\mathcal{A}$.
  \end{claim}
  Suppose that $A \in \mathcal{A}$ and $B \subseteq E$. We need to show that
  \[
    \mu^*(B) = \mu^*(B \cap A) + \mu^*(B \cap A^C).
  \]
  Since $\mu^*$ is countably subadditive, we immediately have $\mu^*(B) \leq \mu^*(B \cap A) + \mu^*(B \cap A^C)$. For the other inequality, we first observe that it is trivial if $\mu^*(B)$ is infinite. If it is finite, then by definition, given $\varepsilon > 0$, we can find some $(B_n)$ in $\mathcal{A}$ such that $B \subseteq \bigcup_n B_n$ and
  \[
    \mu^*(B) + \varepsilon \geq \sum_n \mu(B_n).
  \]
  Then we have
  \begin{align*}
    B \cap A &\subseteq \bigcup_n (B_n \cap A)\\
    B \cap A^C &\subseteq \bigcup_n (B_n \cap A^C)
  \end{align*}
  We notice that $B_n \cap A^C = B_n \setminus A \in \mathcal{A}$. Thus, by definition of $\mu^*$, we have
  \begin{align*}
    \mu^* (B \cap A) + \mu^*(B \cap A^c) &\leq \sum_n \mu(B_n \cap A) + \sum_n \mu(B_n \cap A^C)\\
    &= \sum_n (\mu(B_n \cap A) + \mu(B_n \cap A^C))\\
    &= \sum_n \mu(B_n) &\\
    &\leq \mu^*(B_n) + \varepsilon.
  \end{align*}
  Since $\varepsilon$ was arbitrary, the result follows.

  \begin{claim}
    We show that $\mathcal{M}$ is an algebra.
  \end{claim}
  We first show that $E \in \mathcal{M}$. This is true since iwe obviously have
  \[
    \mu^*(B) = \mu^*(B \cap E) + \mu^*(B \cap E^C)
  \]
  for all $B \subseteq E$.

  Next, note that if $A \in \mathcal{M}$, then by definition we have
  \[
    \mu^*(B) = \mu^*(B \cap A) + \mu^*(B \cap A^C).
  \]
  Now note that this definition is symmetric in $A$ and $A^C$. So we also have $A^C \in M$.

  Finally, we have to show that $\mathcal{M}$ is closed under intersection (which is equivalent to being closed under union when we have complements). Suppose $A_1, A_2 \in \mathcal{M}$ and $B \subseteq E$. Then we have
  \begin{align*}
    \mu^*(B) ={}& \mu^*(B \cap A_1) + \mu^*(B \cap A_1^C)\\
    ={}& \mu^*(B \cap A_1 \cap A_2) + \mu^*(B \cap A_1 \cap A_2^C) + \mu^*(B \cap A_1^C)\\
    ={}& \mu^*(B \cap (A_1 \cap A_2)) + \mu^*(B \cap (A_1\cap A_2)^C \cap A_1) \\
    &+ \mu^*(B \cap (A_1 \cap A_2)^C \cap A_1^C)\\
    ={}& \mu^*(B \cap (A_1 \cap A_2)) + \mu^*(B \cap (A_1 \cap A_2)^C).
  \end{align*}
  So we have $A_1 \cap A_2 \in \mathcal{M}$. So $\mathcal{M}$ is an algebra.
  \begin{claim}
    $\mathcal{M}$ is a $\sigma$-algebra, and $\mu^*$ is a measure on $\mathcal{M}$.
  \end{claim}
  To show that $\mathcal{M}$ is a $\sigma$-algebra, we need to show that it is closed under countable unions. We let $(A_n)$ be a disjoint collection of sets in $\mathcal{M}$, then we want to show that $A = \bigcup_n A_n \in \mathcal{M}$ and $\mu^*(A) = \sum_n \mu^*(A_n)$.

  Suppose that $B \subseteq E$. Then we have
  \begin{align*}
    \mu^*(B) &= \mu^*(B \cap A_1) + \mu^*(B \cap A_1^C)\\
    \intertext{Using the fact that $A_2 \in \mathcal{M}$ and $A_1 \cap A_2 =\emptyset$, we have}
    &= \mu^*(B \cap A_1) + \mu^*(B \cap A_2) + \mu^*(B \cap A_1^C \cap A_2^C)\\
    &= \cdots\\
    &= \sum_{i = 1}^n \mu^*(B\cap A_i) + \mu^*(B \cap A_1^C \cap \cdots \cap A_n^C)\\
    &\geq \sum_{i = 1}^n \mu^*(B \cap A_i) + \mu^*(B \cap A^C).
  \end{align*}
  Taking the limit as $n \to \infty$, we have
  \[
    \mu^*(B) \geq \sum_{i = 1}^\infty \mu^*(B \cap A_i) + \mu^*(B \cap A^C).
  \]
  By the countable-subadditivity of $\mu^*$, we have
  \[
    \mu^*(B \cap A) \leq \sum_{i = 1}^\infty \mu^*(B \cap A_i).
  \]
  Thus we obtain
  \[
    \mu^*(B) \geq \mu^*(B \cap A) + \mu^*(B \cap A^C).
  \]
  By countable subadditivity, we also have inequality in the other direction. So equality holds. So $A \in \mathcal{M}$. So $\mathcal{M}$ is a $\sigma$-algebra.

  To see that $\mu^*$ is a measure on $\mathcal{M}$, noet that the above implies that
  \[
    \mu^*(B) = \sum_{i = 1}^\infty (B \cap A_i) + \mu^*(B \cap A^C).
  \]
  Taking $B = A$, this gives
  \[
    \mu^*(A) = \sum_{i = 1}^\infty (A \cap A_i) + \mu^*(A \cap A^C) = \sum_{i = 1}^\infty \mu^*(A_i).
  \]
\end{proof}
\subsection{Uniqueness of measure}
\begin{thm}
  Suppose that $\mu_1, \mu_2$ are measures on $(E, \mathcal{E})$ with $\mu_1(E) = \mu_2(E) + \infty$. If $\mathcal{A}$ is a $\pi$-system with $\sigma(A) = \mathcal{E}$, and $\mu_1$ agrees with $\mu_2$ on $\mathcal{A}$, then $\mu_1 = \mu_2$.
\end{thm}

\begin{proof}
  Let
  \[
    \mathcal{D} = \{A \in \mathcal{E}: \mu_1(A) = \mu_2(A)\}
  \]
  We know that $\mathcal{D} \supseteq \mathcal{A}$. By Dynkin's lemma, it suffices to show that $\mathcal{D}$ is a d-system. The things to check are:
  \begin{enumerate}
    \item $E \in \mathcal{D}$ --- this is obvious.
    \item If $A, B \in \mathcal{D}$ with $A \subseteq B$, then $B \setminus A \in \mathcal{D}$. Indeed, we have the equations
      \begin{align*}
        \mu_1(B) &= \mu_1(A) + \mu_1(B \setminus A) < \infty\\
        \mu_2(B) &= \mu_2(A) + \mu_2(B \setminus A) < \infty.
      \end{align*}
      Since $\mu_1(B) = \mu_2(B)$ and $\mu_1(A) = \mu_2(A)$, we must have $\mu_1(B \setminus A) = \mu_2(B \setminus A)$.
    \item Let $(A_n) \in \mathcal{D}$ be an increasing sequence with $\bigcup A_n = A$. Then
      \[
        \mu_1(A) = \lim_{n \to \infty}\mu_1(A_n) = \lim_{n \to \infty} \mu_2(A_n) = \mu_2(A).
      \]
      So $A \in\mathcal{D}$.
  \end{enumerate}
\end{proof}
The assumption that $\mu_1(E) = \mu_2(E) < \infty$ is necessary. The theorem does not necessarily hold without it. We can see this from a simple counterexample:

\begin{eg}
  Let $E = \Z$, and let $\mathcal{E} = P(E)$. We let
  \[
    \mathcal{A} = \{\{x, x+1, x+2, \cdots\}: x \in E\} \cup \{\emptyset\}.
  \]
  This is a $\pi$-system with $\sigma(A) = \mathcal{E}$. We let $\mu_1(A)$ be the number of elements in $A$, and $\mu_2 = 2\mu_1(A)$. Then obviously $\mu_1 \not= \mu_2$, but $\mu_1(A) = \infty = \mu_2(A)$ for $A \in \mathcal{A}$.
\end{eg}

\begin{defi}[Borel $\sigma$-algebra]\index{Borel $\sigma$-algebra}\index{$\mathcal{B}(E)$}
  Let $E$ be a topological space. We define the \emph{Borel $\sigma$-algebra} as
  \[
    \mathcal{B}(E) = \sigma(\{U \subseteq \mathcal{E}: U \text{ is open}\}).
  \]
  We write \term{$\mathcal{B}$} for $\mathcal{B}(\R)$.
\end{defi}

\begin{defi}[Borel measure and Radon measure]\index{Borel measure}
  A measure $\mu$ on $(E, \mathcal{B}(E))$ is called a \emph{Borel measure}. If $\mu(K) < \infty$ for all $K \subseteq E$ compact, then $\mu$ is a \term{Radon measure}.
\end{defi}
The most important example of a Lebesgue measure we will consider is the \emph{Lebesgue measure}.

\begin{thm}
  There exists a unique Borel measure $\mu$ on $\R$ with $\mu([a, b]) = b - a$.
\end{thm}

\begin{proof}
  We first show uniqueness. Suppose $\tilde{\mu}$ is another measure on $\mathcal{B}$ satisfying the above property. We want to apply the previous uniqueness theorem, but our measure is not finite. So we need to carefully get around that problem.

  For each $n \in \Z$, we set
  \begin{align*}
    \mu_n(A) &= \mu(A \cap (n, n + 1]))\\
    \tilde{\mu}_n(A) &= \tilde{\mu}(A \cap (n, n + 1]))
  \end{align*}
  Then $\mu_n$ and $\tilde{\mu}_n$ are finite measures on $\mathcal{B}$ which agree on the $\pi$-system of intervals of the form $(a, b]$ with $a, b \in \R$, $a < b$. Therefore we have $\mu_n = \tilde{\mu}_n$ for all $n \in \Z$. Now we have
  \[
    \mu(A) = \sum_{n \in \Z} \mu(A \cap (n, n + 1]) = \sum_{n\in \Z}\mu_n(A) = \sum_{n \in \Z}\tilde{\mu}_n(A) = \tilde{\mu}(A)
  \]
  for all Borel sets $A$.

  To show existence, we want to use the Caratheodory extension theorem. We let $\mathcal{A}$ be the collection of finite, disjoint unions of the form
  \[
    A = (a_1, b_1] \cup (a_2, b_2] \cup \cdots \cup (a_n, b_n].
  \]
  Then $\mathcal{A}$ is a ring of subsets of $R$, and $\sigma(A) = \mathcal{B}$ (details are to be checked on the first example sheet).

  We set
  \[
    \mu(A) = \sum_{i = 1}^n (b_i - a_i).
  \]
  We note that $\mu$ is well-defined, since if
  \[
    A = (a_1, b_1] \cup \cdots \cup (a_n, b_n] = (\tilde{a}_1, \tilde{b}_1] \cup \cdots \cup (\tilde{a}_n, \tilde{b}_n],
  \]
  then
  \[
    \sum_{i = 1}^n (b_i - a_i) = \sum_{i = 1}^n (\tilde{b}_i - \tilde{a}_i).
  \]
  Also, if $\mu$ is additive, $A, B \in \mathcal{A}$, $A \cap B = \emptyset$ and $A \cup B \in \mathcal{A}$, we obviously have $\mu(A \cup B) = \mu(A) + \mu(B)$. So $\mu$ is additive.

  Finally, we have to show that $\mu$ is in fact countably additive. Let $(A_n)$ be a disjoint sequence in $\mathcal{A}$, and let $\bigcup_{i = 1}^\infty A_n \in \mathcal{A}$. Then we need to show that $\mu(A) = \sum_{n = 1}^\infty \mu(A_n)$.

  Since $\mu$ is additive, we have
  \begin{align*}
    \mu(A) &= \mu(A_1) + \mu(A \setminus A_1) \\
    &= \mu(A_1) + \mu(A_2) + \mu(A \setminus A_1 \cup A_2)\\
    &= \sum_{i = 1}^n \mu(A_i) + \mu\left(A \setminus \bigcup_{i = 1}^n A_i\right)
  \end{align*}
  To finish the proof, we show that
  \[
    \mu\left(A \setminus \bigcup_{i = 1}^n A_i\right) \to 0\text{ as }n \to \infty.
  \]
  We are going to reduce this to the \term{finite intersection property} of compact sets in $\R$: if $(K_n)$ is a sequence of compact sets in $\R$ with the property that $\bigcap_{m = 1}^n K_m \not= \emptyset$ for all $n$, then $\bigcap_{m = 1}^\infty K_m \not= \emptyset$.

  We first introduce some new notation. We let
  \[
    B_n = A \setminus \bigcup_{m = 1}^n A_m.
  \]
  We now suppose, for contradiction, that $\mu(B_n) \not\to 0$ as $n \to \infty$. Since the $B_n$'s are decreasing, there must exist $\varepsilon > 0$ such that $\mu(B_n) \geq 2 \varepsilon$ for every $n$.

  For each $n$, we take $C_n \in \mathcal{A}$ with the property that $\overline{C_n} \subseteq B_n$ and $\mu(B_n \setminus C_n) \leq \frac{\varepsilon}{2^n}$. This is possible since each $B_n$ is just a finite union of intervals. Thus we have
  \begin{align*}
    \mu(B_n) - \mu\left(\bigcap_{m = 1}^n C_m\right) &= \mu\left(B_n \setminus \bigcap_{m = 1}^n C_m\right) \\
    &\leq \mu\left(\bigcup_{m = 1}^n (B_m \setminus C_m)\right) \\
    &\leq \sum_{m = 1}^n \mu(B_m \setminus C_m) \\
    &\leq \sum_{m = 1}^n \frac{\varepsilon}{2^m} \\
    &\leq \varepsilon.
  \end{align*}
  On the other hand, we also know that $\mu(B_n) \geq 2\varepsilon$.
  \[
    \mu\left(\bigcap_{m = 1}^n C_m\right) \geq \varepsilon
  \]
  for all $n$. We now let that $K_n = \bigcap_{m = 1}^n \overline{C_m}$. Then $\mu(K_n) \geq \varepsilon$, and in particular $K_n \not= \emptyset$ for all $n$.

  Thus, the finite intersection property says
  \[
    \emptyset \not= \bigcap_{n = 1}^\infty K_n \subseteq \bigcap_{n = 1}^\infty B_n = \emptyset.
  \]
  This is a contradiction. So we have $\mu(B_n) \to 0$ as $n \to \infty$. So done.
\end{proof}

\begin{defi}[Lebesgue measure]\index{Lebesgue measure}
  The \emph{Lebesgue measure} is the unique Borel measure $\mu$ on $\R$ with $\mu([a, b]) = b - a$.
\end{defi}

Note that the Lebesgue measure is not a finite measure, since $\mu(\R) = \infty$. However, it is a $\sigma$-finite measure.
\begin{defi}[$\sigma$-finite measure]\index{$\sigma$-finite measure}
  Let $(E, \mathcal{E})$ be a measurable space, and $\mu$ a measure. We say $\mu$ is \emph{$\sigma$-finite} if there exists a sequence $(E_n)$ in $\mathcal{E}$ such that $\bigcup_n E_n = E$ and $\mu(E_n) < \infty$ for all $n$.
\end{defi}

\begin{prop}
  The Lebesgue measure is \term{translation invariant}, ie.
  \[
    \mu(A + x) = \mu(A)
  \]
  for all $A \in \mathcal{B}$ and $x \in \R$, where
  \[
    A + x = \{y + x, y \in A\}.
  \]
\end{prop}

\begin{proof}
  We use the uniqueness of the Lebesgue measure. We let
  \[
    \mu_x (A) = \mu(A + x)
  \]
  for $A \in \mathcal{B}$. Then this is a measure on $\mathcal{B}$ satisfying $\mu_x([a, b]) = b - a$. So the uniqueness of the Lebesgue measure shows that $\mu_x = \mu$.
\end{proof}

It turns out that translation invariance actually characterizes the Lebesgue measure.

\begin{prop}
  Let $\tilde{\mu}$ be a Borel measure on $\R$ that is translation invariant and $\mu([0, 1]) = 1$. Then $\tilde{\mu}$ is the Lebesgue measure.
\end{prop}

\begin{proof}
  By additivity and translation invariance, we can show that $\mu([p, q]) = q - p$ for all rational $p < q$. By considering $\mu([p, p + 1/n])$ for all $n$ and using the increasing property, we know $\mu(\{p\}) = 0$. So $\mu(([p, q)) = \mu((p, q]) = \mu((p, q)) = q - p$ for all rational $p, q$.

  Finally, by countable additivity, we can extend this to all real intervals. Then the result follows from the uniqueness of the Lebesgue measure.
\end{proof}

In the proof of the Caratheodory extension theorem, we constructed a measure $\mu^*$ on the $\sigma$-algebra $\mathcal{M}$ of $\mu^*$-measurable sets which contains $\mathcal{A}$. This contains $\mathcal{B} = \sigma(A)$, but could in fact be bigger than it. We call $\mathcal{M}$ the \term{Lebesgue $\sigma$-algebra}.

Indeed, it can be given by
\[
  \mathcal{M} = \{A \cup N: A \in \mathcal{B}, N \subseteq B \in \mathcal{B}\text{ with }\mu(B) = 0\}.
\]
If $A \cup N \in \mathcal{M}$, then $\mu(A \cup N) = \mu(A)$. The proof is left for the example sheet.

It is also true that $\mathcal{M}$ is strictly larger than $\mathcal{B}$, so there exists $A \in \mathcal{M}$ with $A \not\in \mathcal{B}$. Construction of such a set was on last year's exam (2016).

On the other hand, it is also true that not all sets are Lebesgue measurable. This is a rather funny construction.

\begin{eg}
  For $x, y \in [0, 1)$, we say $x \sim y$ if $x - y$ is rational. This defines an equivalence relation on $[0, 1)$. By the axiom of choice, we pick a representative of each equivalence class, and put them into a set $S \subseteq [0, 1)$. We will show that $S$ is not Lebesgue measurable.

  Suppose that $S$ were Lebesgue measurable. We are going to get a contradiction to the countable additivity of the Lebesgue measure. For each rational $r \in [0, 1) \cap \Q$, we define
  \[
    S_r = \{s + r \bmod 1: s \in S\}.
  \]
  By translation invariance, we know $S_r$ is also Lebesgue measurable, and $\mu(S_r) = \mu(S)$.

  Also, by construction of $S$, we know $(S_r)_{r \in \Q}$ is disjoint, and $\bigcup_{r \in \Q} S_r = [0, 1)$. Now by countable additivity, we have
  \[
    1 = \mu([0, 1)) = \mu\left(\bigcup_{r \in \Q}S_r\right) = \sum_{r \in \Q}\mu(S_r) = \sum_{r \in \Q}\mu(S),
  \]
  which is clearly not possible. Indeed, if $\mu(S) = 0$, then this says $1 = 0$; If $\mu(S) > 0$, then this says $1 = \infty$. Both are absurd.
\end{eg}

\section{Probability measures and independence}
Since the course is called ``probability and measure'', we'd better start talking about probability!

\begin{defi}[Probability measure and probability space]
  Let $(E, \mathcal{E})$ be a measure space with the property that $\mu(E) = 1$. Then we often call $\mu$ a \term{probability measure}, and $(E, \mathcal{E}, \mu)$ a \term{probability space}.
\end{defi}

Probability spaces are usually written as $(\Omega, \mathcal{F}, \P)$ instead.

\begin{defi}[Sample space]
  In a probability space $(\Omega, \mathcal{F}, \P)$, we often call $\Omega$ the \term{sample space}.
\end{defi}

\begin{defi}[Events]
  In a probability space $(\Omega, \mathcal{F}, \P)$, we often call the elements of $\mathcal{F}$ the \term{events}.
\end{defi}

\begin{defi}[Probaiblity]\index{probability}
  In a probability space $(\Omega, \mathcal{F}, \P)$, if $A \in \mathcal{F}$, we often call $\P[A]$ the \term{probability} of the event $A$.
\end{defi}

These are exactly the same things as measures, but with different names! However, thinking of them as probabilities could make us ask different questions about these measure spaces. For example, in probability, one is often interested in \emph{independence}.

\begin{defi}[Independence of events]\index{independent!events}\index{event!independent}
  A sequence of events $(A_n)$ is said to be \emph{independent} if
  \[
    \P\left[\bigcap_{n \in J} A_n\right] = \prod_{n \in J} \P[A_n]
  \]
  for all finite subsets $J \subseteq \N$.
\end{defi}

More generally, we can talk about independence of $\sigma$-algebras.

\begin{defi}[Independence of $\sigma$-algebras]\index{independent!$\sigma$-algebras}\index{$\sigma$-algebra!independent}
  A sequence of $\sigma$-algebras $(\mathcal{A}_n)$ with $\mathcal{A}_n \subseteq \mathcal{F}$ for all $n$ is said to be independent if the following is true: If $(A_n)$ is a sequence where $A_n \in \mathcal{A}_n$ for all $n$, them $(A_n)$ is independent.
\end{defi}

\begin{thm}\index{$\pi$-system}
  Suppose $\mathcal{A}_1$ and $\mathcal{A}_2$ are $\pi$-systems in $\mathcal{F}$. If
  \[
    \P[A_1 \cap A_2] = \P[A_1] \P[A_2]
  \]
  for all $A_1 \in \mathcal{A}_1$ and $A_2 \in \mathcal{A}_2$, then $\sigma(\mathcal{A}_1)$ and $\sigma(\mathcal{A}_2)$ are independent.
\end{thm}

\begin{proof}
  This will follow from two applications of the fact that a finite measure is determined by its values on a $\pi$-system which generates the entire $\sigma$-algebra.

  We first fix $A_1 \in \mathcal{A}_2$. We define the measures
  \[
    \mu(A) = \P[A \cap A_1]
  \]
  and
  \[
    \nu(A) = \P[A_1] \P[A]
  \]
  for all $A \in \mathcal{F}$. By assumption, we know $\mu$ and $\nu$ agree on $A_2$, and we have that $\mu(\Omega) = \P[A_1] = \nu(\Omega) \leq 1 < \infty$. So $\mu$ and $\nu$ agree on $\sigma(\mathcal{A}_2)$. So we have
  \[
    \P[A_1 \cap A_2] = \mu(A_2) = \nu(A_2) = \P[A_1] \P[A_2]
  \]
  for all $A_2 \in \sigma(\mathcal{A}_2)$.

  So we have now shown that if $\mathcal{A}_1$ and $\mathcal{A}_2$ are independent, then $\mathcal{A}_1$ and $\sigma(\mathcal{A}_2)$ are independent. By symmetry, the same argument shows that $\sigma(\mathcal{A}_1)$ and $\sigma(\mathcal{A}_2)$ are independent.
\end{proof}

\subsection{Borel--Cantelli lemma}
We are now going to prove two useful results.

\begin{defi}[limsup and liminf]\index{$\limsup$}\index{$\liminf$}
  Let $(A_n)$ be a sequence of events. We define
  \begin{align*}
    \limsup A_n &= \bigcap_n \bigcup_{m \geq n} A_m\\
    \liminf A_n &= \bigcup_n \bigcap_{m \geq n} A_m.
  \end{align*}
\end{defi}
To parse these definitions more easily, we can read $\cap$ as ``for all'', and $\cup$ as ``there exits''. For example, we can write
\begin{align*}
  \limsup A_n &= \forall n,\exists m \geq n\text{ such that }A_m\text{ occurs}\\
  &= \{x: \forall n, \exists m \geq n, x \in A_m\}\\
  &= \{A_m\text{ occurs infinitely often}\}\\
  &= \{A_m \text{ i.o.}\}
\end{align*}
Similarly, we have
\begin{align*}
  \lim\inf A_n &= \exists n, \forall m \geq n\text{ such that }A_m\text{ occurs}\\
  &= \{x: \exists n, \forall m \geq n, x \in A_m\}\\
  &= \{A_m\text{ occurs eventually}\}\\
  &= \{A_m\text{ e.v.}\}
\end{align*}

\begin{lemma}[Borel--Cantelli lemma]\index{Borel--Cantelli lemma}
  If
  \[
    \sum_n \P[A_n] < \infty,
  \]
  then
  \[
    \P[A_n\text{ i.o.}] = 0.
  \]
\end{lemma}

\begin{proof}
  For each $n$, we have
  \begin{align*}
    \P[A_n\text{ i.o}] &= \P\left[\bigcap_n \bigcup_m \geq n A_m\right]\\
    &\leq \P\left[\bigcup_{m \geq n} A_m\right]\\
    &\leq \sum_{m = n}^\infty \P[A_m]\\
    &\to 0
  \end{align*}
  as $n \to \infty$. So we have $\P[A_n\text{ i.o.}] = 0$.
\end{proof}

The natural thing to ask is if there is a converse to this. It turns out it does, if we assume the events are independent.

\begin{lemma}[Borel--Cantelli lemma II]\index{Borel--Cantelli lemma II}
  Let $(A_n)$ Be independent events. If
  \[
    \sum_n \P[A_n] = \infty,
  \]
  then
  \[
    \P[A_n\text{ i.o.}] = 1.
  \]
\end{lemma}

\begin{proof}
  By Example Sheet, if $(A_n)$ is independent, then so is $(A_n^C)$. Then we have
  \begin{align*}
    \P\left[\bigcap_{m = n}^N A_m^C\right] &= \prod_{m = n}^N \P[A_m^C]\\
    &= \prod_{m = n}^N (1 - \P[A_m])\\
    &\leq \prod_{m = n}^N \exp(-\P[A_m])\\
    &= \exp\left(- \sum_{m = n}^N \P[A_m]\right)\\
    &\to 0
  \end{align*}
  as $N \to \infty$, as we assumed that $\sum_n \P[A_n] = \infty$. So we have
  \[
    \P\left[\bigcap_{m = n}^\infty A_m^C\right] = 0.
  \]
  By countable subadditivity, we have
  \[
    \P\left[\bigcup_n \bigcap_{m = n}^\infty A_m^C\right] = 0.
  \]
  This in turn implies that
  \[
    \P\left[\bigcap_n \bigcup_{m = n}^\infty A_m\right] = 1 - \P\left[\bigcup_n \bigcap_{m = n}^\infty A_m^C\right] = 1.
  \]
  So we are done.
\end{proof}

\section{Measurable functions and random variables}
We've had enough of measurable sets. We are now going to talk about measurable functions. A measurable function will be a functions for which it makes sense to talk about $\mu(f^{-1}(A))$. We are going to have the monotone class theorem, which is the analogue of Dynkin's lemma for measurable functions. We will also talk about the probabilistic aspects, and in particular random variables and their independence. Finally, we are going to talk about convergence in the sense of measure theory.

\subsection{Measurable functions}
\begin{defi}[Measurable functions]\index{measurable function}
  Let $(E, \mathcal{E})$ and $(G, \mathcal{G})$ be measure spaces. A map $f: E \to G$ is \emph{measurable} if for every $A \in \mathcal{G}$, we have
  \[
    f^{-1}(A) = \{x \in E: f(x) \in E\} \in \mathcal{E}.
  \]
  If $(G, \mathcal{G}) = (\R, \mathcal{B})$, then we will just say that $f$ is measurable on $E$.

  If $(G, \mathcal{G}) = ([0, \infty], \mathcal{B})$, then we will just say that $f$ is \index{non-negative measurable function}\emph{non-negative measurable}.

  If $E$ is a topological space and $\mathcal{E} = \mathcal{B}(E)$, then we call $f$ a \term{Borel function}.
\end{defi}

How do we actually check in practice that a function is measurable? We do not need to check that $f^{-1}(A) \in \mathcal{E}$ for all $A \in \mathcal{G}$. We only need to check it for a generating subset. This is since
\[
  \mathcal{A} = \{f^{-1}(A): A \in \mathcal{G}\}
\]
is going to be a $\sigma$-algebra on $E$ as $f^{-1}$ preserves everything. More precisely, we have
\[
  f^{-1}\left(\bigcup_n A_n\right) = \bigcup_n f^{-1}(A_n),\quad f^{-1}(A^C) = (f^{-1}(A))^C,\quad f^{-1}(\emptyset) = \emptyset.
\]
So $\mathcal{A}$ is a $\sigma$-algebra on $E$.

For the same reason, we know
\[
  \{A: f^{-1}(A) \in \mathcal{E}\}
\]
is also a $\sigma$-algebra on $G$.

So suppose that $G = \sigma(\mathcal{Q})$ for some $\mathcal{Q}$, and $f^{-1}(A) \in \mathcal{E}$ for all $A \in \mathcal{Q}$. Then we have
\[
  \{A: f^{-1}(A) \in \mathcal{E}\}
\]
is a $\sigma$-algebra that contains $\mathcal{Q}$, and thus contains $\mathcal{G}$. So $f$ is measurable. Thus, we have shown that
\begin{lemma}
  Let $(E, \mathcal{E})$ and $(G, \mathcal{G})$ be measurable spaces, and $\mathcal{G} = \sigma(\mathcal{Q})$ for some $\mathcal{Q}$. If $f^{-1}(A) \in \mathcal{E}$ for all $A \in \mathcal{Q}$, then $f$ is measurable.
\end{lemma}

In the particular case where we have a function $f: E \to \R$, we know that $\mathcal{B} = \mathbf{B}(\R)$ is generated by $(-\infty, y]$ for $y \in \R$. So we just have to check that
\[
  \{x \in E: f(x) \leq y\} = f^{-1}((-\infty, y])) \in \mathcal{E}.
\]
With this in mind, we can consider the following examples of measurable functions.

\begin{eg}
  Let $E, F$ be topological spaces, and $f: E \to F$ be continuous. We will see that $f$ is a measurable function (under the Borel $\sigma$-algebras). Indeed, by definition, whenever $U \subseteq F$ is open, we have $f^{-1}(U)$ open as well. So $f^{-1}(U) \in \mathcal{B}(E)$ for all $U \subseteq F$ open. But since $\mathcal{B}(F)$ is the $\sigma$-algebra generated by the open sets, this implies that $f$ is measurable.
\end{eg}

This is one very important example. We can do another very important example.

\begin{eg}
  Suppose that $A \subseteq E$. The indicator function of $A$ is $\boldsymbol1_A(x): E \to \{0, 1\}$ given b y
  \[
    \boldsymbol1_A(x) =
    \begin{cases}
      1 & x \in A\\
      0 & x \not\in A
    \end{cases}.
  \]
  Suppose we give $\{0, 1\}$ the non-trivial measure. Then $\boldsymbol1_A$ is a measurable function iff $A \in \mathcal{E}$.
\end{eg}

\begin{eg}
  The identity function is always measurable.
\end{eg}

\begin{eg}
  Composition of measurable functions are measurable. More precisely, if $(E, \mathcal{E})$, $(F, \mathcal{F})$ and $(G, \mathcal{G})$ are measurable spaces, and the functions $f: E \to F$ and $g: F \to G$ are measurable, then the composition $g \circ f: E \to G$ is measurable.

  Indeed, if $A \in \mathcal{G}$, then $g^{-1}(A) \in \mathcal{F}$, so $f^{-1}(g^{-1}(A)) \in \mathcal{E}$. But $f^{-1}(g^{-1}(A)) = (g \circ f)^{-1}(A)$. So done.
\end{eg}

\begin{defi}[$\sigma$-algebra generated by functions]\index{$\sigma$-algebra generated by functions}
  Now suppose we have a set $E$, and a family of real-valued functions $\{f_i: i \in I\}$ on $E$. We then define
  \[
    \sigma(f_i: i \in I) = \sigma(f^{-1}_i(A): A \in \mathcal{B}, i \in I).
  \]
\end{defi}
This is the smallest $\sigma$-algebra on $E$ which makes all the $f_i$'s measurable. This is analogous to the notion of initial topologies for topological spaces.

\begin{own}
  \begin{defi}[Product measurable space]\index{product measurable space}\index{product $\sigma$-algebra}\index{$\sigma$-algebra!product}\index{measurable space!product}
    Let $(E, \mathcal{E})$ and $(G, \mathcal{G})$ be measure spaces. We define the \emph{product measure space} by $(E \times G, \sigma(\mathcal{E} \times \mathcal{G}))$. More explicitly, the $\sigma$-algebra is given by
    \[
      \sigma(\{A \times B: A \in \mathcal{E}, B \in \mathcal{G}\}).
    \]
  \end{defi}

  This satisfies the following property:
  \begin{prop}
    Let $f_1: E \to F$ and $f_2: E \to G$ be functions. Then $f_1$ and $f_2$ are measurable iff $(f_1, f_2): E \to F \times G$ given by $(f_1, f_2)(x) = (f_1(x), f_2(x))$ is measurable.
  \end{prop}
\end{own}

\begin{prop}
  Let $(E, \mathcal{E})$ be a measurable space. Let $(f_n: n \in \N)$ be a sequence of non-negative measurable functions on $E$. Then the following are measurable:
  \[
    f_1 + f_2,\quad f_1f_2,\quad \inf_n f_n,\quad \sup_n f_n,\quad \liminf_n f_n,\quad \limsup_n f_n.
  \]
  The same is true with ``real'' replaced with ``non-negative'', provided the new functions are real (ie. not infinity).
\end{prop}

\begin{proof}
  Exercise on the example sheet. The idea is that wee only have to show that
  \[
    \{x \in E: f_1(x) + f_2(x) > a\} \in \mathcal{E}.
  \]
  We can then write it as
  \[
    \bigcup_{q \in \Q}\{\text{something}\}.
  \]
  We can also do it in a more abstract way, writing the sum as
  \[
    \begin{tikzcd}
      E \ar[r, "{(f_1, f_2)}"] & \lbrack 0, \infty\rbrack^2 \ar[r, "+"] & \lbrack0, \infty\rbrack.
    \end{tikzcd}
  \]
  We know the second map is continuous, hence measurable. The first function is also measurable, so the composition is also measurable.
\end{proof}

\subsection{Image measures}

\begin{defi}[Image measure]\index{image measure}
  Let $(E, \mathcal{E})$ and $(G, \mathcal{G})$ be measure spaces. Suppose $\mu$ is a measure on $\mathcal{E}$ and $f: E \to G$ is a measurable function. We define the \emph{image measure} $\nu = \mu \circ f^{-1}$ on $G$ by
  \[
    \nu(A) = \mu(f^{-1}(A)).
  \]
\end{defi}
It is a routine check that this is indeed a measure.

\begin{lemma}
  Let $g: \R \to \R$ be non-constant, non-decreasing and right continuous. We set
  \[
    g(\pm \infty) = \lim_{x \to \pm\infty} g(x).
  \]
  We set $I = (g(-\infty), g(\infty))$. Since $g$ is non-constant, this is non-empty.

  For $x \in I$, we set $f: I \to \R$ by
  \[
    f(x) = \inf\{y \in \R: x \leq g(y)\}.
  \]
  Then this is non-decreasing, left continuous and for all $x \in I$ and $y \in \R$, we have
  \[
    x \leq g(y) \Leftrightarrow f(x) \leq y.
  \]
  Thus, taking the negation of this, we have
  \[
    x > g(y) \Leftrightarrow f(x) > y.
  \]
\end{lemma}

\begin{own}
  \begin{proof}
    By the \term{adjoint functor theorem}.
  \end{proof}
\end{own}

\begin{proof}
  For $x \in I$, consider
  \[
    J_x = \{y \in \R: x \leq g(y)\}.
  \]
  Since $g$ is non-decreasing, if $y \in J_x$ and $y' \geq y$, then $y' \in J_x$. Since $g$ is right-continuous, if $y_n \in J_x$ is a decreasing sequence and $y_n \to y$, then $y \in J_x$. So we have
  \[
    J_x = [f(x), \infty).
  \]
  Thus, for $f \in \R$, we have
  \[
    x \leq g(y) \Leftrightarrow f(x) \leq y.
  \]
  So we just have to prove the remaining properties of $f$. Now for $x \leq x'$, we have $J_x \subseteq J_{x'}$. So $f(x) \leq f(x')$. So $f$ is non-decreasing.

  Similarly, if $x_n \to x$ is an increasing sequence, then we have $J_x = \bigcap_n J_{x_n}$. So $f(x_n) \to f(x)$. So this is left continuous.
\end{proof}

\begin{thm}
  Let $g: \R \to \R$ be non-constant, non-decreasing and right continuous. Then there exists a unique Radon measure $\d g$ on $\mathcal{B}$ such that
  \[
    \d g((a, b]) = g(b) - g(a).
  \]
  Moreover, we obtain all non-zero Radon measures on $\R$ in this way.
\end{thm}
We have already seen an instance of this when we $g$ was the identity function.

Given the lemma, this is very easy.
\begin{proof}
  Take $I$ and $f$ as in the previous lemma, and let $\mu$ be the restriction of the Lebesgue measure to Borel subsets of $I$. Now $f$ is measurable since it is left continuous. We define $\d g = \mu \circ f^{-1}$. Then we have
  \begin{align*}
    \d g((a, b]) &= \mu(\{x \in I: a < f(x) \leq b\}) \\
    &= \mu(\{x \in I: g(a) < x \leq g(b)\}) \\
    &= \mu((g(a), g(b)]) = g(b) - g(a).
  \end{align*}
  So $\d g$ is a Radon measure with the required property.

  There are no other such measures by the argument used for uniqueness of the Lebesgue measure.

  To show we get all non-zero Radon measures this way, suppose we have a Radon measure $\nu$ on $\R$, we want to produce a $g$ such that $\nu = \d g$. We set
  \[
    g(y) =
    \begin{cases}
      -\nu ((y, 0]) & y \leq 0\\
      \nu((0, y]) & y > 0
    \end{cases}.
  \]
  Then $\nu((a, b]) = g(b) - g(a)$. We see that $\nu$ is non-zero, so $g$ is non-constant. It is also easy to see it is non-decreasing and right continuous. So $\nu = \d g$ by continuity.
\end{proof}

\subsection{Random variables}
We are going to look at these ideas in the context of probability.

\begin{defi}[Random variable]\index{random variable}
  Let $(\Omega, \mathcal{F}, \P)$ be a probability space, and $(E, \mathcal{E})$ a measurable space. Then an \emph{$E$-valued random variable} is a measurable function $X: \Omega \to E$.

  By default, we will assume the random variables are real.
\end{defi}

\begin{defi}[Distribution]\index{distribution}
  Given a random variable $X: \Omega \to E$, the \emph{distribution} or \emph{law} of $X$ is the image measure $\mu_x: \P \circ X^{-1}$. We usually write
  \[
    \P(X \in A) = \mu_x(A) = \P(X^{-1}(A)).
  \]
\end{defi}
If $E = \R$, then $\mu_x$ is determined by its values on the $\pi$-system of intervals $(-\infty, y]$. We set
\[
  F_X(x) = \mu_X((-\infty, x]) = \P(X \leq x)
\]
This is known as the \term{distribution function} of $X$.

\begin{prop}
  We have
  \[
    F_X(x) \to
    \begin{cases}
      0 & x \to -\infty\\
      1 & x \to +\infty
    \end{cases}.
  \]
  Also, $F_X(x)$ is non-decreasing and right-continuous.
\end{prop}

We call any function $F$ with these properties a distribution function.

We now want to show that every distribution function is indeed a distribution.

\begin{prop}
  Let $F$ be any distribution function. Then there exists a probability space $(\Omega, \mathcal{F}, \P$ and a random variable $X$ such that $F_X = F$.
\end{prop}

\begin{proof}
  Take $(\Omega, \mathcal{F}, \P) = ((0, 1), \mathcal{B}(0, 1), \text{Lebesgue})$. We take $X: \Omega \to \R$ to be
  \[
    X(\omega) = \inf\{x: \omega \leq f(x)\}.
  \]
  Then we have
  \[
    X(\omega) \leq x \Longleftrightarrow w \leq F(x).
  \]
  So we have
  \[
    F_X(x) = \P[X \leq x] = \P[(0, F(x)]] = F(x).
  \]
  Therefore $F_X = F$.
\end{proof}

This construction is actually very useful in practice. If we are writing a computer program and want to sample a random variable, we will use this procedure. The computer usually comes with a uniform (pseudo)-random number generator. Then using this procedure allows us to produce random variables of any distribution from a uniform sample.

\subsection{Independence and random variables}

\begin{defi}[Independence of random variables]\index{random variable!independent}\index{independent!random variable}
  A family $(X_n)$ of random variables is said to be \emph{independent} if the family of $\sigma$-algebras $(\sigma(X_n))$ is independent.
\end{defi}
How does this compare to the notion of independence we've learnt in IA Probability?

It turns out we have the following theorem:
\begin{prop}
  Two real-valued random variables $X, Y$ are independent iff
  \[
    \P[X \leq x, Y \leq y] = \P[X \leq x] \P[Y \leq y].
  \]
  More generally, if $(X_n)$ is a sequence of real-valued random variables, then they are independent iff
  \[
    \P[x_1 \leq x_1, \cdots, x_n \leq x_n] = \prod_{j = 1}^n \P[X_j \leq x_j]
  \]
  for all $n$ and $x_j$.
\end{prop}

\begin{proof}
  See first example sheet.
\end{proof}

One question we might ask is how can we build a sequence of independent random variables?

\begin{prop}
  Let
  \[
    (\Omega, \mathcal{F}, \P) = ((0, 1), \mathcal{B}(0, 1), \text{Lebesgue}).
  \]
  be our probability space. Then there exists as sequence $R_n$ of independent $\Bernoulli(1/2)$ random variables.
\end{prop}

\begin{proof}
  Suppose we have $\omega \in \Omega = (0, 1)$. Then we write $\omega$ as a binary expansion
  \[
    \omega = \sum_{n = 1}^\infty \omega_n,
  \]
  where $\omega_n \in \{0, 1\}$. We make the binary expansion unique by disallowing infinite sequences of zeroes.

  We define $R_n(\omega) = \omega_n$. We will show that $R_n$ is measurable. Indeed, we can write
  \[
    R_1(\omega) = \omega_1 = \mathbf{1}_{(1/2, 1]}(\omega),
  \]
  where $\mathbf{1}_{(1/2, 1]}$ is the indicator function. Since indicator functions of measurable sets are measurable, we know $R_1$ is measurable. Similarly, we have
  \[
    R_2(\omega) = \mathbf{1}_{(1/4, 1/2]}(\omega) + \mathbf{1}_{(3/4, 1]}(\omega).
  \]
  So this is also a measurable function. More generally, we can do this for any $R_n(\omega)$: we have
  \[
    R_n(\omega) = \sum_{j = 1}^{2^{n - 1}} \mathbf{1}_{(2^{-n}(2j - 1), 2^{-n}(2j)]} (\omega).
  \]
  So each $R_n$ is a random variable, as each can be expressed as a sum of indicators of measurable sets.

  Now let's calculate
  \[
    \P[R_n = 1] = \sum_{j = 1}^{2n - 1} 2^{-n}((2j) - (2j - 1)) = \sum_{j = 1}^{2n - 1} 2^{-n} = \frac{1}{2}.
  \]
  Then we have
  \[
    \P[R_n = 0] = 1 - \P[R_n = 1] = \frac{1}{2}
  \]
  as well. So $R_n \sim \Bernoulli(1/2)$. We can straightforwardly check that $(R_n)$ is an independent sequence.
\end{proof}

We will now use the $(R_n)$ to construct any independent sequence for any distribution.

\begin{prop}
  Let
  \[
    (\Omega, \mathcal{F}, \P) = ((0, 1), \mathcal{B}(0, 1), \text{Lebesgue}).
  \]
  Given any sequence $(F_n)$ of distribution functions, there is a sequence $(X_n)$ of random variables with $F_{X_n} = F_n$ for all $n$.
\end{prop}

\begin{proof}
  Let $m: \N^2 \to \N$ be any bijection, and relabel
  \[
    Y_{k, n} = R_{m(k, n)},
  \]
  where the $R_j$ are as in the previous random variable. We let
  \[
    Y_n = \sum_{k = 1}^\infty 2^{-k} Y_{k, n}.
  \]
  Then we know that $(Y_n)$ is an independent sequence of random variables, and each is uniform on $(0, 1)$. As before, we define
  \[
    G_n(y) = \inf\{x: y \leq F_n(x)\}.
  \]
  We set $X_n = G_n(Y_n)$. Then $(X_n)$ is a sequence of random variables with $F_{X_n} = F_n$.
\end{proof}

Interesting fact: We let $(\Omega, \mathcal{F}, \P)$ and $R_j$ be as above. Then $\frac{1}{n} \sum_{j = 1}^n R_j$ is the average of $n$ independent of $\Bernoulli(1/2)$ random variables. The weak law of large numbers says for any $\varepsilon > 0$, we have
\[
  \P\left[\left\lvert\frac{1}{n} \sum_{j = 1}^n R_j - \frac{1}{2}\right\rvert \geq \varepsilon\right] \to 0\text{ as }n\to \infty.
\]
The strong law of large numbers, which we will prove later, says that
\[
  \P\left[\left\{\omega: \frac{1}{n} \sum_{j = 1}^n R_j \to \frac{1}{2}\right\}\right] = 1.
\]
So ``almost every number'' in $(0, 1)$ has an equal proportion of $0$'s and $1$'s in its binary expansion. This is known as the normal number theorem.

\subsection{Convergence of measurable functions}
We want to talk about the convergence of random variables, and later see how they commute with integrals.

\begin{defi}[Convergence almost everywhere]\index{convergence!almost everywhere}\index{almost everywhere}\index{almost sure convergence}\index{convergence!almost sure}
  Suppose that $(E, \mathcal{E}, \mu)$ is a measure space. Suppose that $(f_n), f$ are measurable functions. We say $f_n \to f$ \emph{almost everywhere (a.e.)} if
  \[
    \mu(\{x \in E: f_n(x) \not\to f(x)\}) = 0.
  \]
  If $(E, \mathcal{E}, \mu)$ is a probability space, this is called \emph{almost sure convergence}.
\end{defi}
To see this makes sense note that
\[
  \{x \in E: f_n(x) \not\to f(x)\} = \{x \in E: \limsup |f_n(x) - f(x)| > 0\}
\]
On the example sheet, we will see that $\limsup |f_n - f|$ is non-negative measurable. So the set $\{x \in E: \limsup |f_n(x) - f(x)| > 0\}$ is measurable. This as a weakening of the notion of pointwise convergence, where we don't even require it to converge everywhere.

\begin{defi}[Convergence in measure]\index{convergence!in measure}\index{convergence!in probability}
  Suppose that $(E, \mathcal{E}, \mu)$ is a measure space. Suppose that $(f_n), f$ are measurable functions. We say $f_n \to f$ \emph{in measure} if for each $\varepsilon > 0$, we have
  \[
    \mu(\{x \in E: |f_n(x) - f(x)| \geq \varepsilon\}) \to 0\quad\text{ as }\quad n \to \infty,
  \]
  then we say that $f_n \to f$ \emph{in measure}.

  If $(E, \mathcal{E}, \mu)$ is a probability space, then this is called \emph{convergence in probability}
\end{defi}

In general, neither of these notions imply each other. However, we have the following result:

\begin{thm}\leavevmode
  \begin{enumerate}
    \item If $\mu(E) < \infty$, then $f_n \to f$ a.e. implies $f_n \to f$ in measure.
    \item For any $E$, if $f_n \to f$ in measure, then there exists a subsequence $(f_{n_k})$ such that $f_{n_k} \to f$ a.e.
  \end{enumerate}
\end{thm}

\begin{proof}
  First suppose $\mu(E) < \infty$, and fix $\varepsilon > 0$. Consider
  \[
    \mu(\{x \in E: |f_n(x) - f(x)|\leq \varepsilon\}).
  \]
  We show that as $n \to \infty$, this tends to $\mu(E)$. First of all, we know
  \[
    \mu(\{x \in E: |f_n(x) - f(x)|\leq \varepsilon\}) \geq \mu\left(\bigcap_{m \geq n} \{x \in E: |f_m(x) - f(x)| \leq \varepsilon\}\right).
  \]
  Taking the limit as $n \to \infty$, we have
  \begin{align*}
    \mu(\{x \in E: |f_n(x) - f(x)|\leq \varepsilon\}) &\geq \mu\left(\bigcap_{m \geq n} \{x \in E: |f_m(x) - f(x)| \leq \varepsilon\}\right)\\
    &\geq \mu\left(\bigcup_n \bigcap_{m \geq n} \{x \in E: |f_m(x) - f(x)| \leq \varepsilon\}\right)\\
    &= \mu(\{x \in E: |f_m(x) - f(x)| \leq \varepsilon \text{ eventually}\})\\
    &\geq \mu(\{x \in E: |f_m(x) -f(x)| \to 0\})\\
    &= \mu(E).
  \end{align*}
  As $\mu(E) < \infty$, we have $\mu(\{x \in E: |f_n(x) - f(x)| > \varepsilon\}) \to 0$ as $n \to \infty$.

  For the second part, suppose that $f_n \to f$ in measure. We pick a subsequence $(n_k)$ such that
  \[
    \mu\left(\left\{x \in E: |f_{n_k}(x) - f(x)| > \frac{1}{k}\right\}\right) \leq 2^{-k}.
  \]
  Then we have
  \[
    \sum_{k = 1}^\infty \mu\left(\left\{x \in E: f_{n_k}(x) - f(x)|> \frac{1}{k}\right\}\right) \leq \sum_{k = 1}^\infty 2^{-k} = 1 < \infty.
  \]
  By the first Borel--Cantelli lemma, we know
  \[
    \mu\left(\left\{x \in E: |f_{n_k}(x) - f(x)| > \frac{1}{k} \text{ i.o.}\right\}\right) = 0.
  \]
  So $f_{n_k} \to f$ a.e.
\end{proof}
It is important that we assume that $\mu(E) < \infty$ for the first part.
\begin{eg}
  Consider $(E, \mathcal{E}, \mu) = (\R, \mathcal{B}, \text{Lebesgue})$. Take $f_n(x) = \mathbf{1}_{[n, \infty)}(x)$. Then $f_n(x) \to 0$ for all $x$, and in particular almost everywhere. However, we have
  \[
    \mu\left(\left\{x \in \R: |f_n(x)| > \frac{1}{2}\right\}\right) = \mu([n, \infty)) = \infty
  \]
  for all $n$.
\end{eg}

There is one last type of convergence we are interested in. We will only first formulate it in the probability setting, but there is an analogous notion in measure theory known as \emph{weak convergence}, which we will discuss much later on in the course.
\begin{defi}[Convergence in distribution]\index{convergence!in distribution}
  Let $(X_n), X$ be random variables with distribution functions $F_{X_n}$ and $F_X$, then we say $X_n \to X$ \emph{in distribution} if $F_{X_n}(x) \to F_X(x)$ for all $x \in \R$ at which $F_X$ is continuous.
\end{defi}
Note that here we do not need that $(X_n)$ and $X$ live on the same probability space, since we only talk about the distribution functions.

But why do we have the condition with continuity points? The idea is that if the resulting distribution has a ``jump'' at $x$, it doesn't matter which side of the jump $F_X(x)$ is at. Here is a simple example that tells us why this is very important:

\begin{eg}
  Let $X_n$ to be uniform on $[0, 1/n]$. Intuitively, this should converge to the random variable that is always zero.

  We can compute
  \[
    F_{X_n} (x) =
    \begin{cases}
      0 & x \leq 0\\
      nx & 0 < x< 1/n\\
      1 & x \geq 1/n
    \end{cases}.
  \]
  We can also compute the distribution of the zero random variable as
  \[
    F_0 =
    \begin{cases}
      0 & x < 0\\
      1 & x \geq 0
    \end{cases}.
  \]
  But $F_{X_n}(0) = 0 $ for all $n$, while $F_X(0) = 1$.
\end{eg}

\begin{thm}[Skorokhod representation theorem of weak convergence]\index{Skorokhod representation theorem of weak convergence}\leavevmode
  \begin{enumerate}
    \item If $(X_n), X$ are defined on the same probability space, and $X_n \to X$ in probability. Then $X_n \to X$ in distribution.
    \item If $X_n \to X$ in distribution, then there exists random variables $(\tilde{X}_n)$ and $\tilde{X}$ defined on a common probability space with $F_{\tilde{X}_n} = F_{X_n}$ and $F_{\tilde{X}} = F_X$ such that $\tilde{X}_n \to \tilde{X}$ a.s.
  \end{enumerate}
\end{thm}

\begin{proof}
  Let $S = \{x \in \R: F_X\text{ is continuous}\}$.
  \begin{enumerate}
    \item Assume that $X_n \to X$ in probability. Fix $x \in S$. We need to show that $F_{X_n}(x) \to F_X(x)$ as $n \to \infty$.

      We fix $\varepsilon > 0$. Since $x \in S$, this implies that there is some $\delta > 0$ such that
      \begin{align*}
        F_X(x - \delta) &\geq F_X(x) - \frac{\varepsilon}{2}\\
        F_X(x + \delta) & \leq F_X(x) + \frac{\varepsilon}{2}.
      \end{align*}
      We fix $N$ large such that $n \geq N$ such that $\P[|X_n - X| \geq \delta] \leq \frac{\varepsilon}{2}$. Then
      \begin{align*}
        F_{X_n}(x) &= \P[X_n \leq x] \\
        &= \P[(X_n - X) + X \leq x] \\
        \intertext{We now notice that $\{(X_n - x) + X \leq x\} \subseteq \{X \leq x + \delta\} \cup \{|X_n - X| > \delta\}$. So we have}
        &\leq \P[X \leq x + \delta] + \P[|x_n - X| > \delta]\\
        &\leq F_X(x + \delta) + \frac{\varepsilon}{2}\\
        &\leq F_X(x) + \varepsilon.
       \end{align*}
       Also, we have
       \begin{align*}
         F_{X_n}(x) &= \P[X_n \leq x] \\
         &\geq \P[X \leq x - \delta] - \P[|X_n - X| > \delta]\\
         &\geq F_X(x - \delta) - \frac{\varepsilon}{2}\\
         &\geq F_X(x) - \varepsilon.
       \end{align*}
       Combining, we have that $n \geq N$ implying $|F_{x_n}(x) - F_X(x)| \leq \varepsilon$. Since $\varepsilon$ was arbitrary, we are done.
     \item Suppose $X_n \to X$ in distribution. We again let
       \[
         (\Omega, \mathcal{F}, \mathcal{B}) = ((0, 1), \mathcal{B}((0, 1)), \text{Lebesgue}).
       \]
       We let
       \begin{align*}
         \tilde{X}_n(\omega) &= \inf\{x : \omega \leq F_{X_n}(x)\},\\
         \tilde{X}(\omega) &= \int \{x : \omega \leq F_X(x)\}.
       \end{align*}
       Recall from before that $\tilde{X}_n$ has the same distribution function as $X_n$ for all $n$, and $\tilde{X}$ has the same distribution as $X$. We are now going to show that with this particular choice, we have $\tilde{X}_n \to \tilde{X}$ a.s.

       Note that $\tilde{X}$ is a non-decreasing function $(0, 1) \to \R$. Then by general analysis, $\tilde{X}$ has at most countably many discontinuities. We write
       \[
         \Omega_0 = \{\omega \in (0, 1): \tilde{X}\text{ is continuous at }\omega_0\}.
       \]
       Then $(0, 1) \setminus \Omega$ is countable, and hence has Lebesgue measure $0$. So
       \[
         \P[\Omega_0] = 1.
       \]
       We are now going to show that $\tilde{X}_n (\omega) \to \tilde{X}(\omega)$ for all $\omega \in \Omega_0$.

       Note that $F_X$ is a non-decreasing function, and hence the points of discontinuity $\R \setminus S$ is also countable. So $S$ is dense in $\R$. Fix $\omega \in \Omega_0$ and $\varepsilon > 0$. We want to show that $|\tilde{X}_n(\omega) - \tilde{W}_\omega| \leq \varepsilon$ for all $n$ large enough.

       Since $S$ is dense in $\R$, we can find $x^-, x^+$ in $S$ such that $x^- < \tilde{X}(\omega) < x^+$ and $x^+ - x^- < \varepsilon$. We can also find $\omega^+\in (\omega, 1)$ such that $\tilde{X}(\omega^+) \leq x^+$ since $\tilde{X}$ is continuous at $\omega^+$. Then we have
       \[
         F_X(x^-) < \varepsilon
       \]
       and
       \[
         F-X(x^+) \geq \omega^+ > \omega.
       \]
       Then convergence in distribution implies that there is $N$ such that $n \geq N$ implies $F_{X_n}(x^- < \varepsilon)$, and $F_{X_n}(x^+) \geq \omega$. So we have $x^- < \tilde{X}_n(\omega) \leq x^+$. So $|\tilde{X}_n(\omega)- \tilde{X}(\omega)| < x^+ - x^- = \varepsilon$. So done.
  \end{enumerate}
\end{proof}

\begin{defi}[Tail $\sigma$-algebra]\index{tail $\sigma$-algebra}\index{$\sigma$-algebra!tail}
  Let $(X_n)$ be a sequence of random variables. We let
  \[
    \mathcal{T}_n = \sigma(X_{n + 1}, X_{n + 2}, \cdots),
  \]
  and
  \[
    \mathcal{T} = \bigcap_n \mathcal{T}_n.
  \]
  Then $\mathcal{T}$ is the \emph{tail $\sigma$-algebra}
\end{defi}
$\mathcal{T}$-measurable \index{$\mathcal{T}$-measurable} events and random variables only depend on the asymptotic behaviour of the $X_n$'s.

\begin{eg}
  Let $(X_n)$ be a sequence of real-valued random variables. Then
  \[
    \limsup_{n \to \infty} \frac{1}{n} \sum_{j = 1}^n X_j,\quad \liminf_{n \to \infty} \frac{1}{n} \sum_{j = 1}^n X_j
  \]
  are $\mathcal{T}$-measurable random variables. Finally,
  \[
    \left\{\lim_{n \to \infty} \frac{1}{n} \sum_{j = 1}^n X_j\text{ exists }\right\} \in \mathcal{T},
  \]
  since this is just the set of all points where the previous two things agree.
\end{eg}

\begin{thm}[Kolmogorov 0-1 law]\index{Kolmogorov 0-1 law}
  Let $(X_n)$ be a sequence of independent (real-valued) random variables. If $A \in \mathcal{T}$, then $\P[A] = 0$ or $1$.

  Moreover, if $X$ is a $\mathcal{T}$-measurable random variable, then there exists a constant $c$ such that
  \[
    \P[X = c] = 1.
  \]
\end{thm}

\begin{proof}
  The proof is very funny the first time we see it. We are going to prove the theorem by checking something that seems very strange. We are going to show that if $A \in \mathcal{T}$, then $A$ is independent of $A$. It then follows that
  \[
    \P[A] = \P[A \cap A] = \P[A] \P[A],
  \]
  so $\P[A] = 0$ or $1$. In fact, we are going to prove that $\mathcal{T}$ is independent of $\mathcal{T}$.

  Let
  \[
    \mathcal{F}_n = \sigma(X_1, \cdots, X_n).
  \]
  This $\sigma$-algebra is generated by the $\pi$-system of events of the form
  \[
    A = \{X_1 \leq x_1, \cdots, X_n \leq x_n\}.
  \]
  Similarly, $\mathcal{T}_n = \sigma(X_{n + 1}, X_{n + 2}, \cdots)$ is generated by the $\pi$-system of events of the form
  \[
    B = \{X_{n + 1} \leq x_{n + 1}, \cdots, X_{n + k} \leq x_{n + k}\},
  \]
  where $k$ is any natural number.

  Since the $X_n$ are independent, we know for any such $A$ and $B$, we have
  \[
    \P[A \cap B] = \P[A]\P[B].
  \]
  Since this is true for all $A$ and $B$, it follows that $\mathcal{F}_n$ is independent of $\mathcal{T}_n$.

  Since $\mathcal{T} = \bigcap_k \mathcal{T}_k \subseteq \mathcal{T}_n$ for each $n$, we know $\mathcal{F}_n$ is independent of $\mathcal{T}$.

  Now $\bigcup_k \mathcal{F}_k$ is a $\pi$-system, which generates the $\sigma$-algebra $\mathcal{F}_\infty = \sigma(X_1, X_2, \cdots)$. We know that if $A \in \bigcup_n \mathcal{F}_n$, then there has to exist an index $n$ such that $A \in \mathcal{F}_n$. So $A$ is independent of $\mathcal{T}$. So $\mathcal{F}_\infty$ is independent of $\mathcal{T}$.

  Finally, note that $\mathcal{T} \subseteq \mathcal{F}_\infty$. So $\mathcal{T}$ is independent of $\mathcal{T}$.

  Finally, suppose that $X$ is $\mathcal{T}$-measurable. Then
  \[
    \P[X \leq x] \in \{0, 1\}
  \]
  for all $x \in \R$ since $\{X \leq x\} \in \mathcal{T}$.

  Now take
  \[
    c = \inf\{x \in \R: \P[X \leq x] = 1\}.
  \]
  Then with this particular choice of $c$, it is easy to see that $\P[X = c] = 1$. This completes the proof of the theorem.
\end{proof}

We are now going to prove the monotone class theorem, which is a ``Dynkin's lemma'' for measurable functions.
\begin{thm}[Monotone class theorem]\index{monotone class theorem}
  Let $(E, \mathcal{E})$ be a measurable space, and $\mathcal{A} \subseteq \mathcal{E}$ be a $\pi$-system with $\sigma(\mathcal{A}) = \mathcal{E}$. Let $\mathcal{V}$ be a vector space of functions such that
  \begin{enumerate}
    \item The constant function $1 = \mathbf{1}_E$ is in $\mathcal{V}$.
    \item The indicator functions $\mathbf{1}_A \in \mathcal{V}$ for all $A \in \mathcal{A}$
    \item $\mathcal{V}$ is closed under bounded, monotone limits. More explicitly, if $(f_n)$ is a bounded, non-negative, increasing sequence in $\mathcal{V}$ and $f_n \to f$ is bounded, then $f \in \mathcal{V}$.
  \end{enumerate}
  Then $\mathcal{V}$ contains all bounded measurable functions.
\end{thm}

\begin{proof}
  Note that the conditions for $\mathcal{V}$ is pretty like the conditions for a d-system, where taking a bounded, monotone limit is something like taking increasing unions.

  In fact, let
  \[
    \mathcal{D} = \{A \in \mathcal{E}: \mathbf{1}_A \in \mathcal{V}\}.
  \]
  We want to show that $\mathcal{D} = \mathcal{E}$. To do this, we have to show that $\mathcal{D}$ is a $d$-system.
  \begin{enumerate}
    \item Since $\mathbf{1}_E \in \mathcal{V}$, we know $E \in \mathcal{D}$.
    \item If $\mathbf{1}_A \in V$, then $1 - \mathbf{1}_A = \mathbf{1}_{E \setminus A} \in \mathcal{V}$. So $E \setminus A \in \mathcal{D}$.
    \item If $(A_n)$ is an increasing sequence in $\mathcal{D}$, then $\mathbf{1}_{A_n} \to \mathbf{1}_{\bigcup A_n}$ monotonically increasingly. So $\mathbf{1}_{\bigcup A_n}$ is in $\mathcal{D}$.
  \end{enumerate}
  So, by Dynkin's lemma, we know $\mathcal{D} = \mathcal{E}$. So $\mathcal{V}$ contains indicators of all measurable sets. We will now try to obtain any measurable function by approximating.

  Suppose that $f$ is bounded and non-negative measurable. We want to show that $f \in \mathcal{V}$. To do this, we approximate it by letting
  \[
    f_n = 2^{-n} \lfloor 2^n f\rfloor = \sum_{k = 0}^\infty k 2^{-n} \mathbf{1}_{\{k 2^{-n} \leq f < (k + 1) 2^{-n}\}}.
  \]
  Note that since $f$ is bounded, this is a finite sum. So it is a finite linear combination of indicators of elements in $\mathcal{E}$. So $f_n \in \mathcal{V}$, and $0 \leq f_n \to f$ monotonically. So $f \in \mathcal{V}$.

  More generally, if $f$ is bounded and measurable, then we can write
  \[
    f = f^+ - f^- = \max(f, 0) - \max(-f, 0) = (f \vee 0) - ((-f) \vee 0),
  \]
  where $f \vee g = \max(f, g)$.
  Then $f^+$ and $f^-$ are bounded and non-negative measurable.
\end{proof}
These results are important, but we will not get to use them until quite later on in the course.

\section{Integration}
We are now going to work towards defining the integral of a measurable function on a measure space $(E, \mathcal{E}, \mu)$. Different sources use different notations for the integral. The following notations are all commonly used:
\[
  \mu(f) = \int_E f \;\d \mu = \int_E f(x) \;\d \mu(x) = \int_E f(x) \mu(\d x).
\]
In the case where $(E, \mathcal{E}, \mu) = (\R, \mathcal{B}, \mathrm{Lebesgue})$, people often just write this as
\[
  \mu(f) = \int_\R f(x)\;\d x.
\]
On the other hand, if $(E, \mathcal{E}, \mu) = (\Omega, \F, \P)$ is a probability space, and $X$ is a random variable, then people write the integral as $\E[X]$, the \emph{expectation} of $X$.

So how are we going to define the integral? There are two steps to defining the integral. The idea is that we first define the integral on \emph{simple functions}, and then extend the definition to more general measurable functions by taking the limit. When we do the definition for simple functions, it will be obvious that the definition satisfies the nice properties, and we will have to check that they are preserved when we take the limit.

\begin{defi}[Simple function]\index{simple function}
  A \emph{simple function} is a measurable function that can be written as a finite non-negative linear combination of indicator functions of measurable sets, ie.
  \[
    f = \sum_{k = 1}^n a_k \mathbf{1}_{A_k}
  \]
  for some $A_k \in \mathcal{E}$ and $a_k \geq 0$.
\end{defi}
Note that some sources do not assume that $a_k \geq 0$, but assuming this makes our life easier.

It is obvious that
\begin{prop}
  A function is simple iff it is measurable, non-negative, and takes on only finitely-many values.
\end{prop}

\begin{defi}[Integral of simple function]\index{integral!simple function}\index{simple function!integral}
  The integral of a simple function
  \[
    f = \sum_{k = 1}^n a_k \mathbf{1}_{A_k}
  \]
  is given by
  \[
    \mu(f) = \sum_{k = 1}^n a_k \mu(A_k).
  \]
\end{defi}
Note that it can be that $\mu(A_k) = \infty$, but $a_k = 0$. When this happens, we are just going to declare that $0 \cdot \infty = 0$ (this makes sense because this means we are ignoring all $0 \cdot\mathbf{1}_A$ terms for any $A$). After we do this, we can check the integral is well-defined.

We are now going to extend this definition to non-negative measurable functions by a limiting procedure. Once we've done this, we are going to extend the definition to measurable functions by linearity of the integral. Then we would have a definition of the integral, and we are going to deduce properties of the integral using approximation.

\begin{defi}[Integral]\index{integral}
  Let $f$ be a non-negative measurable function. We set
  \[
    \mu(f) = \sup\{\mu(g): g \leq f, g\text{ is simple}\}.
  \]
  For arbitrary $f$, we write
  \[
    f = f^+ - f^- = (f \vee 0) - ((-f) \vee 0).
  \]
  We put $|f| = f^+ + f^-$. We say $f$ is \emph{integrable}\index{integrable function} if $\mu(|f|) < \infty$. In this case, set
  \[
    \mu(f) = \mu(f^+) - \mu(f^-).
  \]
  If only one of $\mu(f^+), \mu(f_-) < \infty$, then we can still make the above definition, and the result will be infinite.
\end{defi}

We are now going to study some basic properties of the integral. We will first look at the properties of integrals of simple functions, and then extend them to general integrable functions.

For $f, g$ simple, and $\alpha, \beta \geq 0$, we have that
\[
  \mu(\alpha f + \beta g) = \alpha \mu(f) + \beta\mu(g).
\]
So the integral is linear.

Another important property is monotonicity --- if $f \leq g$, then $\mu(f) \leq \mu(g)$.

Finally, we have $f = 0$ a.e. iff $\mu(f) = 0$. It is absolutely crucial here that we are talking about non-negative functions.

Our goal is to show that these three properties are also satisfied for arbitrary non-negative measurable functions, and the first two hold for integrable functions.

In order to achieve this, we prove a very important tool --- the monotone convergence theorem. Later, we will also learn about the dominated convergence theorem, and Fatou's lemma. These are the main and very important results about exchanging limits and integration.

\begin{thm}[Monotone convergence theorem]\index{monotone convergence theorem}
  Suppose that $(f_n), f$ are non-negative measurable with $f_n \nearrow f$. Then $\mu(f_n) \nearrow \mu(f)$.
\end{thm}

In the proof we will use the fact that the integral is monotonic, which we shall prove later.
\begin{proof}
  We will prove the theorem step by step to work towards the case of $(f_n), f$ non-negative measurable.

  We first consider the case where $f_n = \mathbf{1}_{A_n}$ and $f = \mathbf{1}_A$. Then $f_n \nearrow f$ is true iff $A_n \nearrow A$. On the other hand, $\mu(f_n) \nearrow \mu(f)$ iff $\mu(A_n) \nearrow \mu(A)$.

  For convenience, we let $A_0 = \emptyset$. We can write
  \begin{align*}
    \mu(A) &= \mu\left(\bigcup_n (A_n) \setminus A_{n - 1}\right) \\
    &= \sum_{n = 1}^\infty \mu(A_n \setminus A_{n - 1}) \\
    &= \lim_{N \to \infty} \sum_{n = 1}^N \mu(A_n \setminus A_{n - 1}) \\
    &= \lim_{N \to \infty}\mu (A_N).
  \end{align*}
  So done.

  \separator

  We next consider the case where $f_n$ is simple, and $f = \mathbf{1}_A$ for some $A$. Fix $\varepsilon > 0$, and set
  \[
    A_n = \{f_n > 1 - \varepsilon\} \in \mathcal{E}.
  \]
  Then we know that $A_n \nearrow A$, as $f_n \nearrow f$. Moreover, by definition, we have
  \[
    (1 - \varepsilon) \mathbf{1}_{A_n} \leq f_n \leq f = \mathbf{1}_A.
  \]
  As $A_n \nearrow A$, we have that
  \[
    (1 - \varepsilon) \mu(f) = (1 - \varepsilon) \lim_{n \to \infty} \mu(A_n) \leq \lim_{n \to \infty} \mu(f_n) \leq \mu(f)
  \]
  since $f_n \leq f$. Since $\varepsilon$ is arbitrary, we know that
  \[
    \lim_{n \to \infty} \mu(f_n) = \mu(f).
  \]

  \separator

  Next, we consider the case where $f_n, f$ are both simple. We write
  \[
    f = \sum_{k = 1}^m a_k \mathbf{1}_{A_k},
  \]
  where $a_k > 0$ and $A_k$ are pairwise disjoint. Since $f_n \nearrow f$, we know
  \[
    a_k^{-1} f_n \mathbf{1}_{A_k} \nearrow \mathbf{1}_{A_k}.
  \]
  So we have
  \[
    \mu(f_n) = \sum_{k = 1}^m \mu(f_n \mathbf{1}_{A_k}) = \sum_{k = 1}^m a_k \mu(a_k^{-1} f_n \mathbf{1}_{A_k}) \to \sum_{k = 1}^m a_k \mu(A_k) = \mu(f).
  \]

  \separator

  Suppose $f_n$ are simple, and $f$ is non-negative measurable. Suppose $g \leq f$ is a simple function. Then $f_n \wedge g = \min(f_n, g)$ is a simple function. As $f_n \nearrow f$, we know $f_n \wedge g \nearrow f \wedge g = g$. So by the previous case, we know that
  \[
    \mu(f_n \wedge g) \to \mu(g).
  \]
  We also know that
  \[
    \mu(f_n) \geq \mu(f_n\wedge g).
  \]
  So we have
  \[
    \lim_{n \to \infty} \mu(f_n) \geq \mu(g)
  \]
  for all $g \leq f$. This is possible only if
  \[
    \lim_{n \to \infty} \mu(f_n) \geq \mu(f)
  \]
  by definition of the integral. However, we also know that $\mu(f_n) \leq \mu(f)$ for all $n$, again by definition of the integral. So we must have equality. So we have
  \[
    \mu(f) = \lim_{n \to \infty} \mu(f_n).
  \]

  \separator

  Suppose $f_n$ and $f$ are all non-negative measurable functions. Let
  \[
    g_n = 2^{-n} \lfloor 2^n f_n\rfloor \wedge n.
  \]
  Since this is non-negative, bounded and finite-valued, this is a simple function. Also, we have
  \[
    g_n \leq f_n \leq f.
  \]
  So
  \[
    \mu(g_n) \leq \mu(f_n) \leq \mu(f).
  \]
  Since $f_n \nearrow f$, we know $g_n \nearrow f$. So $\mu(g_n) \nearrow \mu(f)$ by the previous case. Therefore $\mu(f_n) \nearrow \mu(f)$.
\end{proof}

\begin{thm}
  Let $f, g$ be non-negative measurable, and $\alpha, \beta \geq 0$. We have that
  \begin{enumerate}
    \item $\mu(\alpha f + \beta g) = \alpha \mu(f) + \beta \mu(g)$.
    \item $f \leq g$ implies $\mu(f) \leq \mu(g)$.
    \item $f = 0$ a.e. iff $\mu(f) = 0$.
  \end{enumerate}
\end{thm}

\begin{proof}\leavevmode
  \begin{enumerate}
    \item Let
      \begin{align*}
        f_n = 2^{-n}\lfloor 2^n f\rfloor \wedge n\\
        g_n = 2^{-n}\lfloor 2^n g\rfloor \wedge n.
      \end{align*}
      Then $f_n, g_n$ are simple with $f_n \nearrow f$ and $g_n \nearrow g$. Hence $\mu(f_n) \nearrow \mu(f)$ and $\mu(g_n \nearrow g$ and $\mu(\alpha f_n + \beta g_n) \nearrow \mu(\alpha f + \beta g)$, by the monotone convergence theorem. As $f_n, g_n$ are simple, we have that
      \[
        \mu(\alpha f_n + \beta g_n) = \alpha \mu(f_n) + \beta \mu(g_n).
      \]
      Taking the limit as $n \to \infty$, we get
      \[
        \mu(\alpha f + \beta g) = \alpha \mu(f) + \beta \mu(g).
      \]
    \item We shall be careful not to use the monotone convergence theorem. We have
      \begin{align*}
        \mu(g) &= \sup\{\mu(h): h \leq g\text{ simple}\}\\
        &\geq \sup \{\mu(h): h \leq f\text{ simple}\}\\
        &= \mu(f).
      \end{align*}
    \item Suppose $f \not= 0$ a.e. Let
      \[
        A_n = \left\{x: f(x) > \frac{1}{n}\right\}.
      \]
      Then
      \[
        \{x: f(x) \not= 0\} = \bigcup_n A_n.
      \]
      Since the left hand set has non-negative measure, it follows that there is some $A_n$ with non-negative measure. For that $n$, we define
      \[
        h = \frac{1}{n} \mathbf{1}_{A_n}.
      \]
      Then $\mu(f) \geq \mu(h) > 0$. So $\mu(f) \not= 0$.

      Conversely, suppose $f = 0$ a.e. We let
      \[
        f_n = 2^{-n} \lfloor 2^n f\rfloor \wedge n
      \]
      be a simple function. Then $f_n \nearrow f$ and $f_n = 0$ a.e. So
      \[
        \mu(f) = \lim_{n \to \infty}\mu(f_n) = 0.
      \]
  \end{enumerate}
\end{proof}

We now prove the analogous statement for general integrable functions.
\begin{thm}
  Let $f, g$ be integrable, and $\alpha, \beta \geq 0$. We have that
  \begin{enumerate}
    \item $\mu(\alpha f + \beta g) = \alpha \mu(f) + \beta \mu(g)$.
    \item $f \leq g$ implies $\mu(f) \leq \mu(g)$.
    \item $f = 0$ a.e. implies $\mu(f) = 0$.
  \end{enumerate}
\end{thm}
Note that in the last case, the converse is no longer true, as one can easily see from the sign function $\sgn: [-1, 1] -> \R$.

\begin{proof}\leavevmode
  \begin{enumerate}
    \item We are going to prove these by applying the previous theorem.

      By definition of the integral, we have $\mu(-f) = - \mu(f)$. Also, if $\alpha \geq 0$, then
      \[
        \mu(\alpha f) = \mu(\alpha f^+) - \mu(\alpha f^-) = \alpha \mu(f^+) - \alpha \mu(f^-) = \alpha \mu(f).
      \]
      Combining these two properties, it then follows that if $\alpha$ is a real number, then
      \[
        \mu(\alpha f) = \alpha \mu(f).
      \]
      To finish the proof of (i), we have to show that $\mu(f + g) = \mu(f) + \mu(g)$. We know that this is true for non-negative functions, so we need to employ a little trick to make this a statement about the non-negative version. If we let $h = f + g$, then we can write this as
      \[
        h^+ - h^- = (f^+ - f^-) + (g^+ - g^-).
      \]
      We now rearrange this as
      \[
        h^ + f^- + g^- = f^+ + g^+ + h^-.
      \]
      Now everything is non-negative measurable. So applying $\mu$ gives
      \[
        \mu(f^+) + \mu(f^-) + \mu(g^-) = \mu(f^+) + \mu(g^+) + \mu(h^-).
      \]
      Rearranging, we obtain
      \[
        \mu(h^+) - \mu (h^-) = \mu(f^+) - \mu(f^-) + \mu(g^+) - \mu(g^-).
      \]
      This is exactly the same thing as saying
      \[
        \mu(f + g) = \mu(h) = \mu(f) = \mu(g).
      \]
    \item If $f \leq g$, then $g - f \geq 0$. So $\mu(g - f) \geq 0$. By (i), we know $\mu(g) - \mu(f) \geq 0$. So $\mu(g) \geq \mu(f)$.

    \item If $f = 0$ a.e., then $f^+, f^- = 0$ a.e. So $\mu(f^+) = \mu(f^-) = 0$. So $\mu(f) = \mu(f^+) - \mu(f^-) = 0$.
  \end{enumerate}
\end{proof}
As mentioned, the converse to (iii) is no longer true. However, we do have the following partial converse:
\begin{prop}
  If $\mathcal{A}$ is a $\pi$-system with $E \in \mathcal{A}$ and $\sigma(\mathcal{A}) = \mathcal{E}$, and $f$ is an integrable function that
  \[
    \mu(f\mathbf{1}_A) =0
  \]
  for all $A \in \mathcal{A}$. Then $\mu(f) = 0$ a.e.
\end{prop}

\begin{proof}
  On example sheet 2.
\end{proof}

\begin{prop}
  Suppose that $(g_n)$ is a sequence of non-negative measurable functions. Then we have
  \[
    \mu\left(\sum_{n = 1}^\infty g_n\right) = \sum_{n = 1}^\infty \mu(g_n).
  \]
\end{prop}

\begin{proof}
  We know
  \[
    \left(\sum_{n = 1}^N g_n\right) \nearrow \left(\sum_{n = 1}^\infty g_n\right)
  \]
  as $N \to \infty$. So by the monotone convergence theorem, we have
  \[
    \sum_{n = 1}^N \mu(g_n) \mu \left(\sum_{n = 1}^\infty g_n\right) \nearrow \mu\left(\sum_{n = 1}^\infty g_n\right).
  \]
  But we also know that
  \[
    \sum_{n = 1}^\infty \mu(g_n) \nearrow \sum_{n = 1}^\infty
  \]
  by definition. So we are done.
\end{proof}

So for non-negative measurable functions, we can alweays switch the order of integration and summation.

Note that we can consider summation as integration. We let $E = \N$ and $\mathcal{E} = \{\text{all subsets of $\N$}\}$. We let $\mu$ be the counting measure, so that $\mu(A)$ is the size of $A$. Then integrability (and having a finite integral) is the same as absolute convergence. Then if it converges, then we have
\[
  \int f \;\d \mu = \sum_{n = 1}^\infty f(n).
\]
So we can just view our proposition as proving that we can swap the order of two integrals. The general statement is known as Fubini's theorem.

\subsection{Integrals and limits}
We are now going to prove more things about exchanging limits and integrals.
\begin{thm}[Fatou's lemma]\index{Fatou's lemma}
  Let $(f_n)$ be a sequence of non-negative measurable functions. Then
  \[
    \mu(\lim\inf f_n) \leq \lim\inf \mu(f_n).
  \]
\end{thm}
Note that a special case was proven in the first example sheet, where we did it for the case where $f_n$ are indicator functions.

\begin{proof}
  We start with the trivial observation that if $k\geq n$, then we always have that
  \[
    \inf_{m \geq n} f_m \leq f_k.
  \]
  By the monotonicity of the integral, we know that
  \[
    \mu\left(\inf_{m \geq n} f_m\right) \leq \mu(f_k).
  \]
  for all $k \geq n$.

  So we have
  \[
    \mu(\inf_{m \geq n} f_m) \leq \inf_{k \geq n} \mu(f_k) \leq \liminf_m \mu(f_m).
  \]
  It remains to show that the left hand side converges to $\mu(\liminf f_m)$. Indeed, we know that
  \[
    \inf_{m \geq n} f_m \nearrow \liminf_m f_m.
  \]
  Then by monotone convergence, we have
  \[
    \mu\left(\inf_{m \geq n} f_m\right) \nearrow \mu\left(\liminf_m f_m\right).
  \]
  So we have
  \[
    \mu\left(\liminf_m f_m\right) \leq \liminf_m \mu(f_m).
  \]
\end{proof}
No one ever remembers which direction Fatou's lemma goes, and this leads to many incorrect proofs and results.

How do we remember which way the inequality goes in Fatou's lemma? We let $(E, \mathcal{E}, \mu) = (\R, \mathcal{B}, \text{Lebesgue})$. We let
\[
  f_n = \mathbf{1}_{[n, n + 1]}.
\]
Then we have
\[
  \liminf_n f_n = 0.
\]
So we have
\[
  \mu(f_n) = 1\text{ for all }n.
\]
So we have
\[
  \liminf \mu(f_n) = 1,\quad \mu(\liminf f_n) = 0.
\]
So we have
\[
  \mu\left(\liminf_m f_m\right) \leq \liminf_m \mu(f_m).
\]
The next result we want to prove is the dominated convergence theorem. This is like the monotone convergence theorem, but we are going to remove the increasing and non-negative measurable condition, and add in something else.
\begin{thm}[Dominated convergence theorem]\index{dominated convergence theorem}
  Let $(f_n), f$ be measurable with $f_n(x) \to f(x)$ for all $x \in E$. Suppose that there is an integrable function $g$ such that
  \[
    |f_n| \leq g
  \]
  for all $n$, then we have
  \[
    \mu(f_n) \to \mu(f)
  \]
  as $n \to \infty$.
\end{thm}

\begin{proof}
  Note that
  \[
    |f| = \lim_n |f|_n \leq g.
  \]
  So we know that
  \[
    \mu(|f|) \leq \mu(g) < \infty.
  \]
  So we know that $f$, $f_n$ is integrable.

  Now note also that
  \[
    0 \leq g + f_n,\quad 0 \leq g - f_n
  \]
  for all $n$. We are now going to apply Fatou's lemma twice with these series. We have that
  \begin{align*}
    \mu(g) + \mu(f) &= \mu(g + f) \\
    &= \mu\left(\liminf_n (g + f_n)\right) \\
    &\leq \liminf_n \mu(g + f_n)\\
    &= \liminf_n (\mu(g) + \mu(f_n))\\
    &= \mu(g) + \liminf_n \mu(f_n).
  \end{align*}
  Since $\mu(g)$ is finite, we know that
  \[
    \mu(f) \leq \liminf_n \mu(f_n).
  \]
  We now do the same thing with $g - f_n$. We have
  \begin{align*}
    \mu(g) - \mu(f) &= \mu(g - f) \\
    &= \mu\left(\liminf_n (g - f_n)\right) \\
    &\leq \liminf_n \mu(g - f_n)\\
    &= \liminf_n (\mu(g) - \mu(f_n))\\
    &= \mu(g) - \limsup_n \mu(f_n).
  \end{align*}
  Again, since $\mu(g)$ is finite, we know that
  \[
    \mu(f) \geq \limsup_n \mu(f_n).
  \]
  These combine to tell us that
  \[
    \mu(f) \leq \liminf_n \mu(f_n) \leq \limsup_n \mu(f_n) \leq \mu(f).
  \]
  So they must be all equal, and thus $\mu(f_n) \to \mu(f)$.
\end{proof}

\subsection{Relationship between the Riemann and Lebesgue integrals}
Suppose that $f: [0, 1] \to \R$ is a continuous function. Then $f$ is measurable, and is integrable as it is bounded. It is an exercise to show that the Lebesgue integral of $f$ is equal to the Riemann integral of $f$. More generally, every Riemann integrable function is Lebesgue integrable.% fill in?

However, a lot of functions are Lebesgue integrable but not Riemann integrable. Indeed, take the standard non-Riemann integrable function
\[
  f = \mathbf{1}_{[0, 1]\setminus \Q}.
\]
Then $f$ is not Riemann integrable, but it is Lebesgue integrable, since
\[
  \mu(f) = \mu([0, 1] \setminus \Q) = 1.
\]

\printindex
\end{document}
