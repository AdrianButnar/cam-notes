\documentclass[a4paper]{article}

\usepackage[pdftex,
  hidelinks,
  pdfauthor={Dexter Chua},
  pdfsubject={Cambridge Maths Notes: Part IA - Probability},
  pdftitle={Part IA - Probability},
pdfkeywords={Cambridge Mathematics Maths Math IA Lent Probability}]{hyperref}

\input{header}

\title{Part IA - Probability}
\author{Lectured by R. Weber}
\date{Lent 2015}

\begin{document}
\maketitle
{\small
  \noindent\textbf{Basic concepts}\\
  Classical probability, equally likely outcomes. Combinatorial analysis, permutations and combinations.  Stirling's formula (asymptotics for $\log n!$ proved).\hspace*{\fill}  [3]
 
  \vspace{10pt}
  \noindent\textbf{Axiomatic approach}\\
  Axioms (countable case). Probability spaces. Inclusion-exclusion formula. Continuity and subadditivity of probability measures. Independence. Binomial, Poisson and geometric distributions. Relation between Poisson and binomial distributions. Conditional probability, Bayes's formula. Examples, including Simpson's paradox.\hspace*{\fill} [5]
 
  \vspace{10pt}
  \noindent\textbf{Discrete random variables}\\
  Expectation. Functions of a random variable, indicator function, variance, standard deviation. Covariance, independence of random variables. Generating functions: sums of independent random variables, random sum formula, moments.
 
  \vspace{5pt}
  \noindent Conditional expectation. Random walks: gambler's ruin, recurrence relations. Difference equations and their solution. Mean time to absorption. Branching processes: generating functions and extinction probability. Combinatorial applications of generating functions.\hspace*{\fill} [7]
 
  \vspace{10pt}
  \noindent\textbf{Continuous random variables}\\
  Distributions and density functions. Expectations; expectation of a function of a random variable.  Uniform, normal and exponential random variables. Memoryless property of exponential distribution.  Joint distributions: transformation of random variables (including Jacobians), examples. Simulation: generating continuous random variables, independent normal random variables. Geometrical probability: Bertrand's paradox, Buffon's needle. Correlation coefficient, bivariate normal random variables.\hspace*{\fill} [6]
 
  \vspace{10pt}
  \noindent\textbf{Inequalities and limits}\\
  Markov's inequality, Chebyshev's inequality. Weak law of large numbers. Convexity: Jensen's inequality for general random variables, AM/GM inequality.
 
  \vspace{5pt}
  \noindent Moment generating functions and statement (no proof) of continuity theorem. Statement of central limit theorem and sketch of proof. Examples, including sampling.\hspace*{\fill} [3]}


\tableofcontents
\setcounter{section}{-1}
\section{Introduction}
In every day life, we often encounter the use of the term probability, and they are used in many different ways. For example, we can hear people say:
\begin{enumerate}
  \item The probability that a fair coin will land heads is $1/2$.
  \item The probability that a selection of 6 members wins the National Lottery Lotto jackpot is 1 in ${19\choose 6} = 13 983 816$ or $7.15112\times 10^{-8}$.
  \item The probability that a drawing pin will land 'point up' is 0.62.
  \item The probabilty that a large earthquake will occur on the San Andreas Fault in the next 30 years is about $21\%$
  \item The probability that humanity will be extinct by 2100 is about $50\%$
\end{enumerate}
The first two cases are things derived from logic. We know that the coin either lands heads or tails. By definition, a fair coin is equally likely to land heads or tail. So the probability of either must be $1/2$.

The third is something probably derived from experiments. Perhaps we did 1000 experiments and 620 of the pins point up. The fourth and fifth examples belong to yet another category that talks about are beliefs and predictions.

We call the first kind ``classical probability'', the second kind ``frequentist probability'' and the last ``subjective probability''. In this course, we only consider classical probability.

\section{Classical probability}
\subsection{Classical probability}
\begin{defi}[Classical probability]
  \emph{Classical probability} applies in a situation when there are a finite number of equally likely outcome.
\end{defi}

Consider the problem of points

$A$ and $B$ play a game in which they keep throwing coins. If a head lands, then $A$ gets a point. Otherwise, $B$ gets a point. The first person to get 10 points wins a prize.

Now suppose $A$ has got 8 points and $B$ has got 7. The game has to end because an earthquake struck. How should they divide the prize? We answer this by finding the probability of $A$ winning. Someone must have won by the end of 19 rounds, i.e. after 4 more rounds. If $A$ wins at least 2 of them, then $A$ wins.

The number of ways this can happen is ${4 \choose 2} + {4\choose 3} + {4 \choose 4} = 11$. So $A$ should get $11/16$ of the prize.

\subsection{Sample space and events}
Consider an experiment that has a random outcome.

\begin{defi}[Sample space]
  The set of all possible outcomes is the \emph{sample space}, $\Omega$. We can lists the outcomes as $\omega_1, \omega_2, \cdots \in \Omega$. Each $\omega \in \Omega$ is an \emph{outcome}.
\end{defi}

\begin{defi}[Event]
  A subset of $\Omega$ is called an \emph{event}.
\end{defi}

\begin{defi}[Set notations]
  Given any two events $A, B\subseteq \Omega$,
  \begin{itemize}
    \item The \emph{complement} of $A$ is $A^C = A' = \bar A = \omega\setminus A$.
    \item ``$A$ or $B$'' is the set $A\cup B$.
    \item ``$A$ and $B$''  is the set $A\cap B$.
    \item $A$ and $B$ are \emph{mutually exclusive} or \emph{disjoint} if $A\cap B = \emptyset$.
    \item $A\subseteq B$ means $A\Rightarrow B$.
  \end{itemize}
\end{defi}

\begin{defi}[Probability]
  Suppose $\Omega = \{\omega_1,\omega_2, \cdots, \omega_N\}$. Let $A\subseteq \Omega$ be an event. Then the \emph{probability} of $A$ is
  \[
    P(A) = \frac{\text{Number of outcomes in } A}{\text{Number of outcomes in }\omega} = \frac{|A|}{N}
  \]
\end{defi}

\begin{eg}
  Suppose $r$ digits are drawn at random from a table of random digits from 0 to 9. What is the probability of
  \begin{enumerate}
    \item No digit exceeds $k$
    \item The largest digit drawn is $k$
  \end{enumerate}

  The sample space is $\Omega = \{(a_1, a_2, \cdots, a_r): 0 \leq a_i \leq 9\}$. Then $|\Omega| = 10^r$.

  Let $A_k = [\text{no digit exceeds }k] = \{(a_1, \cdots, a_r): 0 \leq a_i \leq k\}$. Then $|A_k| = (k + 1)^r$. So $P(A_k) = (k + 1)^r/10^r$.

  Now let $B_k = [\text{largest digit drawn is k}]$. We can find this by finding all outcomes in which no digits exceed $k$, and subtract it by the number of outcomes in which no digit exceeds $k - 1$. So $|B_k| = |A_k| - |A_{k - 1}|$ and $P(B_k) = [(k + 1)^r - k^r]/10^r$.
\end{eg}

\end{document}
