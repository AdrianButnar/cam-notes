\documentclass[a4paper]{article}

\usepackage[pdftex,
  hidelinks,
  pdfauthor={Dexter Chua},
  pdfsubject={Cambridge Maths Notes: Part IB - Statistics},
  pdftitle={Part IB - Statistics},
pdfkeywords={Cambridge Mathematics Maths Math IB Lent Statistics}]{hyperref}

\title{Part IB - Statistics}
\author{Lectured by D. Spiegelhalter}
\date{Lent 2015}

\input{header}

\begin{document}
\maketitle
{\small
\noindent\textbf{Estimation}\\
Review of distribution and density functions, parametric families. Examples: binomial, Poisson, gamma.  Sufficiency, minimal sufficiency, the Rao-Blackwell theorem. Maximum likelihood estimation. Confidence intervals. Use of prior distributions and Bayesian inference.\hspace*{\fill} [5]

\vspace{10pt}
\noindent\textbf{Hypothesis testing}\\
Simple examples of hypothesis testing, null and alternative hypothesis, critical region, size, power, type I and type II errors, Neyman-Pearson lemma. Significance level of outcome. Uniformly most powerful tests. Likelihood ratio, and use of generalised likelihood ratio to construct test statistics for composite hypotheses. Examples, including $t$-tests and $F$-tests. Relationship with confidence intervals. Goodness-of-fit tests and contingency tables.\hspace*{\fill} [4]

\vspace{10pt}
\noindent\textbf{Linear models}\\
Derivation and joint distribution of maximum likelihood estimators, least squares, Gauss-Markov theorem. Testing hypotheses, geometric interpretation. Examples, including simple linear regression and one-way analysis of variance. Use of software.\hspace*{\fill} [7]}

\tableofcontents

\section{Introduction and probability review}
*Lecturer apologizes for not turning up the previous lecture*
\begin{defi}[Statistics]
  \emph{Statistics} is a set of principle a nd procedures for gaining and processing quantitative evidence in order to help us make judgements and decisions.
\end{defi}
Note that we did not mention data. We don't necessarily need data for statistics (even though most often we do).

In this course, we focus on formal \emph{statistical inference}. In the process, we assume
\begin{itemize}
  \item we have data generated from some unknown probability model
  \item we aim to use the data to learn about certain properties of the underlying probability model
\end{itemize}
In particular, we perform parametric inference:

We assume a random variable $X$ takes values in $\mathcal{X}$. We assume its distribution belongs to a family of distribution (eg. Poisson) indexed by a scalar or vector parameter $\theta$, taking values in some parameter space $\Theta$. We call this a \emph{parametric family}.

For example, we can have $X\sim \text{Poisson}(\mu)$ and $\theta = \mu\in\Theta = (0, \infty)$.

We assume that we already which family it belongs to, and then try to find out $\theta$.

Suppose $X_1, X_2, \cdots, X_n$ are iid with the same distribution as $X$. Then $\mathbf{X} = (X_1, X_2, \cdots, X_n)$ is a simple random sample (ie. our data).

We use the observed $\mathbf{X} = \mathbf{x}$ to make inferences about $\theta$, such as
\begin{itemize}
  \item giving an estimate $\hat{\theta}(\mathbf{x})$ of the true value of $\theta$.
  \item Giving an interval estimate $(\hat{\theta}_1(\mathbf{x}), \hat{\theta}_2(\mathbf{x}))$ for $\theta$
  \item testing a hypothesis about $\theta$, eg. whether $\theta = 0$.
\end{itemize}
\section{Estimation}
Suppose that $X_1, \cdots, X_n$ are iid, each with a probability density/mass function $f_X(x|\theta)$. We know $f_X$ but not $\theta$.

\begin{defi}[Statistic]
  A \emph{statistic} is an estimate of $\theta$. It is a function $T$ of a data. If $\mathbf{X} = \mathbf{x} = (x_1, \cdots, x_n)$, then our estimate is written as $\hat{\theta} = T(\mathbf{x})$. $T(\mathbf{X})$ is an \emph{estimator} of $\theta$.

  The distribution of $T = T(\mathbf{X})$ is its sampling distribution.

  Note that capital $\mathbf{X}$ denotes a random variable and $\mathbf{x}$ is an observed value. So $T(\mathbf{X})$ is a random variable and $T(\mathbf{x})$ is a particular value.
\end{defi}

\begin{eg}
  Let $X_1, \cdots, X_n$ is an iid $N(\mu, 1)$. A possible estimator for $\mu$ is
  \[
    T(\mathbf{X}) = \frac{1}{n}\sum X_i.
  \]
  The for any particular observed sample $\mathbf{x}$, our estimate is
  \[
    T(\mathbf{x}) = \frac{1}{n}\sum x_i.
  \]
  Recall that in general, if $X_i \sim N(\mu_i, \sigma_i^2)$, then $\sum X_i \sim N(\sum \mu_i, \sum \sigma_i^2)$, which can be proved by moment-generating functions.
  
  So we have $T(\mathbf{X})\sim N(\mu, 1/n)$. Note that by the Central Limit Theorem, even if $X_1$ were not normal, we still have $T(\mathbf{X})\sim N(\mu, 1/n)$ for large values of $n$.
\end{eg}

\begin{defi}[Bias]
  Let $\hat{\theta} = T(\mathbf{X})$ be an estimator of $\theta$. The \emph{bias} of $\hat{\theta}$ is the difference between its expected value and true value.
  \[
    \bias(\hat{\theta}) = \E_\theta(\hat{\theta}) - \theta.
  \]
  Note that the subscript $_\theta$ does not represent the random variable, but the thing we want to estimate. This is inconsistent with the use for, say the pmf.

  An estimator is \emph{unbiased} if it has no bias, ie. $\E_\theta(\hat{\theta}) = \theta$.
\end{defi}
To find out $\E_\theta (T)$, we can either find the distribution of $T$ and find its expected value, or evaluate $T$ as a function of $\mathbf{X}$ directly, and find its expected value. 

\begin{eg}
  In the above example, $\E_\mu(T) =\mu$. So $T$ is unbiased for $\mu$. 
\end{eg}
\subsection{Mean squared error}
In general, we prefer estimators whose sampling distributions ``cluster more closely'' around the true value of $\theta$.
\begin{defi}[Mean squared error]
  The \emph{mean squared error} of an estimator $\hat{\theta}$ is $\E_\theta[(\hat{\theta} - \theta)^2]$.

  Sometimes, we use the \emph{root mean squared error}, that is the square root of the above.
\end{defi}
For an unbiased estimator, the mean squared error is just the variance. In general,
\begin{align*}
  \E_\theta[(\hat{\theta} - \theta)^2] &= \E_\theta[(\hat{\theta} - E_\theta(\hat{\theta}) + \E_\theta(\hat{\theta}) - \theta)^2]\\
  &= \E_\theta[(\hat{\theta} - E_\theta(\hat{\theta}))^2] + \E_\theta(\hat{\theta}) - \theta]^2 + 2[\E_\theta(\hat{\theta}) - \theta]\E_\theta[\hat{\theta} - \E_\theta(\hat{\theta})]\\
  &= \var(\hat{\theta}) + \bias^2(\hat{\theta}).
\end{align*}
Sometimes it can be preferable to have a biased estimator with a low variance - sometimes know as the ``bias-variance trade-off''.

For example, suppose $X\sim \binomial(n, \theta)$. The standard estimator is $T_U = \mathbf{X}/n$, which is unbiased. $T_U$ has variance
\[
  \var_\theta(T_U) = \var_\theta(\mathbf{X})/n^2 = \theta(1 - \theta)/n.
\]
Consider an alternative estimator 
\[
  T_B = \frac{X + 1}{n + 2} = w\frac{X}{n} + (1 - w)\frac{1}{2},
\]
where $w = n/(n + 2)$. This can be interpreted to be a weighted average (by the sample size) of the sample mean and $1/2$. We have
\[
  \E_\theta(T_B) - \theta = \frac{n\theta + 1}{n + 2} - \theta = (1 - w)\left(\frac{1}{2} - \theta\right),
\]
and is biased. However,
\[
  \var_\theta(T_B) = \frac{\var_\theta(\mathbf{X})}{(n + 2)^2} = w^2\theta(1 - \theta)/n
\]
Now
\[
  \mse (T_U) = \var_\theta(T_U) + \bias^2(T_U) = \theta(1 - \theta)/n.
\]
and
\[
  \mse (T_B) = \var_\theta(T_B) + \bias^2(T_B) = w^2\frac{\theta(1 - \theta)}{n} + (1 - w)^2\left(\frac{1}{2} - \theta\right)^2
\]
We can plot the mean squared error for possible values of $\theta$. Here we plot the case where $n = 10$.
\begin{center}
  \begin{tikzpicture}[xscale=6,yscale=100]
    \draw [dashed, domain=0:1] plot (\x, {\x * (1 - \x) / 10});
    \node [above] at (0.5, 0.025) {unbiased estimator};
    \draw [domain=0:1] plot (\x, {25 * \x * (1 - \x) / 360 + (1 - 5/6)*(1 - 5/6)*(0.5 - \x)*(0.5 - \x)}) node [right] {biased estimator};
    \draw [->] (0, 0) -- (1.1, 0) node [right] {$\theta$};
    \draw [->] (0, 0) -- (0, 0.035) node [above] {mse};
    \foreach \x in {0, 0.2, 0.4, 0.6, 0.8, 1.0} {
      \draw (\x, 0) -- (\x, -0.002) node [below] {\x};
    }
    \foreach \y in {0, 0.01, 0.02, 0.03} {
      \draw (0, \y) -- (-.03, \y) node [left] {\y};
    }
  \end{tikzpicture}
\end{center}
We see that this biased estimator has smaller MSE unless $\theta$ has extreme values.

Also, suppose $X\sim \Poisson(\lambda)$, and for some reason, you want to estimate $\theta = [P(\mathbf{X} = 0)]^2 = e^{-2\lambda}$. Then any estimator $T(\mathbf{X})$ must satisfy $\E_\theta(T(\mathbf{X})) = \theta$, or equivalently,
\[
  E_\lambda(T(\mathbf{X})) = e^{-\lambda}\sum_{x = 0}^\infty T(x) \frac{\lambda^x}{x!} = e^{-2\lambda}.
\]
Then the only function $T$ that can satisfy this equation is $T(X) = (-1)^X$.

Then the only unbiased estimator estimate $e^{-2\lambda}$ to be $1$ if $X$ is even, -1 if $X$ is odd. CLEARLY crazy. 
\subsection{Sufficiency}
``Sufficiency'' is the question ``Is there a statistic $T(\mathbf{X})$ that in some sense contains all the information about $\theta$ that is in the sample?'' This is important in ``big data'' in which we have to store a large number of data.

\begin{eg}
  Let $X_1, \cdots X_n$ be iid $\Bernoulli(\theta)$, so that $\P(X_i = 1) = 1 - \P(X_i = 0) = \theta$ for some $0 < \theta < 1$. So
  \[
    f_{\mathbf{X}} (\mathbf{x}|\theta) = \prod_{i = 1}^n \theta^{x_i}(1 - \theta)^{1 - x_i} = \theta^{\sum x_i}(1 - \theta)^{n - \sum x_i}.
  \]
  This depends on the data only through $T(\mathbf{X}) = \sum x_i$, the total number of ones. Note that $T(\mathbf{X})\sim \binomial(n, \theta)$.

  If $T(\mathbf{x}) = t$, then
  \[
    f_{\mathbf{X}|T = t} = \frac{\P_\theta(\mathbf{X} = \mathbf{x}, T = t)}{\P_\theta T = t} = \frac{\P_\theta(\mathbf{X} = \mathbf{x})}{T = t}.
  \]
  Where the last equality comes because since if $\mathbf{X} = \mathbf{x}$, then $T$ must be equal to $t$. This is equal to
  \[
    \frac{\theta^{\sum x_i}(1 - \theta)^{n - \sum x_i}}{\binom{n}{t}\theta^t (1 - \theta)^{n - t}} = \binom{n}{t}^{-1}.
  \]
  So we know that the conditional distribution of $\mathbf{X}$ given $T = t$ does not depend on $\theta$. So if we know $T$, then additional knowledge of $\mathbf{x}$ does not give more information about $\theta$.
\end{eg}

\begin{defi}[Sufficient statistic]
  A statistic $T$ is \emph{sufficient} for $\theta$ if the conditional distribution of $\mathbf{X}$ given $T$ does not depend on $\theta$.
\end{defi}

In practice, we find them as follows:
\begin{thm}[The factorization criterion]
  $T$ is sufficient for $\theta$ iff $f_{\mathbf{X}}(\mathbf{x} |\theta) = g(T(\mathbf{x}), \theta)h(\mathbf{x})$ for some functions $g$ and $h$.
\end{thm}

\begin{proof}
  (discrete case only)

  Suppose $f_\mathbf{X}(\mathbf{x}|\theta) = g(T(\mathbf{x}, \theta)h(\mathbf{x})$. If $T(\mathbf{x}) = t$, then
  \begin{align*}
    f_{\mathbf{X}|T = t}(\mathbf{x}|T = t) &= \frac{\P_\theta(\mathbf{X} = \mathbf{x}, T(\mathbf{X}) = t)}{\P_\theta (T = t)}\\
    &= \frac{g(T(\mathbf{x}), \theta)h(\mathbf{x})}{\sum_{\{x:T(\mathbf{x}) = t\}}g(T(\mathbf{x}), \theta)h(\mathbf{x})}\\
    &=\frac{g(t, \theta)h(\mathbf{x})}{g(t, \theta)\sum h(\mathbf{x})}\\
    &= \frac{h(\mathbf{x})}{\sum h(\mathbf{x})}
  \end{align*}
  which doesn't depend on $\theta$. So $T$ is sufficient.

  Now suppose $T$ is sufficient so that the conditional distribution of $\mathbf{X}|T = t$ does not depend on $\theta$. If $T(\mathbf{x}) = t$, then
  \[
    \P_\theta(\mathbf{X} = \mathbf{x}) = \P_\theta(\mathbf{X} = \mathbf{x}, T=t') = \P_\theta(\mathbf{X} = \mathbf{x}|T = t)\P_\theta (T = t).
  \]

  The first factor does not depend on $\theta$ by assumption; call it $h(\mathbf{x})$. Let the second factor be $g(t, \theta)$, and so we have the required factorisation.
\end{proof}

\begin{eg}
  Continuing the above example,
  \[
    f_\mathbf{X}(\mathbf{x}|\theta) = \theta^{\sum x_i}(1 - \theta)^{n - \sum x_i}.
  \]
  Take $g(t, \theta) = \theta^t(1 - \theta)^{n - t}$ and $h(\mathbf{x}) = 1$ to see that $T(\mathbf{X}) = \sum X_i$ is sufficient for $\theta$.
\end{eg}

\begin{eg}
  Let $X_1, \cdots, X_n$ be iid $U[0, \theta]$. Write $1_{[A]}$ for the indicator function of $A$. We have
  \[
    f_\mathbf{X}(\mathbf{x}|\theta) = \prod_{i = 1}^n \frac{1}{\theta}1_{[0 \leq x_i\leq \theta]} = \frac{1}{\theta^n}1_{[\max_i x_i \leq \theta]}1_{[\min_i x_i\geq 0]}.
  \]
  So $T = \max_i x_i$ is sufficient.
\end{eg}

Note that sufficient statistics are not unique. If $T$ is sufficient for $\theta$, then so is any 1-1 function of $T$. $\mathbf{X}$ is always sufficient for $\theta$ as well, but it is not of much use.


The sample space $\mathcal{X}^n$ is partitioned by $T$ into sets $\{\mathbf{x}\in \mathcal{X}: t(\mathbf{X}) = t\}$. If $T$ is sufficient, then this data reduction does not lose any information on $\theta$. We seek a sufficient statistic that achieves the maximum-possible reduction.

\begin{defi}[Minimal sufficiency]
  A sufficient statistic $T(\mathbf{x})$ is minimal if it is a function of every other sufficient statistic, ie. if $T'(\mathbf{X})$ is also sufficient, then $T'(\mathbf{X}) = T'(\mathbf{Y}) \Rightarrow  T(\mathbf{X}) = T(\mathbf{Y})$. ie. it has the coarsest partitions.
\end{defi}

We can find them using the following theorem:
\begin{thm}
  Suppose $T = T(\mathbf{X})$ is a statistic such that $f_\mathbf{X}(\mathbf{x}; \theta)/f_\mathbf{X}(\mathbf{y}; \theta)$ is a constant as a function of $\theta$ if and only if $T(\mathbf{x}) = T(\mathbf{y})$. Then $T$ is minimal sufficient for $\theta$.
\end{thm}

\begin{eg}
  Suppose $X_1, \cdots, X_n$ are iid $N(\mu, \sigma^2)$. Then
  \begin{align*}
    \frac{f_\mathbf{X}(\mathbf{x}|\mu, \sigma^2)}{f_\mathbf{X}(\mathbf{y}|\mu, \sigma^2)} &= \frac{(2\pi\sigma^2)^{-n/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_i(x_i - \mu)^2\right\}}{(2\pi\sigma^2)^{-n/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_i(y_i - \mu)^2\right\}}\\
    &=\exp\left\{-\frac{1}{2\sigma^2}\left(\sum_i x_i^2 - \sum_i y_i^2\right) + \frac{\mu}{\sigma^2}\left(\sum_{i}x_i - \sum_i y_i\right)\right\}.
  \end{align*}
  This is a constant function of $(\mu, \sigma^2)$ iff  $\sum_i x_i^2 = \sum_i y_i^2$ and $\sum_i x_i = \sum_i y_i$. So $T(\mathbf{X}) = (\sum_i X_i^2, \sum_i X_i)$ is minimal sufficient for $(\mu, \sigma^2)$.
\end{eg}
\note If the range of $\mathbf{X}$ depends on $\theta$, then ``$f_\mathbf{X}(\mathbf{x}; \theta)/f_\mathbf{X}(\mathbf{y};\theta)$ is constant in $\theta$'' means ``$f_\mathbf{X}(\mathbf{x}; \theta = c(\mathbf{x}, \mathbf{y})f_\mathbf{X}(\mathbf{y}; \theta)$''

\begin{thm}[Rao-Blackwell Theorem]
  Let $T$ be a sufficient statistic for $\theta$ and let $\tilde{\theta}$ be an estimator for $\theta$ with $\E(\tilde{\theta}^2) < \infty$ for all $\theta$. Let $\hat{\theta} = \E[\hat{\theta}|T]$. Then for all $\theta$,
  \[
    \E[(\hat{\theta} - \theta)^2] \leq \E[(\tilde{\theta} - \theta)^2].
  \]
  The inequality is strict unless $\tilde{\theta}$ is a function of $T$.
\end{thm}

\begin{proof}
  By the conditional expectation formula, we have $\E(\hat{\theta}) = \E[\E(\tilde{\theta}|T)] = E(\tilde{\theta})$. So they have the same bias.

  By the conditional variance formula,
  \[
    \var(\tilde{\theta}) = \E[\var (\tilde{\theta}|T)] + \var [\E(\tilde{\theta}|T)] = \E[\var(\tilde{\theta}|T)] + \var(\hat{\theta}).
  \]
  Hence $\var(\tilde{\theta}) \geq \var (\hat{\theta})$. So $\mse(\tilde{\theta} \geq \mse\hat{\theta})$, with equality only if $\var(\tilde{\theta}|T) = 0$.
\end{proof}
\note
\begin{enumerate}
  \item Since $T$ is sufficient for $\theta$, the conditional distribution of $\mathbf{X}$ given $T = t$ does not depend on $\theta$. Hence $\hat{\theta} = \E[\tilde{\theta}(\mathbf{X})|T]$ does not depend on $\theta$, and so is a genuine estimator.
  \item The theorem says that given any estimator, we can find one that is a function of a sufficient statistic that is at least as good in terms of mean squared error of estimation.
  \item If $\tilde{\theta}$ is unbiased, then so is $\hat{\theta}$.
  \item If $\tilde{\theta}$ is already a function of $T$, then $\hat{\theta} = \tilde{\theta}$.
\end{enumerate}

\begin{eg}
  Suppose $X_1, \cdots, X_n$ are iid $\Poisson(\lambda)$, and let $\theta = e^{-\lambda}$ (ie. $\P(X_1 = 0)$. Then
  \[
    p_\mathbf{X}(\mathbf{x}|\lambda) =\frac{-e^{n\lambda}\lambda^{\sum x_i}}{\prod x_i!}.
  \]
  So
  \[
    p_\mathbf{X}(\mathbf{x}|\theta) = \frac{\theta^n(-\log \theta)^{\sum x_i}}{\prod x_i!}.
  \]
  We see that $T = \sum X_i$ is sufficient for $\theta$, and $\sum X_i \sim \Poisson (n\lambda)$.

  We use an easy estimator $\theta$ is $\tilde{\theta} = 1_{X_1 = 0}$, which is unbiased. (ie. if we observe nothing in the first observation period, we assume the event is impossible) Then
  \begin{align*}
    \E[\tilde{\theta}| T = t] &= \P\left(X_1 = 0|\sum_1^n X_i = t\right)\\
    &= \frac{\P(X_1 = 0)\P(\sum_2^n X_i = t)}{\P(\sum_1^n X_i = t)}\\
    &= \left(\frac{n - 1}{n}\right)^t.
  \end{align*}
  So $\hat{\theta} = (1 - 1/n)^{\sum x_i}$. This approximately $(1 - 1/n)^{n\bar{X}} \approx e^{-\bar X} = e^{-\hat{\lambda}}$, which makes sense.
\end{eg}

\begin{eg}
  Let $X_1, \cdots X_n$ be iid $U[0, \theta]$, and suppose that we want to estimate $\theta$. We have shown above that $T = \max X_i$ is sufficient for $\theta$. Let $\hat{\theta} = 2X_1$, and unbiased estimator. Then
  \begin{align*}
    \E[\tilde{\theta}|T = t] &= 2\E[X_1|\max X_i = t]\\
    &= 2(E[X_1|\max X_i = t, X_1 = \max X_i]\P(X_1 = \max X_i)\\
    &+ \E[X_1|\max X_i = t, X_1 \not= \max X_i] \P(X_1 \not= \max X_i))\\
    &= 2\left(t\times \frac{1}{n} + \frac{t}{2}\frac{n - 1}{n}\right)\\
    &= \frac{n + 1}{n}t.
  \end{align*}
  So $\hat{\theta} = \frac{n + 1}{n}\max X_i$.
\end{eg}

\subsection{Likelihood}
Maximum likelihood estimation is one of the most important and widely used methods for finding estimations. Let $X_1, \cdots , X_n$ be random variables with joint pdf/pmg $f_\mathbf{X}(\mathbf{x}|\theta)$. We observe $\mathbf{X} = \mathbf{x}$.

\begin{defi}[Likelihood]
  The \emph{likelihood} of $\theta$ is $\like(\theta) = f_\mathbf{X}(\mathbf{x} | \theta)$, regarded as a function of $\theta$. The \emph{maximum likelihood estimator} (mle) of $\theta$ is the value of $\theta$ that maximizes $\like (\theta)$.
\end{defi}
It is often easier to maximize the \emph{log-likelihood}.

If $X_1, \cdots, X_n$ are iid, each with pdf/pmf $f_X(x|\theta)$, then
\begin{align*}
  \like(\theta) &= \prod_{i = 1}^n f_X(x_i|\theta)\\
  \log\like (\theta) &= \sum_{i = 1}^n \log f_X(x_i|\theta).
\end{align*}

\begin{eg}
  Let $X_1, \cdots, X_n$ be iid $\Bernoulli(p)$. Then
  \[
    l(p) =\log\like(p) = \left(\sum x_i\right)\log p + \left(n - \sum x_i\right)\log(1 - p).
  \]
  Thus
  \[
    \frac{\d l}{\d p} = \frac{\sum x_i}{p} - \frac{n - \sum x_i}{1 - p}.
  \]
  This is zero when $p = \sum x_i /n$, and this is the maximum likelihood estimator. (and is unbiased)
\end{eg}

\begin{eg}
  Let $X_1, \cdots, X_n$ be iid $N(\mu, \sigma^2)$, $\theta = (\mu, \sigma^2)$. Then
  \[
    l(\mu, \sigma^2) = \log\like(\mu, \sigma^2) = -\frac{n}{2}\log (2\pi) - \frac{n}{2}\log (\sigma^2) - \frac{1}{2\sigma^2}\sum (x_i - \mu)^2.
  \]
  This is maximized when
  \[
    \frac{\partial l}{\partial\mu} = \frac{\partial l}{\partial \sigma^2} = 0.
  \]
  We have
  \[
    \frac{\partial l}{\partial \mu} = -\frac{1}{\sigma^2}\sum (x_i - \mu), \quad \frac{\partial l}{\sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}\sum (x_i - \mu)^2.
  \]
  So the solution, hence maximum likelihood estimator is $(\hat{\mu}, \hat{\sigma}^2) = (\bar x, S_{xx}/n)$, where $\bar{x} = \frac{1}{n}\sum x_i$ and $S_{xx} = \sum(x_i - \bar{x})^2$.

  We shall see later that $S_{XX}/s^2 = \frac{n\hat{\sigma}^2}{\sigma^2}\sim \chi_{n - 1}^2$, and so $\E(\hat{\sigma}^2) = \frac{(n - 1)\sigma^2}{n}$, ie. $\hat{\sigma}^2$ is biased.
\end{eg}

\begin{eg}
  Let $X_1, \cdots X_n$ be iid $U(0, \theta)$. This is the ``German tank problem'': suppose the American army discovers some German tanks that are sequentially numbered, i.e. the first tank is numbered 1, the second is numbered 2, etc. Then if $\theta$ tanks are produced, then the probability distribution of the tank number is $U(0, \theta)$. Suppose we have discovered $n$ tanks whose numbers are $x_1, x_2, \cdots, x_n$, and we want to estimate $\theta$, the total number of tanks produced. We want to find the maximum likelihood estimator.
  
  Then
  \[
    \like(\theta) = \frac{1}{\theta^n}1_{[\max x_i \leq \theta]}1_{[\min x_i\geq 0]}.
  \]
  So for $\theta\geq \max _i$, $\like (\theta) = 1/\theta^n$ and is decreasing as $\theta$ increases, while for $\theta < \max x_i$, $\like (\theta) = 0$. Hence the value $\hat{\theta} = \max x_i$ maximizes the likelihood.

  Is $\hat{\theta}$ unbiased? First we need to find the distribution of $\hat{\theta}$. For $0 \leq t \leq \theta$, the distribution function of $\hat{\theta}$ is
  \[
    F_{\hat{\theta}}(t) = P(\hat{\theta} \leq t) = \P(X_i \leq t, \text{all }i) = (\P(X_i \leq t))^n = \left(\frac{t}{\theta}\right)^n,
  \]
  Differentiating with respect to $T$, we find the $pdf f_{\hat{\theta}} = \frac{nt^{n - 1}}{\theta^n}$. Hence
  \[
    \E(\hat{\theta}) = \int_0^\theta t\frac{nt^{n - 1}}{\theta^n} \;\d t = \frac{n\theta}{n + 1}.
  \]
  So $\hat{\theta}$ is biased, but asymptotically unbiased.
\end{eg}

\note
\begin{enumerate}
  \item if $T$ is sufficient for $\theta$, then the likelihood is $g(T(\mathbf{x}), \theta)h(\mathbf{x})$, which depends on $\theta$ through $T(\mathbf{x})$. To maximise this as a function of $\theta$, we only need to maximize $g$, and so the mle $\hat{\theta}$ is a function of the sufficient statistic.
  \item If $\phi = h(\theta)$ with $h$ injective, then the mle of $\phi =h(\hat{\theta})$. If the mle of $\theta$ is $\hat{\theta}$, then the mle of $\theta^2$ is $\hat{\theta}^2$.
  \item Often there is no closed form for the mle, and we have to find $\hat{\theta}$ numerically.
\end{enumerate}

\begin{eg}
  Smarties come in $k$ equally frequent colours, but suppose we do not know $k$. Assume that we sample with replacement (although this is unhygienic).

  Our first Smarties are Red, Purple, Red, Yellow. Then
  \begin{align*}
  \like(k)&=\P_k(\text{1st is a new colour})\P_k(\text{2nd is a new colour})\\\
  & \P_k(\text{3rd matches 1st})\P_k(\text{4th is a new colour})\\
  &= 1\times \frac{k - 1}{k}\times \frac{1}{k}\times \frac{k - 2}{k}\\
  &= \frac{(k - 1)(k - 2)}{k^3}.
\end{align*}
The maximum likelihood is 5 (by trial and error), even though it is not much likelier than the others.
\end{eg}

\subsection{Confidence intervals}
\begin{defi}
  A $100\gamma\%$ ($0 < \gamma < 1$) \emph{confidence interval} (CO) for $\theta$ is a random interval $(A(\mathbf{X}), B(\mathbf{X}))$ such that $\P(A(\mathbf{X}) < \theta < B(\mathbf{X})) = \gamma$, no matter what the true value of $\theta$ may be.
\end{defi}

Notice that it is endpoints of the interval are random quantities, while $\theta$ is a fixed constant we want to find out.

We can interpret this in terms of repeat sampling. If we calculate $(A(\mathbf{x}), B(\mathbf{x}))$ for a large number of samples $\mathbf{x}$, then approximately $100\gamma\%$ of them will cover the true value of $\theta$.

IMPORTANT: having observed some data $\mathbf{x}$ and calculated $95\%$ confidence interval, we \emph{cannot} say that $\theta$ has 95\% chance of being within the interval.

\begin{eg}
  Suppose $X_1, \cdots, X_n$ are iid $N(\theta, 1)$. Find a $95\%$ confidence interval for $\theta$.

  We know $\bar X \sim N(\theta, \frac{1}{n}\sigma^2)$, so that $\sqrt{n}(\bar X - \theta)\sim N(0, 1)$.

  Let $z_1, z_2$ be such that $\phi(z_2) - \phi(z_1) = 0.95$, where $\phi$ is the standard normal distribution function.

  We have $\P[z_1 < \sqrt{n}(\bar X - \theta) < z_2] = 0.95$, which can be rearranged to give
  \[
    \P\left[\bar X - \frac{z_2}{\sqrt{n}} < \theta < \bar X - \frac{z_1}{\sqrt{n}}\right] = 0.95.
  \]
  so that
  \[
    \left(\bar X - \frac{z_2}{\sqrt{n}}, \bar X - \frac{z_1}{\sqrt{n}}\right).
  \]
  There are many possible choices for $z_1$ and $z_2$. Since $N(0, 1)$ density is symmetric, the shortest such interval is obtained by $z_2 = z_{0.025} = -z_1$. We can also choose other values such as $z_1 = -\infty$, $z_2 = 1.64$, but we usually choose symmetric end points.
\end{eg}
The above example illustrates a common procedure for finding confidence intervals:
\begin{itemize}
  \item Find a quantity $R(\mathbf{X}, \theta)$ such that the $\P_\theta$-distribution of $R(\mathbf{X}, \theta)$ does not depend on $\theta$. This is called a \emph{pivot}. In our example, $R(\mathbf{X}, \theta) = \sqrt{n}(\bar X - \theta)$.
  \item Write down a probability statement of the form $\P_\theta(c_1 < R(\mathbf{X}, \theta) < c_2) = \gamma$.
  \item Rearrange the inequalities inside $\P(\ldots)$ to find the interval.
\end{itemize}
\note
\begin{enumerate}
  \item Usually $C_1, c_2$ are percentage points from a know standardised distribution, often equitailed so that use, say, $2.5\%$ and $97.5\%$ points for a $95\%$ confidence interval. We could use, say $0\%$ and $95\%$, but the interval would generally be wider.
  \item We can have confidence intervals for vector parameters.
  \item IF $(A(\mathbf{x}, B(\mathbf{x}))$ is a $100\gamma\%$ confidence interval for $\theta$, and $T(\theta)$ is a monotone increasing function of $\theta$, then $(T(A(\mathbf{x})), T(B(\mathbf{x})))$ is a $100\gamma\%$ confidence interval for $T(\theta)$.
\end{enumerate}
\begin{eg}
  Suppose $X_1, \cdots, X_{50}$ are iid $N(0, \sigma^2)$. Find a $99\%$ confidence interval for $\sigma^2$.

  We know that $X_i/\sigma \sim N(0, 1)$. So $\displaystyle\frac{1}{\sigma^2}\sum_{i = 1}^{50}X_i^2 \sim \chi^2_{50}$.

  So $R(\mathbf{X}, \sigma^2) = \sum_{i = 1}^{50} X_2^2/\sigma^2$ is a pivot.

  Recall that $\chi_n^2(\alpha)$ is the upper $100\alpha\%$ point of $\chi_n^2$, ie.
  \[
    \P(\chi_n^2 \leq \chi_n^2(\alpha)) = 1 - \alpha.
  \]
  So we have $c_1 = \chi_{50}^2(0.995) = 27.99$ and $c_2 = \chi_{50}^2(0.005) = 79.49$.

  So
  \[
    \P \left(c_1 < \frac{\sum X_i^2}{\sigma^2} < c_2\right) = 0.99,
  \]
  and so
  \[
    \P\left(\frac{\sum X_i^2}{c_2} < \sigma^2 < \frac{\sum X_i^2}{c_1}\right) = 0.99.
  \]
  Furthermore, a $99\%$ confidence interval for $\sigma$ is $\left(\sqrt{\frac{\sum X_i^2}{c_2}}, \sqrt{\frac{\sum X_i^2}{c_1}}\right)$.
\end{eg}

\begin{eg}
  Suppose $X_1, \cdots, X_n$ are iid $\Bernoulli(p)$. Find an approximate confidence interval for $p$.

  The mle of $p$ is $\hat p = \sum X_i/n$.

  By the Central Limit theorem, $\hat{p}$ is approximately $N(p, p(1 - p)/n)$ for large $n$.

  So $\displaystyle \frac{\sqrt{n}(\hat{p} - p)}{\sqrt{p(1 - p)}}$ is approximately $N(0, 1)$ for large $n$. So we have
  \[
    \P\left(\hat p - z_{(1 - \gamma)/2}\sqrt{\frac{p(1 - p)}{n}} < p < \hat{p} + z_{(1 - \gamma)/2}\sqrt{\frac{p(1 - p)}{n}}\right)\approx \gamma.
  \]
  But $p$ is unknown! So we approximate it by $\hat{p}$ to get a confidence interval for $p$ when $n$ is large:
  \[
    \P\left(\hat p - z_{(1 - \gamma)/2}\sqrt{\frac{\hat p(1 - \hat p)}{n}} < p < \hat{p} + z_{(1 - \gamma)/2}\sqrt{\frac{\hat p(1 - \hat p)}{n}}\right)\approx \gamma.
  \]
\end{eg}

\begin{eg}
  Suppose an opinion poll says $20\%$ are going to vote $UKIP$, based on a random sample of $1,000$ people. What might the true proportion be?

  We assume we have an observation of $x = 200$ from a $\binomial(n, p)$ distribution with $n = 1,000$. Then $\hat p = x/n = 0.2$ is an unbiased estimate, also the mle. 
  Now $\var(X/n) = \frac{p(1 - p)}{n}\approx \frac{\hat p(1 - \hat p}{n} = 0.00016$.

  So a $85\%$ confidence interval is
  \[
    \left(\hat p - 1.96\sqrt{\frac{\hat p(1 - \hat p)}{n}}, \hat p + 1.96\sqrt{\frac{\hat p(1 - \hat p)}{n}}\right) = 0.20 \pm 1.96\times 0.013 = (0.175, 0.225),
  \]
  \note Since $p(1 - p)\leq 1/4$ for all $0 \leq p \leq 1$, then a conservative $95\%$ interval is $\hat p \pm 1.96\sqrt{1/4n} \approx \hat p \pm \sqrt{1/n}$. So whatever proportion is reported, it will be 'accurate' to $\pm 1/\sqrt{n}$. 
\end{eg}

\begin{eg}
  Suppose $X_1, X_2$ are iid from $U(\theta - 1/2, \theta + 1/2)$. What is a sensible $50\%$ confidence interval for $\theta$?

  Consider the probability of getting one observation on each side of $\theta$:
  \[
    \P_\theta(\min(X_1, X_2) \leq \theta \leq \max(X_1, X_2)) = \frac{1}{2}.
  \]
  So $(\min(X_1, X_2), \max (X_1, X_2))$ is a confidence interval for $\theta$.

  But suppose $|X_1 - X_2| \geq \frac{1}{2}$, e.g. $x_1 = 0.2, x_2 = 0.9$, then we \emph{know} that, in this particular case, $\theta$ \emph{must} lie in $(\min (X_1, X_2), \max(X_1, X_2))$. This means that we can get a much narrower $50\%$ confidence interval! So guaranteed sampling properties does not necessarily mean a sensible conclusion in all cases.
\end{eg}
\subsection{Bayesian estimation}
So far we have seen the \emph{frequentist} approach to a statistical inference, ie. inferential statements about $\theta$ are interpreted in terms of repeat sampling. For example, the confidence interval is what's the probability that the interval will contain $\theta$, not the probability that $\theta$ lies in the interval.

In contrast, the Bayesian approach treats $\theta$ as a random variable taking values in $\Theta$. The investigator's information and beliefs about the possible values of $\theta$ before any observation of data, are summarised by a \emph{prior distribution} $\pi(\theta)$. When $\mathbf{X} = \mathbf{x}$ are observed, the extra information about $\theta$ is combined with the prior to obtain the \emph{posterior distribution} $\pi(\theta|\mathbf{x})$ for $\theta$ given $\mathbf{X} = \mathbf{x}$.

There has been a long-running argument between the two approaches. Recently, things have settled down, and Bayesian methods are seen to be appropriate in huge numbers of application where one seeks to assess a probability about a ``state of the world''. For example, spam filters will assess the probability that a specific email is a spam, even though from a frequentist's point of view, this is nonsense, because the email either is or is not a spam, and it makes no sense to assign a probability to the email's being a spam.

In Bayesian inference, we usually have some \emph{prior} knowledge about the distribution of $\theta$ (eg. between $0$ and $1$). After collecting some data, we will find a \emph{posterior} distribution of $\theta$ given $\mathbf{X} = \mathbf{x}$.

\begin{defi}[Prior and posterior distribution]
  The \emph{prior distribution} of $\theta$ is the probability distribution of the value of $\theta$ before conducting the experiment. We usually write as $\pi(\theta)$. 

  The \emph{posterior distribution} of $\theta$ is the probability distribution of the value of $\theta$ given an outcome of the experiment $\mathbf{x}$. We write as $\pi(\theta|\mathbf{x})$.
\end{defi}
By Bayes' theorem, the distributions are related by
\[
  \pi(\theta|\mathbf{x}) = \frac{f_{\mathbf{X}}(\mathbf{x}|\theta)\pi(\theta)}{f_{\mathbf{X}}(\mathbf{x})}.
\]
Thus
\begin{align*}
  \pi(\theta|\mathbf{x}) &\propto f_{\mathbf{X}}(\mathbf{x}|\theta)\pi(\theta).\\
  \text{posterior} &\propto \text{likelihood}\times\text{prior}.
\end{align*}
where the constant of proportionality is chosen to make the total mass of the posterior distribution equal to one. Usually, we use this form, instead of attempting to calculate $f_\mathbf{X}(\mathbf{x})$.

In practice, we can recognise the family for $\pi(\theta|\mathbf{x})$. It should be clear that the data enter through the likelihood, so the inference is automatically based on any sufficient inference.

\begin{eg}
  Suppose I have $3$ coins in my pocket. One is $3:1$ in favour of tails, one is a fair coin, and one is $3:1$ in favour of heads.

  I randomly select one coin and flip it once, observing a head. What is the probability that I have chosen coin 3?

  Let $X = 1$ denote the event that I observe a head, $X = 0$ if a tail. Let $\theta$ denote the probability of a head. So $\theta$ is either 0.25, 0.5 or 0.75.

  Our prior distribution is $\theta(\theta = 0.25) = \pi(\theta = 0.5) = \pi(\theta = 0.75) = 1/3$.

  The probability mass function $f_X(x|\theta) = \theta^x(1 - \theta)^{1 - x}$. So we have to following results:

  \begin{tabular}{ccccc}
    \toprule
    $\theta$ & $\pi(\theta)$ & $f_X(x = 1|\theta)$ & $f_X(x = 1|\theta)\pi(\theta)$ & $\pi(x)$\\
    \midrule
    0.25 & 0.33 & 0.25 & 0.0825 & 0.167\\
    0.50 & 0.33 & 0.50 & 0.1650 & 0.333\\
    0.75 & 0.33 & 0.75 & 0.2475 & 0.500\\
    \midrule
    Sum & 1.00 & 1.50 & 0.4950 & 1.000\\
    \bottomrule
  \end{tabular}

  So if we observe a head, then there is now a $50\%$ chance that we have picked the third coin.
\end{eg}

\begin{eg}
  Suppose we are interested in the true mortality risk $\theta$ in a hospital $H$ which is about to try a new operation. On average in the country, around $10\%$ of the people die, but mortality rates in different hospitals vary from around $3\%$ to around $20\%$. Hospital $H$ has no deaths in their first 10 operations. What should we believe about $\theta$?

  Let $X_i = 1$ if the $i$th patient in $H$ dies. The
  \[
    f_{\mathbf{x}}(\mathbf{x}|\theta) = \theta^{\sum x_i}(1 - \theta)^{n - \sum x_i}.
  \]
  Suppose a priori that $\theta\sim \betaD(a, b)$ for some unknown $a > 0, b > 0$ so that
  \[
    \pi(\theta)\propto \theta^{a - 1}(1 - \theta)^{b - 1}.
  \]
  Then the posteriori is
  \[
    \pi(\theta|\mathbf{x})\propto f_{\mathbf{x}}(\mathbf{x}|\theta)\pi(\theta)\propto \theta^{\sum x_i + a - 1}(1 - \theta)^{n- \sum x_i + b - 1}.
  \]
  We recognize this as $\betaD(\sum x_i + a, n - \sum x_i + b)$. So
  \[
    \pi(\theta|\mathbf{x}) = \frac{\theta^{\sum x_i + a - 1}(1 - \theta)^{n - \sum x_i + b - 1}}{B(\sum x_i + a, n - \sum x_i + b)}.
  \]

  In practice, we need to find a Beta prior distribution that matches our information from other hospitals. It turns out that $\betaD(a = 3, b = 27)$ prior distribution has mean 0.1 and $\P(0.03 < \theta < .20) = 0.9$.

  Then we observe data $\sum x_i = 0$, $n = 0$. So the posterior is $\betaD(\sum x_i + a, n - \sum x_i + b) = \betaD(3, 37)$.

  This has a mean of $3/40 = 0.075$.

  Even though nobody has died so far, the mle is $0$, which does not seem plausible. But we have a higher mean than $0$ because we take into account the data from other hospitals.
\end{eg}

For this problem, a beta prior leads to a beta posterior. We say that the beta family is a \emph{conjugate} family of prior distribution for Bernoulli samples.

Suppose that $a = b = 1$ so that $\pi (\theta) = 1$ for $0 < \theta < 1$ - the uniform distribution. Then the posterior is $\betaD(\sum x_i + 1, n - \sum x_i + 1)$, with properties

\begin{tabular}[]{cccc}
  \toprule
  &mean & mode & variance\\
  \midrule
  prior & 1/2 & non-unique & 1/12\\
  posterior & $\displaystyle \frac{\sum x_i + 1}{n + 2}$ & $\displaystyle \frac{\sum x_i}{n}$ & $\displaystyle\frac{(\sum x_i + 1)(n - \sum x_i + 1)}{(n + 2)^2(n + 3)}$\\
  \bottomrule
\end{tabular}

Note that the mode of the posterior is the mle.

The posterior mean estimator, $\frac{\sum x_i + 1}{n + 2}$ is discussed in Lecture 2, where we showed that this estimator had smaller mse than the mle for non-extreme value of $\theta$. This is known as the Laplace's estimator.

The posterior variance is bounded above by $1/(4(n + 3))$, and this is smaller than the prior variance, and is smaller for larger $n$.

Again, note that the posterior automatically depends on the data through the sufficient statistic.

Let $L(\theta, a)$ be the loss incurred in estimating the value of a parameter to be $a$ when the true value is $\theta$.

A common loss functions are quadratic loss $L(\theta, a) = (\theta - a)^2$, absolute error loss $L(\theta, a) = |\theta - a|$, but we can have others.

When our estimate is $a$, the expected posterior loss is $a$, the expected posterior loss is
\[
  h(a) = \int L(\theta, a)\pi(\theta|\mathbf{x})\;\d \theta.
\]
\begin{defi}[Bayes estimator]
  The \emph{Bayes estimator} $\hat{\theta}$ is the estimator that minimises the expected posterior loss.
\end{defi}

For quadratic loss,
\[
  h(a) = \int (a - \theta)^2 \pi(\theta|\mathbf{x})\;\d \theta.
\]
$h'(a) = 0$ if 
\[
  a\int \pi (\theta|\mathbf{x})\;\d \theta = \int \theta \pi(\theta|\mathbf{x}\;\d \theta.
\]
So $\hat{\theta} = \int \theta\pi(\theta|\mathbf{x})\;\d \theta$, the \emph{posterior mean}, minimises $h(a)$.

For absolute error loss,
\begin{align*}
  h(a) &= \int |\theta - a|\pi(\theta|\mathbf{x})\;\d \theta\\
  &= \int_{-\infty}^a (a - \theta)\pi(\theta|\mathbf{x}) = \;\d \theta + \int_a^\infty (\theta - a)\pi(\theta|\mathbf{x})\;\d \theta\\
  &= a\int_{-\infty}^a \pi(\theta|\mathbf{x})\;\d \theta - \int_{-\infty}^a\theta\pi(\theta|\mathbf{x})\;\d \theta\\
  &+ \int_a^\infty \theta\pi(\theta|\mathbf{x})\;\d \theta - a\int_a^\infty \pi(\theta|\mathbf{x})\;\d \theta.
\end{align*}
Now $h'(a) = 0$ if
\[
  \int_{-\infty}^a \pi(\theta|\mathbf{x})\;\d \theta = \int_a^\infty \pi(\theta|\mathbf{x})\;\d \theta.
\]
This occurs when each side is $1/2$. So $\hat{\theta}$ is the \emph{posterior median}.

\begin{eg}
  Suppose that $X_1, \cdots , X_n$ are iid $N(\mu, 1)$, and that a prior $\mu\sim N(0, \tau^{-2})$ for unknown $\tau^{-2}$. So $\tau$ is the certainty of our prior knowledge.

  The posterior is given by
  \begin{align*}
    \pi(\mu|\mathbf{x})&\propto f_\mathbf{x}(\mathbf{x}|\mu)\pi(\mu)\\
    &\propto \exp\left[-\frac{1}{2}\sum(x_i - \mu)^2\right]\exp\left[-\frac{\mu^2\tau^2}{2}\right]\\
    &\propto \exp\left[-\frac{1}{2}(n + \tau^2)\left\{\mu - \frac{\sum x_i}{n + \tau^2}\right\}^2\right]
  \end{align*}
  which is a normal distribution. So the posterior distribution of $\mu$ given $\mathbf{x}$ is a normal distribution with mean $\sum x_i/(n + \tau^2)$ and variance $1/(n + \tau^2)$.

  The normal density is symmetric, and so the posterior mean and the posterior media have the same value $\sum x_i/(n + \tau^2)$.

  This is the optimal estimator for both quadratic and absolute loss.
\end{eg}

\begin{eg}
  Suppose that $X_1, \cdots, X_n$ Are iid $\Poisson(\lambda)$ random variables, and $\lambda$ has an exponential distribution with mean $1$.So $\pi(\lambda) = e^{-\lambda}.$

  The posterior distribution is given by
  \[
    \pi(\lambda|\mathbf{x}) \propto e^{n\lambda} \lambda^{\sum x_i}e^{-\lambda} = \lambda^{\sum x_i}e^{-(n + 1)\lambda},\quad \lambda > 0,
  \]
  which is $\gammaD\left(\sum x_i + 1, n + 1\right)$. Hence under quadratic loss,
  \[
    \hat{\lambda} = \frac{\sum x_i + 1}{n + 1},
  \]
  the posterior mean.

  Under absolute error loss, $\hat{\lambda}$ solves
  \[
    \int_0^{\lambda} \frac{(n + 1)^{\sum x_i + 1}\lambda^{\sum x_i}e ^{-(n + 1)\lambda}}{(\sum x_i)!}\;\d \lambda = \frac{1}{2}.
  \]
\end{eg}

\section{Hypothesis testing}
\subsection{Simple hypotheses}
Let $X_1, \cdots, X_n$ be iid, each taking values in $\mathcal{X}$, each with unknown pdf/pmf $f$, and suppose that we have two hypotheses, $H_0$ and $H_1$ about $f$.

On the basis of data $\mathbf{X} = \mathbf{x}$, we make a choice between the two hypotheses.

\begin{eg}\leavevmode
  \begin{itemize}
    \item A coin has $\P(\text{Heads}) = \theta$, and is thrown independently $n$ times. We could have $H_0:\theta = \frac{1}{2}$ versus $H_1: \theta = \frac{3}{4}.$
    \item Suppose $X_1, .., X_n$ are iid discrete random variables. We could have $H_0:$ the distribution is Poisson with unknown mean, and $H_1:$ the distribution is not Poisson. This is not a goodness-of-fit test.
    \item General parametric cases: $X_1, \cdots , X_n$ are iid with density $f(x|\theta)$, with $H_0: \theta\in \Theta_0$ and $H_1:\theta\in \Theta_1$, with $\Theta_0\cap \Theta_1 = \emptyset$.
    \item We could have $H_0: f = f_0$ and $H_1 = f = f_1$, where $f_0$ and $f_1$ are densities that are completely specified but do not come form the same parametric family.
  \end{itemize}
\end{eg}

\begin{defi}[Simple hypotehsis]
  A \emph{simple hypothesis} $H$ specifies $f$ completely (eg. $H_0: \theta = \frac{1}{2}$). Otherwise, $H$ is a \emph{composite hypothesis} $H_1$.
\end{defi}

\begin{defi}[Critical region]
  For testing $H_0$ against an alternative hypothesis $H_1$, a test procedure has to partition $\mathcal{X}^n$ into two disjoint exhaustive regions $C$ and $\bar C$, such that if $\mathbf{x}\in C$, then $H_0$ is rejected, and if $\mathbf{x}\in \bar C$, then $H_00$ is not rejected. $C$ is the \emph{critical region}.
\end{defi}

When performing a test, we may either arrive at a correct conclusion, or make one of two types of error:
\begin{defi}[Type I and II error]
  \begin{enumerate}
    \item \emph{Type I error}: reject $H_0$ when $H_0$ is true.
    \item \emph{Type II error}: not rejecting $H_0$ when $H_0$ is false.
  \end{enumerate}
\end{defi}

\begin{defi}[Size and power]
When $H_0$ and $H_1$ are both simple, let
\[
  \alpha = \P(\text{Type I error}) = \P(\mathbf{X}\in C|H_0\text{ is true}).
\]
\[
  \beta = \P(\text{Type II error}) = \P(\mathbf{X}\not\in C|H_1\text{ is true}).
\]
The \emph{size} of the test is $\alpha$, and $1 - \beta$ is the \emph{power} of the test to detect $H_1$.
\end{defi}

\begin{defi}[Likelihood]
  The \emph{likelihood} of a simple hypothesis $H\theta = \theta^*$ given data $\mathbf{x}$ is
  \[
    L_x(H) = f_\mathbf{X}(\mathbf{x}|\theta = \theta^*).
  \]
  The \emph{likelihood ratio} of two simple hypotheses $H_0, H_1$ given data $\mathbf{x}$ is
  \[
    \Lambda_\mathbf{x}(H_0; H_1) = \frac{L_x(H_1)}{L_x(H_0)}.
  \]
  A \emph{likelihood ratio test} (LR test) is one where the critical region $C$ is of the form
  \[
    C = \{\mathbf{x}: \Lambda_x (H_0; H_1) > k\}
  \]
  for some $k$.
\end{defi}

\begin{lemma}[Neyman-Pearson lemma]
  Suppose $H_0: f = f_0$, $F_1: f = f_1$, where $f_0$ and $f_1$ are continuous densities that are nonzero on the same regions. Then among all tests of size less than or equal to $\alpha$, the test with smallest probability of a Type II error is given by $C = \{\mathbf{x}: f_1(\mathbf{x})/f_0(\mathbf{x}) > k\}$, where $k$ is chosen such that $\alpha=\P(\text{reject }H_0|H_0) = \P(\mathbf{X}\in C| H_0) = \int_C f_0(\mathbf{x})\;\d \mathbf{x}$.
\end{lemma}

\begin{proof}
  The given $C$ specifies a likelihood ratio test with size $\alpha$. Let
  \[
    \beta = \P(\mathbf{X}\not\in C|f_1) = \int_{\bar C}f_x(\mathbf{x})\;\d \mathbf{x}.
  \]
  Let $C^*$ be the critical region of any other test with size less than or equal to $\alpha$.

  Let $\alpha^* = \P(X\in C^*|f_0$ and $\beta^* = \P(\mathbf{X}\not\in C^*|f_1)$. We want to show $\beta \leq \beta^*$.

  We know $\alpha^* \leq \alpha$, ie
  \[
    \int_{C^*}f_0(\mathbf{x})\;\d \mathbf{x}\leq \int_Cf_0(\mathbf{x}) \;\d \mathbf{x}.
  \]
  Also, on $C$, we have $f_1(\mathbf{x}) > kf_0*(\mathbf{x})$, while on $\bar C$ we have $f_1(\mathbf{x}) \leq kf_0(\mathbf{x})$. So
  \[
    \int_{\bar C^*\cap C} f_1(\mathbf{x}) \;\d \mathbf{x} \geq k\int_{\bar C^*\cap C}f_0(\mathbf{x}) \;\d \mathbf{x},\quad \int_{\bar C\cap C^*}f_1(\mathbf{x}) \;\d \mathbf{x} \leq k\int_{\bar C\cap C^*}f_0 (\mathbf{x})\;\d \mathbf{x}.
  \]
  Hence
  \begin{align*}
    \beta - \beta^* &= \int_{\bar C}f_x(\mathbf{x}) \;\d \mathbf{x} - \int_{\bar C^*}f_1(\mathbf{x})\;\d \mathbf{x}\\
    &= \int_{\bar C\cap C^*} f_1(\mathbf{x}) \;\d \mathbf{x} + \int_{\bar C\cap \bar C^*}f_1(\mathbf{x})\;\d \mathbf{x} \\
    &\quad - \int_{\bar C^*\cap C} f_1(\mathbf{x}) \;\d \mathbf{x} - \int_{\bar C\cap \bar C^*}f_1(\mathbf{x})\;\d \mathbf{x}\\
    &\leq k\int_{\bar C \cap C^*}f_0(\mathbf{x})\;\d \mathbf{x} - k\int_{\bar C^*\cap C}f_0(\mathbf{x})\;\d \mathbf{x}\\
    &= k\left\{\int_{\bar C\cap C^*}f_0(\mathbf{x})\;\d \mathbf{x} + \int_{C \cap C^*}f_0(\mathbf{x})\;\d \mathbf{x}\right\} \\
    &\quad- k\left\{\int_{\bar C^*\cap C}f_0(\mathbf{x})\;\d \mathbf{x} + \int_{C\cap C^*}f_0(\mathbf{x})\;\d \mathbf{x}\right\}\\
    &= k(\alpha^* - \alpha)\\
    &\leq 0.
  \end{align*}
  % diagram
\end{proof}

\note We assume continuous densities to ensure that a LR test of exactly size $\alpha$ exists. Then Neyman-Pearson Lemma shows that $\alpha$ and $\beta$ cannot both be arbitrarily small. It says that the most powerful test (ie the one with the smallest Type II error probability), among tests with size smaller or equal to $\alpha$, is the size $\alpha$ likelihood ratio test.

Thus we should fix $\P(\text{Type I error})$ at some level $\alpha$ and then use the Neyman-Pearson Lemma to find the best test.

Here the hypothesis are not treated symmetrically. $H_0$ has precedence over $H_1$ and a Type I error is treated as more serious than a Type II error (eg it is more serious to claim an innocent person is guilty than to say a guilty person is innocent)

\begin{defi}[Null and alternative hypothesis]
  $H_0$ is the null hypothesis and $H_1$ is the alternative hypothesis. The null hypothesis is that of ``no change'' (eg a drug is not helpful), and is assumed unless there is significant reason not to think so. The alternative hypothesis a departure from $H_0$ that is of interest to us.

  We would rather not reject $H_0$ when we should, that to reject $H_0$ when we shouldn't.
\end{defi}

\begin{eg}
  Suppose $X_1, \cdots, X_n$ are iid $N(\mu, \sigma_0^2)$, where $\sigma_0^2$ is known. We want to find the best size $\alpha$ test of $H_0: \mu = \mu_0$ against $H_1: \mu - \mu_1$, where $\mu_0$ and $\mu_1$ are known fixed values with $\mu_1 > \mu_0$. Then
  \begin{align*}
    \Lambda_\mathbf{x}(H_0; H_1) &= \frac{(2\pi\sigma_0^2)^{-n/2}\exp\left(-\frac{1}{2\sigma^2_0}\sum(x_i - \mu_1)^2\right)}{(2\pi\sigma_0^2)^{-n/2}\exp\left(-\frac{1}{2\sigma^2_0}\sum(x_i - \mu_0)^2\right)}\\
    &= \exp\left(\frac{\mu_1 - \mu_0}{\sigma_0^2}n\bar x + \frac{n(\mu_0^2 - \mu_1^2)}{2\sigma_0^2}\right).
  \end{align*}
  This is an increasing function of $\bar x$, so for any $k$, $\Lambda_x > k\Leftrightarrow \bar x > c$ for some $c$. Hence we reject $H_0$ if $\bar x > c$, where $c$ is chosen such that $\P(\bar X > c | H_0) = \alpha$.

  Under $H_0$, $\bar X \sim N(\mu_0. \sigma_0^2/n)$, so $Z = \sqrt{n}(\bar X - \mu_0)/\sigma_0 \sim N(0, 1)$.

  Since $\bar x > c\Leftrightarrow z > c'$ for some $c'$, the size $\alpha$ test rejects $H_0$ if
  \[
    z = \frac{\sqrt{n}(\bar x - \mu_0)}{\sigma_0} > z_\alpha.
  \]
  For example, suppose $\mu_0 = 5$, $\mu_1 = 6$, $\sigma_0 = 1$, $\alpha = 0.05$, $n = 4$ and $\mathbf{x} = (5.1, 5.5, 4.9, 5.3)$. So $\bar x = 5.2$.

  From tables, $z_{0.05} = 1.645$. We have $z = 0.4$ and this is less than $1.645$. So $\mathbf{x}$ is not in the rejection region.

  We do not reject $H_0$ at the 5\% level; the data are consistent with $H_0$.

  This does NOT mean that $H_0$ is ``true'', just that it is not false.

  This is called a $z$-\emph{test}.
\end{eg}

In this example, LR tests reject $H_0$ if $z > k$ for some constant $k$. The size of such a test is $\alpha = \P(Z > k| H_0) = 1 - \Phi(k)$, and is decreasing as $k$ increasing.
Our observed value $z$ will be in the rejected region iff $z > k \Leftrightarrow \alpha > p^* = \P(Z > z|H_0)$.
\begin{defi}[$p$-value]
  The quantity $p^*$ is called the \emph{$p$-value} of our observed data $\mathbf{x}$. For the example above, $z = 0.4$ and so $p^* = 1 - \Phi(0.4) = 0.3446$.
\end{defi}

In general, the $p$-value is sometimes called the ``observed significance level'' of $\mathbf{x}$ and is the probability under $H_0$ of seeing data that are ``more extreme'' than our observed data $\mathbf{x}$. Extreme observations are viewed as providing evidence against $H_0$.

\note The $p$-value has Uniform$(0, 1)$ pdf under the null hypothesis.
\subsection{Composite hypotheses}
For composite hypotheses like $H:\theta \geq 0$, the error probabilities do not have a single value. We define
\begin{defi}[Power function]
  The \emph{power function} is
  \[
    W(\theta) = \P(\mathbf{X}\in C|\theta) = \P(\text{reject }H_0|\theta),
  \]
\end{defi}
We want $W(\theta)$ to be small on $H_0$ and large on $H_1$.

\begin{defi}[Size]
  The \emph{size} of the test is
  \[
    \alpha =\sup_{\theta\in \Theta_0}W(\theta),
  \]
\end{defi}
It is the worst possible size.

For $\theta\in \Theta_1$, $1 - W(\theta) = \P(\text{Type II error}|\theta)$.

Sometimes the Neyman-Pearson theory can be extended to one-sided alternatives.

For example, in the previous example, we have shown that the most powerful size $\alpha$ test of $H_0: \mu = \mu_0$ versus $H_1: \mu = \mu_1$ (where $\mu_1 > \mu_0$) is given by
\[
  C = \{x: \frac{\sqrt{n}(\bar x - \mu_0)}{\sigma_0} >  z_\alpha\}.
\]
The critical region depends on $\mu_0, n \sigma_0, \alpha$, and the fact that $\mu_1 > \mu_0$. It does NOT depend on the particular value of $\mu_1$. This test is then uniformly the most powerful size $\alpha$ for testing $H_0: \mu = \mu_0$ against $H_1: \mu> \mu_0$.

\begin{defi}[Uniformly most powerful test]
  A test specified by a critical region $C$ is \emph{uniformly most powerful} (UMP) size $\alpha$ test for test $H_0:\theta\in \Theta_0$ against $H_1: \theta \in \Theta_1$ if
  \begin{enumerate}
    \item $\sup_{\theta\in \Theta_0} W(\theta) = \alpha$.
    \item For any other test $C^*$ with size $\leq \alpha$ and with power function $W^*$, we have $W(\theta) \geq W^*(\theta)$ for all $\theta\in \Theta_1$.
  \end{enumerate}
  Note that these may not exist. However, likelihood ratio test often work.
\end{defi}

\begin{eg}
  Suppose $X_1, \cdots, X_n$ are iid where $\sigma_0$ is known, and we wish to test $H_0: \mu\leq \mu_0$ against $H_1: \mu > \mu_0$.

  First consider testing $H_0': \mu = \mu_0$ against $H_1': \mu = \mu_1$, where $\mu_1 > \mu_0$. The Neyman-Pearson test of size $\alpha$ of $H_0'$ against $H_1'$ has
  \[
    C = \left\{x: \frac{\sqrt{n}(\bar x - \mu_0)}{\sigma_0} >  z_\alpha\right\}.
  \]
  We show that $C$ is in fact UMP for the composite hypotheses $H_0$ against $H_1$. For $\mu\in \R$, the power function is
  \begin{align*}
    W(\mu) &= \P_\mu(\text{reject } H_0)\\
    &= \P_\mu\left(\frac{\sqrt{n}(\bar X - \mu_0)}{\sigma_0} > z_\alpha\right)\\
    &= \P_\mu\left(\frac{\sqrt{n}(\bar X - \mu)}{\sigma_0} > z_\alpha + \frac{\sqrt{n}(\mu_0 - \mu)}{\sigma_0}\right)\\
    &= 1 - \Phi\left(z_\alpha + \frac{\sqrt{n}(\mu_0 - \mu)}{\sigma_0}\right)
  \end{align*}
  To show this is UMP, we know that $W(\mu_0) = \alpha$ (by plugging in). $W(\mu)$ is an increasing function of $\mu$. So
  \[
    \sup_{ \mu \leq \mu_0} W(\mu) =- \alpha.
  \]
  So the first condition is satisfied.

  For the second condition, observe that for any $\mu > \mu_0$, the Neyman Pearson size $\alpha$ test of $H_0'$ vs $H_1'$ has critical region $C$. Let $C^*$ and $W^*$ belong to any other test of $H_0$ vs $H_1$ of size $\leq \alpha$. Then $C^*$ can be regarded as a test of $H_0$ vs $H_1$ of size $\leq \alpha$, and NP-Lemma says that $W^*(\mu_1) \leq W(\mu_1)$. This holds for all $\mu_1 > \mu_0$. So the condition is satisfied and it is UMP.
\end{eg}
We now consider likelihood ratio tests for more general situations.
\begin{defi}[Likelihood of a composite hypothesis]
  The \emph{likelihood} of a composite hypothesis $H:\theta\in \Theta$ given data $\mathbf{x}$ to be
  \[
    L_x(H) = \sup_{\theta\in \Theta}f(\mathbf{x}|\theta).
  \]
\end{defi}
So far we have considered disjoint hypotheses $\Theta_0, \Theta_1$, but we are not interested in any specific alternative. So it is easier to take $\Theta_1 = \Theta$ rather than $\Theta \setminus\Theta_0$. Then
\[
  \Lambda_x (H_0; H_1) = \frac{L_x(H_1}{L_x(H_0)}=\frac{\sup_{\theta\in \Theta_1}f(\mathbf{x}|\theta)}{\sup_{\theta\in \Theta_0}f(\mathbf{x}|\theta)} \geq 1,
\]
with large values of $\Lambda$ indicating departure from $H_0$.

\begin{eg}
  Single sample: testing a given mean, known variance ($z$-test). Suppose that $X_1, \cdots, X-n$ are iid $N(\mu, \sigma_0^2)$, with $\sigma_0^2$ known, and we wish to test $H_0: \mu = \mu_0$ against $H_1 = \mu \not= \mu_0$ (for given constant $\mu_0$).

  Here $\Theta_0 = \{\mu_0\}$ and $\Theta = \R$.
  For the denominator, we have $\sup_\Theta f(\mathbf{x}|\mu) = f(\mathbf{x}|\hat{\mu})$, where $\hat{\mu}$ is the mle. We know that $\hat{\mu} = \bar x$. Hence
  \begin{align*}
    \Lambda_\mathbf{x}(H_0; H_1) &= \frac{(2\pi\sigma_0^2)^{-n/2}\exp\left(-\frac{1}{2\sigma^2_0}\sum(x_i - \bar x)^2\right)}{(2\pi\sigma_0^2)^{-n/2}\exp\left(-\frac{1}{2\sigma^2_0}\sum(x_i - \mu_0)^2\right)}\\
    &= \exp\left(\frac{\mu_1 - \mu_0}{\sigma_0^2}n\bar x + \frac{n(\mu_0^2 - \mu_1^2)}{2\sigma_0^2}\right).
  \end{align*}
  Then $H_0$ is rejected if $\Lambda_x$ is large.

  To make our lives easier, we can use the logarithm instead:
  \[
    2\log \Lambda(H_0;H_1) = \frac{1}{\sigma_0^2}\left[\sum (x_i - \mu_0)^2 - \sum (x_i - \bar x)^2\right] = \frac{n}{\sigma_0^2}(\bar x - \mu_0)^2.
  \]
  So we can reject $H_0$ if we have 
  \[
    \left|\frac{\sqrt{n}(\bar x - \mu_0)}{\sigma_0}\right| > c
  \]
  for some $c$. 
  
  We know that under $H_0$, $\displaystyle Z = \frac{\sqrt{n}(\bar X - \mu_0)}{\sigma_0}\sim N(0, 1)$. So the size $\alpha$ generalised likelihood test rejects $H_0$ if
  \[
    \left|\frac{\sqrt{n}(\bar x - \mu_0}{\sigma_0}\right| > z_{\alpha/2}.
  \]
  Alternatively, since $\displaystyle \frac{n(\bar X - \mu_0)}{\sigma_0^2}\sim \chi_1^2$, we reject $H_0$ if
  \[
    \frac{n(\bar X - \mu_0)^2}{\sigma_0^2} > \chi_1^2(\alpha),
  \]
  (check that $z_{\alpha/2}^2 = \chi_1^2(\alpha)$).

  Note that this is a two-tailed test - ie. reject $H_0$ both for high and low values of $\bar x$.
\end{eg}

The next theorem allows us to use likelihood ratio tests even when we cannot find the exact relevant null distribution.

First consider the ''size`` or ''dimension`` of our hypotheses: suppose that $H_0$ imposes $p$ independent restrictions on $\Theta$. So for example, if $\Theta = \{\theta: \theta = (\theta_1, \cdots, \theta_k)\}$, and we have
\begin{itemize}
  \item $H_0: \theta_{i_1} = a_1, \theta_{i_2} = a_2, \cdots \theta_{i_p} = a_p$
  \item $H_0: A\theta = \mathbf{b}$ (with $A$ $p\times k$, $\mathbf{b}$ $p\times 1$ given)
  \item $H_0: \theta_i = f_i(\varphi), i = 1, \cdots, k$ for some $\varphi = (\varphi_1, \cdots, \varphi_{k - p}$
\end{itemize}
We say $\Theta$ has $k$ free parameters and $\Theta_0$ has $k - p$ free parameters. We write $|\Theta_0| = k - p$ and $|\Theta| = k$.

\begin{thm}[Generalized likelihood ratio theorem]
  Suppose $\Theta_0 \subseteq \Theta_1$ and $|\Theta_1| - |\Theta_0| = p$. Then under regularity conditions, as $n\to \infty$, with $\mathbf{X} = (X_1, \cdots, X_n)$ , all $X_i$ iid, we have, if $H_0$ is true,
  \[
    2\log \Lambda_\mathbf{X}(H_0:H_1)\sim \chi_p^2.
  \]
  If $H_0$ is not true, then $2\log \Lambda$ tends to be larger. We reject $H_0$ if $2\log \Lambda > c$, where $c = \chi_p^2(\alpha)$ for a test of approximately size $\alpha$.
\end{thm}

For example, in our example above, $|\Theta_1| - |\Theta_0| = 1$, and in this case, we saw that under $H_0$, $2\log \Lambda \sim \chi_1^2$ \emph{exactly} for all $n$ in that particular case, rather than just approximately.

\subsection{Tests of goodness-of-fit and independence}
\subsubsection{Goodness-of-fit of a fully-specified null distribution}
Suppose the observation space $\mathcal{X}$ is partitioned into $k$ sets, and let $p_i$ be the probability that an observation is in set $i$ for $i = 1, \cdots, k$.

Consider testing $H_0:$ the $p_i$'s arise from a fully specified model against $H_1:$ the $p_i$'s are unrestricted (but we must still have $p_i \geq 0, \sum p_i = 1$, of course).

\begin{eg}
  Birth month of admissions to Oxford and Cambridge in 2012.
  
  \noindent\begin{tabular}{cccccccccccc}
    \toprule
    Sep & Oct & Nov & Dec & Jan & Feb & Mar & Apr & May & Jun & Jul & Aug\\
    470 & 515 & 470 & 457 & 473 & 381 & 466 & 457 & 437 & 396 & 384 & 394\\
    \bottomrule
  \end{tabular}

  Is this compatible with a uniform distribution over the year?

  Out of $n$ independent observations, let $N_i$ be the number of observations in $i$th set. So $(N_1,\cdots, N_k)\sim \multinomial(n; p_1, \cdots, p_n)$.

  For a generalized likelihood ratio test of $H_0$, we need to find the maximised likelihood under $H_0$ and $H_1$.

  Under $H_1$: $\like(p_1, \cdots, p_k) \propto p_1^{m_1}\cdots p_k^{m_k}$. So the log likelihood is $l = \text{constant} + \sum n_i \log p_i$. We want to maximise this subject to $\sum p_i = 1$. Using the Lagrange multiplier, we will find that the mle is$\hat{p_i}/n$. Also $|\Theta_1| = k - 1$ (not $k$, since they must sum up to 1).

  Under $H_0$, the values of $p_i$ are specified completely, say $p_i = \tilde{p}_i$. So $|\Theta_0| = 0$. We find that
  \[
    2\log \Lambda = 2\log\left(\frac{\hat{p}_1^{n_1}\cdots \hat{p}_k^{n_k}}{\tilde{p}_{1}^{n_1}\cdots \tilde{p}_k^{n_k}}\right) = 2\sum n_i \log \left(\frac{n_i}{n\tilde{p}_i}\right)\tag{1}
  \]

  Here $|\Theta_1| - |\Theta_0| = k - 1$. So we reject $H_0$ if $2\log \Lambda > \chi_{k - 1}^2(\alpha)$ for an approximate size $\alpha$ test.

  Under $H_0$ (no effect of month of birth), $\tilde{p}_i$ is the proportion of births in month $i$ in 1993/1994 - this is \emph{not} simply proportional to the number of days in month, as there is for example an excess of September births (the ``Christmas effect''). So

  Then
  \[
    2\log \Lambda = 2\sum n_i \log\left(\frac{n_i}{n\tilde{p}_i}\right) = 44.9.
  \]
  $\P(\chi_11^2 > 44.86) = 3\times 10^{-9}$, which is our $p$-value. Since this is certainly less than 0.001, we can reject $H_0$ At the $0.1\%$ level, or can say ``significant at the $0.1\%$ level''.

  \note The traditional levels for comparison are $\alpha = 0.04, 0.01, 0.001$, roughly corresponding to ``evidence'', ``strong evidence'' and ``very strong evidence''.
\end{eg}
A similar common situation has $H_0: p_i = p_i(\theta)$ for some parameter $\theta$ and $H_1$ as before. Now $|\Theta_0|$ is the number of independent parameters to be estimated under $H_0$.

Under $H_0$, we find mle $\hat{\theta}$ by maximizing $n_i \log p_i (\theta)$, and then
\[
  2\log \Lambda = 2\log \left(\frac{\hat{p_1}^{n_1}\cdots \hat{p_k}^{n_k}}{p_1(\hat{\theta})^{n_1}\cdots p_k (\hat{\theta})^{n_1}}\right) = 2\sum n_i \log \left(\frac{n_i}{np_i(\hat{\theta})}\right).\tag{2}
\]
The degrees of freedom are $k - 1 - |\Theta_0|$.

\subsubsection{Pearson's Chi-squared test}
Notice that the two log likelihoods are of the same form. Let $o_i = n_i$ (observed number) and let $e_i = n\tilde{p_i}$ or $np_i(\hat{\theta})$. Let $\delta_i = o_i - e_i$. Then
\begin{align*}
  2\log \Lambda &= 2\sum o_i \log \left(\frac{o_i}{e_i}\right)\\
  &= 2\sum (e_i + \delta_i) \log\left(1 + \frac{\delta_i}{e_i}\right)\\
  &= 2\sum \left(\delta_i + \frac{\delta_i^2}{e_i} - \frac{\delta_i^2}{2e_i}\right)\\
  \intertext{We know that $\sum \delta_i = 0$ since $\sum e_i = \sum o_i$. So}
  &= \sum \frac{\delta_i^2}{e_i}\\
  &= \sum\frac{(o_i - e_0)^2}{e_i},
\end{align*}
where we have assumed $\log\left(1 + \frac{\delta_i}{e_i} - \frac{\delta_i^2}{2e_i^2}\right)$.

\begin{eg}
  Mendel crossed 556 smooth yellow male peas with wrinkled green peas. From the progeny let
  \begin{enumerate}
    \item $N_1$ be the number of smooth yellow peas,
    \item $N_2$ be the number of smooth green peas,
    \item $N_3$ be the number of wrinkled yellow peas,
    \item $N_4$ be the number of wrinkled green peas.
  \end{enumerate}
  We wish to test the goodness of fit of the model $H_0: (p_1, p_2, p_3, p_4) = \left(\frac{9}{16}, \frac{3}{16}, \frac{3}{16}, \frac{1}{16}\right)$.

  We find $(e_1, e_2, e_3, e_4) = (312.75, 104.25, 104.25, 34.75)$. So $2\log \Lambda = 0.618$ and $\sum \frac{(o_i - e_i)^2}{e_i} = 0.604$.

  Here $|\Theta_0| = 0$ and $|\Theta_1| = 4 - = 3$. So we refer to test statistics $\chi_3^2(\alpha)$.

  Since $\chi_3^2(0.05) = 7.815$, we see that neither value is significant at $5\%$. So there is no evidence against Mendel's theory. In fact, the $p$-value is approximately $\P(\chi_3^2 > 0.6) \approx 0.96$.
\end{eg}

\begin{eg}
  In a genetics problem, each individual has one of the three possible genotypes, with probabilities $p_1, p_2, p_3$. Suppose we wish to test $H_0: p_o = p_i(\theta)$, where
  \[
    p_1(\theta) = \theta^2,\quad p_2 2\theta(1 - \theta), \quad p_3(\theta) = (1 - \theta)^2.
  \]
  for some $\theta \in (0, 1)$.

  We observe $N_i = n_i$. Under $H_0$, the mle $\hat{\theta}$ is found by maximising
  \[
    \sum n_i \log p_i(\theta) = 2n_1 \log \theta + n_2\log(2\theta(1 - \theta)) + 2n_3 \log (1 - \theta).
  \]
  We find that $\hat{\theta} = \frac{2n_1 + n_2}{2n}$. Also, $|\Theta_0| = 1$ and $|\Theta_1| = 2$.

  Now substitute $p_i(\hat{\theta}$ into (2), or find the corresponding Pearson's chi-squared statistic, and refer to $\chi_1^2$.
\end{eg}

\subsubsection{Testing independence in contingency tables}
\begin{defi}[Contingency table]
  A \emph{contingency table} is a table in which observations or individuals are classified according to one or more criteria.
\end{defi}

\begin{eg}
  500 people with recent car changes were asked about their previous and new cars. The results are as follows:

  \begin{tabular}{ccccc}
    \toprule
     & & & New car &\\
     & & Large & Medium & Small\\\midrule
     \multirow{3}{*}{\rotatebox[origin=c]{90}{Previous}\;\rotatebox[origin=c]{90}{car}}& Large & 56 & 52 & 42\\
     & Medium & 50 & 83 & 67\\
     & Small & 18 & 51 & 81\\\bottomrule
  \end{tabular}

  This is a two-way contingency table: Each person is classified according to the previous car size and new car size.
\end{eg}
Consider a two-way contingency table with $r$ rows and $c$ columns. For $i = 1, \cdots, r$ And $j = 1, \cdots, c$, let $p_{ij}$ be the probability that an individual selected from the population under consideration is classified in row $i$ and column $j$. (ie. in the $(i, j)$ cell of the table).

Let $p_{i+} = \P(\text{in row }i)$ and $p_{+j} = \P(\text{in column }j)$. Then we must have $p_{++} = \sum_i \sum_j p_{ij} = 1$.

Suppose a random sample of $n$ individuals is taken, and let $n_{ij}$ be the number of these classified in the $(i, j)$ cell of the table.

Let $n_{i+} = \sum_j n_{ij}$ and $n_{+j} = \sum_i n_{ij}$. So $n_{++} = n$.

We have
\[
  (N_{11}, N_{12}, \cdots, N_{1c}, n_{21}, \cdots, N_{rc}) \sim \multinomial (n; p_{11}, p_{12}, \cdots, p_{1c}, p_{21}, \cdots, p_{rc}).
\]

We may be interested in testing the null hypothesis that the two classifications are independent. So we test
\begin{itemize}
  \item $H_0$: $p_i = p_{i+}p_{+j}$ for all $i, j$, ie. independence of columns and rows.
  \item $H_1$: $p_{ij}$ are unrestricted.
\end{itemize}
Of course we have the usual restrictions like $p_{++} = 1$, $p_{ij} \geq 0$.

Under $H_1$, the mle's are $\hat{p_{ij}} = \frac{n_{ij}}{n}$.

Under $H-0$, using Lagrangian methods, the mle's are $\hat{p}_{i+} = \frac{n_{i+}}{n}$ and $\hat{p}_{+j} = \frac{n_{+j}}{n}$.

Write $o_{ij} = n_{ij}$ and $e_{ij} = n\hat{p}_{i+}\hat{p}_{+j} = n_{i+}n_{+j}/n$.

Then
\[
  2\log \Lambda = 2\sum_{i = 1}^r \sum_{j = 1}^c o_{ij}\log\left(\frac{o_{ij}}{e_{ij}}\right) \approx \sum_{i = 1}^r \sum_{j = 1}^c \frac{(o_{ij} - e_{ij})^2}{e_{ij}}
\]
using the same approximating steps for Pearson's Chi-squared test.

We have $|\Theta_1| = rc - 1$, because under $H_1$ the $p_{ij}$'s sum to one. Also, $|\Theta_0| = (r - 1) + (c - 1)$ because $p_{1+}, \cdots, p_{r+}$ must satisfy $\sum_i p_{i+} = 1$ and $p_{+1}, \cdots, p_{+c}$ must satisfy $\sum_j p_{+j} = 1$. So
\[
  |\Theta_1| - |\Theta_0| = rc - 1 - (r - 1) - (c - 1) = (r - 1)(c - 1).
\]

\begin{eg}
  In our previous example, we wish to test $H_0$: the new and previous car sizes are independent. The actual data is:

  \begin{tabular}{cccccc}
    \toprule
     & & & New car &\\
     & & Large & Medium & Small & \textbf{Total}\\\midrule
     \multirow{4}{*}{\rotatebox[origin=c]{90}{Previous}\;\rotatebox[origin=c]{90}{car}}& Large & 56 & 52 & 42 & \textbf{150}\\
     & Medium & 50 & 83 & 67 & \textbf{120}\\
     & Small & 18 & 51 & 81 & \textbf{150}\\\cmidrule{2-6}
     & \textit{Total} & \textit{124} & \textit{186} & \textit{190} & \textit{\textbf{500}}\\\bottomrule
  \end{tabular}

  \vspace{3pt}
  while the expected values given by $H_0$ is
  \vspace{3pt}

  \begin{tabular}{cccccc}
    \toprule
     & & & New car &\\
     & & Large & Medium & Small & \textbf{Total}\\\midrule
     \multirow{4}{*}{\rotatebox[origin=c]{90}{Previous}\;\rotatebox[origin=c]{90}{car}}& Large & 37.2 & 55.8 & 57.0 & \textbf{150}\\
     & Medium & 49.6 & 74.4 & 76.0 & \textbf{120}\\
     & Small & 37.2 & 55..8 & 57.0 & \textbf{150}\\\cmidrule{2-6}
     & \textit{Total} & \textit{124} & \textit{186} & \textit{190} & \textit{\textbf{500}}\\\bottomrule
  \end{tabular}

  Note the margins are the same. It is quite clear that they do not match well, but we can find the $p$ value to be sure.

  $\displaystyle\sum\sum \frac{(o_{ij} - e_{ij})^2}{e_{ij}} = 36.20$, and the degrees of freedom is $(3 - 1)(3 - 1) = 4$.

  From the tables, $\chi_4^2(0.05) = 9.488$ and $\chi_4^2(0.01) = 13.28$.

  So our observed value of 36.20 is significant at the $1\%$ level, ie. there is strong evidence against $H_0$. So we conclude that the new and present car sizes are not independent.
\end{eg}
\subsection{Tests of homogeneity, and connections to confidence intervals}
\subsubsection{Tests of homogeneity}
\begin{eg}
  150 patients were randomly allocated to three groups of 50 patients each. Two groups were given a new drug at different dosage levels, and the third group received a placebo. The responses were as shown in the table below.

  \begin{tabular}{ccccc}
    \toprule
    & Improved & No difference & Worse & \textbf{Total}\\\midrule
    Placebo & 18 & 17 & 15 & \textbf{50}\\
    Half dose & 20 & 10 & 20 & \textbf{50}\\
    Full dose & 25 & 13 & 12& \textbf{50}\\\midrule
    \textit{Total} & \textit{63} & \textit{40} & \textit{47} & \textbf{\textit{150}}\\\bottomrule
  \end{tabular}

  Here the row totals are fixed in advance, in contrast to our last section, where the row totals are random variables.

  For the above, we may be interested in testing $H_0:$ the probability of ``improved'' is the same for each of the three treatment groups, and so are the probabilities of ``no difference'' and ``worse'', ie. $H_0$ says that we have homogeneity down the rows.
\end{eg}
In general, we have independent observations from $r$ multinomial distributions, each of which has $c$ categories,

ie. we observe an $r\times c$ table $(n_{ij})$, for $i = 1, \cdots, r$ and $j = 1, \cdots, c$, where
\[
  (N_{i1}, \cdots, N_{ic}) \sim \multinomial(n_{i+}, p_{i1}, \cdots, p_{ic})
\]
independently for each $i = 1, \cdots, r$.
We want to test
\[
  H_0: p_{1j} = p_{2j} = \cdots = p_{rj} = p_j,
\]
for $j = 1, \cdots, c$,  and
\[
  H_1: p_{ij}\text{ are unrestricted}.
\]
Using $H_1$, 
\[
  \like((p_{ij})) = \prod_{i = 1}^r \frac{n_{i+}!}{n_{i1}!\cdots n_{ic}!}p_{i1}^{n_{i1}} \cdots p_{ic}^{n_{ic}},
\]
and
\[
  \log\like = \text{constant} + \sum_{i = 1}^r \sum_{j = 1}^c n_{ij}\log p_{ij}.
\]
Using Lagrangian methods, we find that $\hat{p}_{ij} = \frac{n_{ij}}{n_{i+}}$.

Under $H_0$,
\[
  \log\like = \text{constant} + \sum_{j = 1}^c n_{+j}\log p_j.
\]
By Lagrangian methods, we have $\hat{p}_j = \frac{n_{+j}}{n_{++}}$.

Hence
\[
  2\log \Lambda = \sum_{i = 1}^{r}\sum_{j = 1}^c n_{ij}\log\left(\frac{\hat{p}_{ij}}{\hat{p}_j}\right) = 2\sum_{i = 1}^r\sum_{j = 1}^c n_{ij}\log\left(\frac{n_{ij}}{n_{i+}n_{+j}/n_{++}}\right),
\]
which is the same as what we had last time, when the row totals are unrestricted!

We have $|\Theta_1| = r(c - 1)$ and $|\Theta_0| = c - 1$. So the degrees of freedom is $r(c - 1) - (c - 1) = (r - 1)(c - 1)$, and under $H_0$, $2\log\Lambda$ is approximately $\chi^2_{r - 1)(c - 1)}$. Again, it is exactly the same as what we had last time!

We reject $H_0$ if $2\log \Lambda > \chi_{(r - 1)(c - 1)}^2 (\alpha)$ for an approximate size $\alpha$ test.

If we let $o_{ij}= n_{ij}, e_{ij} = \frac{n_{i+}n_{+j}}{n_{++}}$, and $\delta_{ij} = o_{ij} - e_{ij}$, using the same approximating steps as for Pearson's Chi-squared to obtain
\[
  2\log \Lambda \approx \sum \frac{(o_{ij} - e_{ij})^2}{e_{ij}}.
\]
\begin{eg}
  Continuing our previous example, our data is

  \begin{tabular}{ccccc}
    \toprule
    & Improved & No difference & Worse & \textbf{Total}\\\midrule
    Placebo & 18 & 17 & 15 & \textbf{50} \\
    Half dose & 20 & 10 & 20 & \textbf{50} \\
    Full dose & 25 & 13 & 12& \textbf{50} \\\midrule
    \textit{Total} & \textit{6}3 & \textit{40} & \textit{47} & \textbf{\textit{150}} \\ \bottomrule
  \end{tabular}

  The expected under $H_0$ is

  \begin{tabular}{ccccc}
    \toprule
    & Improved & No difference & Worse &\textbf{Total}\\\midrule
    Placebo   & 21 & 13.3 & 15.7 & \textbf{50} \\
    Half dose & 21 & 13.3 & 15.7 & \textbf{50}\\
    Full dose & 21 & 13.3 & 15.7 & \textbf{50}\\\midrule
    \textit{Total}& \textit{63} & \textit{40} & \textit{47} & \textbf{\textit{150}}\\ \bottomrule
  \end{tabular}

  We find $2\log \Lambda = 5.129$, and we refer this to $\chi_4^2$. Clearly this is not significant, as the mean of $\chi_4^2$ is $4$, and is something we would expect to happen solely by chance.

  We can calculate the $p$-value: from tables, $\chi_4^2(0.05) = 9.488$, so our observed value is not significant at $5\%$, and the data are consistent with $H_0$.

  We conclude that there is no evidence for a difference between the drug at the given doses and the placebo.

  For interest,
  \[
    \sum\frac{(o_{ij} - e_{ij})^2}{e_{ij}} = 5.173,
  \]
  giving the same conclusion.
\end{eg}
\subsubsection{Confidence intervals and hypothesis tests}
Confidence intervals or sets can be obtained by inverting hypothesis tests, and vice versa

\begin{defi}[Acceptance region]
  The \emph{acceptance region} $A$ of a test is the complement of the critical region $C$.

  Note that when we say ``acceptance'', we really mean ``non-rejection''! The name is purely for historical reasons.
\end{defi}
Suppose $X_1, \cdots, X_n$ have joint pdf $f_\mathbf{X}(\mathbf{x}|\theta)$ for $\theta\in \Theta$

\begin{thm}[]\leavevmode
  \begin{enumerate}
    \item Suppose that for every $\theta_0\in \Theta$ there is a size $\alpha$ test of $H_0: \theta = \theta_0$. Denote the acceptance region by $A(\theta_0)$. Then the set $I(\mathbf{X}) = \{\theta:\mathbf{X}\in A(\theta)\}$ is a $100(1 - \alpha)\%$ confidence set for $\theta$.
    \item Suppose $I(\mathbf{X})$ is a $100(1 - \alpha)\%$ confidence set for $\theta$. Then $A(\theta_0) = \{\mathbf{X}: \theta_0 \in I(\mathbf{X})\}$ is an acceptance region for a size $\alpha$ test of $H_0: \theta = \theta_0$.
  \end{enumerate}
\end{thm}
Intuitively, this says that ``confidence intervals'' and ``hypothesis acceptance/rejection'' are the same thing. After gathering some data $\mathbf{X}$, we can produce a, say, $95\%$ confidence interval $(a, b)$. Then if we want to test the hypothesis $H_0: \theta = \theta_0$, we simply have to check whether $\theta_0 \in (a, b)$.

On the other hand, if we have a test for $H_0: \theta = \theta_0$, then the confidence interval is all $\theta_0$ in which we would accept $H_0: \theta = \theta_0$.
\begin{proof}
  First note that $\theta_0\in I(\mathbf{X})$ iff $\mathbf{X}\in A(\theta_0)$.

  For (i), since the test is size $\alpha$, we have 
  \[
    \P(\text{accept }H_0|H_0\text{ is true}) = \P(\mathbf{X}\in A_(\theta_0)|\theta=\theta_0) = 1 - \alpha.
  \]
  And so
  \[
    \P(\theta_0\in I(\mathbf{X})|\theta = \theta_0) = \P(\mathbf{X}\in A(\theta_0)|\theta = \theta_0) = 1 - \alpha.
  \]
  For (ii), since $I(\mathbf{X})$ is a $100(1 - \alpha)\%$ confidence set, we have
  \[
    P(\theta_0\in I(\mathbf{X})| \theta = \theta_0) = 1- \alpha.
  \]
  So
  \[
    \P(\mathbf{X}\in A(\theta_0)|\theta = \theta_0) = \P(\theta\in I(\mathbf{X})|\theta = \theta_0) = 1- \alpha.
  \]
\end{proof}

\begin{eg}
  Suppose $X_1, \cdots, X_n$ are iid $N(\mu, 1)$ random variables and we want a $95\%$ confidence set for $\mu$.

  One way is to use the theorem and find the confidence set that belongs to the hypothesis test that we found in the previous example. We find a test of size 0.05 of $H_0 : \mu= \mu_0$ against $H_1: \mu\not= \mu_0$ that rejects $H_0$ when $|\sqrt{n}(\bar x - \mu_0)| > 1.96$ (where 1.96 is the upper $2.5\%$ point of $N(0, 1)$.

  Then $I(\mathbf{X}) = \{\mu: \mathbf{X}\in A(\mu)\} = \{\mu:\sqrt{n}(\bar X - \mu)| < 1.96\%\}$. So a $95\%$ confidence set for $\mu$ is $(\bar X - 1.96/\sqrt{n}, \bar X + 1.96/\sqrt{n})$.
\end{eg}
\end{document}
